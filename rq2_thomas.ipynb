{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns                       #visualisation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "import os\n",
    "from sklearn import preprocessing \n",
    "\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thomas_path = \"C:\\\\Users\\\\User\\\\Desktop\\\\Data Airbnb\"\n",
    "thomas_path = \"/home/user2/CLEAN/inside-airbnb-vacancies/Data Airbnb\"\n",
    "\n",
    "listings = pd.read_csv(os.path.join(thomas_path, \"listings.csv\"))\n",
    "sum_listings = pd.read_csv(os.path.join(thomas_path, \"listings summary.csv\"))\n",
    "sum_nbhd = pd.read_csv(os.path.join(thomas_path, \"neighbourhoods summary.csv\"))\n",
    "sum_rev = pd.read_csv(os.path.join(thomas_path, \"reviews summary.csv\"))\n",
    "revs = pd.read_csv(os.path.join(thomas_path, \"reviews.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimum_nights              minimum number of night stay for the listing (calendar rules may be different)\n",
    "minimum_minimum_nights      the smallest minimum_night value from the calender (looking 365 nights in the future)\n",
    "maximum_minimum_nights      the largest minimum_night value from the calender (looking 365 nights in the future)\n",
    "minimum_nights_avg_ntm      the average minimum_night value from the calender (looking 365 nights in the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ2: How does the minimum number of nights that customers must book affect the likelihood of a property being vacant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
      "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
      "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
      "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
      "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
      "       'host_neighbourhood', 'host_listings_count',\n",
      "       'host_total_listings_count', 'host_verifications',\n",
      "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
      "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
      "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
      "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
      "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
      "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
      "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
      "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
      "       'availability_30', 'availability_60', 'availability_90',\n",
      "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
      "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
      "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
      "       'review_scores_cleanliness', 'review_scores_checkin',\n",
      "       'review_scores_communication', 'review_scores_location',\n",
      "       'review_scores_value', 'license', 'instant_bookable',\n",
      "       'calculated_host_listings_count',\n",
      "       'calculated_host_listings_count_entire_homes',\n",
      "       'calculated_host_listings_count_private_rooms',\n",
      "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(listings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>number_of_reviews_ltm</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34935</td>\n",
       "      <td>Rental unit in Greater London · ★4.55 · 1 bedr...</td>\n",
       "      <td>133271</td>\n",
       "      <td>Hendryks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "      <td>51.52367</td>\n",
       "      <td>-0.068886</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>151.0</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198258</td>\n",
       "      <td>Rental unit in Barking · ★4.74 · 1 bedroom · 1...</td>\n",
       "      <td>967537</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>51.53430</td>\n",
       "      <td>0.081780</td>\n",
       "      <td>Private room</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>2023-03-16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1</td>\n",
       "      <td>363</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228389</td>\n",
       "      <td>Home in Croydon · 1 bedroom · 1 bed · 1.5 shar...</td>\n",
       "      <td>1023326</td>\n",
       "      <td>Jocelyn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>51.36646</td>\n",
       "      <td>-0.121180</td>\n",
       "      <td>Private room</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229684</td>\n",
       "      <td>Home in Lewisham · ★4.41 · 1 bedroom · 1 bed ·...</td>\n",
       "      <td>448365</td>\n",
       "      <td>Roland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lewisham</td>\n",
       "      <td>51.48793</td>\n",
       "      <td>-0.042040</td>\n",
       "      <td>Private room</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2016-11-06</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230839</td>\n",
       "      <td>Rental unit in Hackney · ★4.50 · 1 bedroom · 1...</td>\n",
       "      <td>671259</td>\n",
       "      <td>Kiki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hackney</td>\n",
       "      <td>51.53680</td>\n",
       "      <td>-0.077530</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-08-16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               name  host_id  \\\n",
       "0   34935  Rental unit in Greater London · ★4.55 · 1 bedr...   133271   \n",
       "1  198258  Rental unit in Barking · ★4.74 · 1 bedroom · 1...   967537   \n",
       "2  228389  Home in Croydon · 1 bedroom · 1 bed · 1.5 shar...  1023326   \n",
       "3  229684  Home in Lewisham · ★4.41 · 1 bedroom · 1 bed ·...   448365   \n",
       "4  230839  Rental unit in Hackney · ★4.50 · 1 bedroom · 1...   671259   \n",
       "\n",
       "  host_name  neighbourhood_group         neighbourhood  latitude  longitude  \\\n",
       "0  Hendryks                  NaN         Tower Hamlets  51.52367  -0.068886   \n",
       "1      Ryan                  NaN  Barking and Dagenham  51.53430   0.081780   \n",
       "2   Jocelyn                  NaN               Croydon  51.36646  -0.121180   \n",
       "3    Roland                  NaN              Lewisham  51.48793  -0.042040   \n",
       "4      Kiki                  NaN               Hackney  51.53680  -0.077530   \n",
       "\n",
       "         room_type  price  minimum_nights  number_of_reviews last_review  \\\n",
       "0  Entire home/apt  151.0               2                122  2023-10-02   \n",
       "1     Private room   67.0               2                 41  2023-03-16   \n",
       "2     Private room   50.0               2                  0         NaN   \n",
       "3     Private room   30.0               1                 22  2016-11-06   \n",
       "4  Entire home/apt    NaN               7                  4  2012-08-16   \n",
       "\n",
       "   reviews_per_month  calculated_host_listings_count  availability_365  \\\n",
       "0               0.75                              10                 7   \n",
       "1               0.27                               1               363   \n",
       "2                NaN                               3               341   \n",
       "3               0.20                               3                 0   \n",
       "4               0.03                               1                 0   \n",
       "\n",
       "   number_of_reviews_ltm  license  \n",
       "0                      3      NaN  \n",
       "1                      1      NaN  \n",
       "2                      0      NaN  \n",
       "3                      0      NaN  \n",
       "4                      0      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Barnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bexley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Brent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bromley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neighbourhood_group         neighbourhood\n",
       "0                  NaN  Barking and Dagenham\n",
       "1                  NaN                Barnet\n",
       "2                  NaN                Bexley\n",
       "3                  NaN                 Brent\n",
       "4                  NaN               Bromley"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_nbhd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13913</td>\n",
       "      <td>2010-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13913</td>\n",
       "      <td>2011-07-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13913</td>\n",
       "      <td>2011-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13913</td>\n",
       "      <td>2011-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13913</td>\n",
       "      <td>2011-10-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date\n",
       "0       13913  2010-08-18\n",
       "1       13913  2011-07-11\n",
       "2       13913  2011-09-13\n",
       "3       13913  2011-10-03\n",
       "4       13913  2011-10-09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13913</td>\n",
       "      <td>80770</td>\n",
       "      <td>2010-08-18</td>\n",
       "      <td>177109</td>\n",
       "      <td>Michael</td>\n",
       "      <td>My girlfriend and I hadn't known Alina before ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13913</td>\n",
       "      <td>367568</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>19835707</td>\n",
       "      <td>Mathias</td>\n",
       "      <td>Alina was a really good host. The flat is clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13913</td>\n",
       "      <td>529579</td>\n",
       "      <td>2011-09-13</td>\n",
       "      <td>1110304</td>\n",
       "      <td>Kristin</td>\n",
       "      <td>Alina is an amazing host. She made me feel rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13913</td>\n",
       "      <td>595481</td>\n",
       "      <td>2011-10-03</td>\n",
       "      <td>1216358</td>\n",
       "      <td>Camilla</td>\n",
       "      <td>Alina's place is so nice, the room is big and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13913</td>\n",
       "      <td>612947</td>\n",
       "      <td>2011-10-09</td>\n",
       "      <td>490840</td>\n",
       "      <td>Jorik</td>\n",
       "      <td>Nice location in Islington area, good for shor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id      id        date  reviewer_id reviewer_name  \\\n",
       "0       13913   80770  2010-08-18       177109       Michael   \n",
       "1       13913  367568  2011-07-11     19835707       Mathias   \n",
       "2       13913  529579  2011-09-13      1110304       Kristin   \n",
       "3       13913  595481  2011-10-03      1216358       Camilla   \n",
       "4       13913  612947  2011-10-09       490840         Jorik   \n",
       "\n",
       "                                            comments  \n",
       "0  My girlfriend and I hadn't known Alina before ...  \n",
       "1  Alina was a really good host. The flat is clea...  \n",
       "2  Alina is an amazing host. She made me feel rig...  \n",
       "3  Alina's place is so nice, the room is big and ...  \n",
       "4  Nice location in Islington area, good for shor...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2.0\n",
       "1         2.0\n",
       "2         4.0\n",
       "3        14.0\n",
       "4         1.0\n",
       "         ... \n",
       "91773     7.0\n",
       "91774     3.0\n",
       "91775     1.0\n",
       "91776     1.0\n",
       "91777     1.0\n",
       "Name: minimum_minimum_nights, Length: 91778, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings['minimum_minimum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         2\n",
       "2         4\n",
       "3        14\n",
       "4         1\n",
       "         ..\n",
       "91773     7\n",
       "91774     3\n",
       "91775     1\n",
       "91776     1\n",
       "91777     3\n",
       "Name: minimum_nights, Length: 91778, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings['minimum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    91777.000000\n",
       "mean        -0.361627\n",
       "std          9.631561\n",
       "min       -997.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        364.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(listings['minimum_minimum_nights']-listings['minimum_nights']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
    "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
    "       'maximum_nights_avg_ntm', 'has_availability',\n",
    "       'availability_30', 'availability_60', 'availability_90',\n",
    "       'availability_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'minimum_nights'}>]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame.hist(data = sum_listings, column='minimum_nights', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                3521\n",
       "name                              3521\n",
       "host_id                           3521\n",
       "host_name                         3521\n",
       "neighbourhood_group                  0\n",
       "neighbourhood                     3521\n",
       "latitude                          3521\n",
       "longitude                         3521\n",
       "room_type                         3521\n",
       "price                             3381\n",
       "minimum_nights                    3521\n",
       "number_of_reviews                 3521\n",
       "last_review                       1970\n",
       "reviews_per_month                 1970\n",
       "calculated_host_listings_count    3521\n",
       "availability_365                  3521\n",
       "number_of_reviews_ltm             3521\n",
       "license                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_listings[sum_listings['minimum_nights']>21].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04385314863662187\n",
      "0.03566644735993\n",
      "0.037195711037940074\n",
      "0.04552554648042026\n"
     ]
    }
   ],
   "source": [
    "min_nights = listings['minimum_nights']\n",
    "avail_356 = listings['availability_365']\n",
    "avail_90 = listings['availability_90']\n",
    "avail_60 = listings['availability_60']\n",
    "avail_30 = listings['availability_30']\n",
    "\n",
    "corr_365 = avail_356.corr(min_nights)\n",
    "corr_90 = avail_90.corr(min_nights)\n",
    "corr_60 = avail_60.corr(min_nights)\n",
    "corr_30 = avail_30.corr(min_nights)\n",
    "\n",
    "print(corr_365)\n",
    "print(corr_90)\n",
    "print(corr_60)\n",
    "print(corr_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does not appear to be a substantial correlation between number of minimum nights and availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scrape_id</th>\n",
       "      <th>description</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_listings_count</th>\n",
       "      <th>host_total_listings_count</th>\n",
       "      <th>neighbourhood_group_cleansed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>license</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>calculated_host_listings_count_entire_homes</th>\n",
       "      <th>calculated_host_listings_count_private_rooms</th>\n",
       "      <th>calculated_host_listings_count_shared_rooms</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.512000e+04</td>\n",
       "      <td>1.512000e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.512000e+04</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13909.000000</td>\n",
       "      <td>13911.000000</td>\n",
       "      <td>13910.000000</td>\n",
       "      <td>13910.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>15120.000000</td>\n",
       "      <td>13901.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.158278e+17</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.482204e+08</td>\n",
       "      <td>8.629233</td>\n",
       "      <td>17.027315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.507531</td>\n",
       "      <td>-0.132041</td>\n",
       "      <td>3.177183</td>\n",
       "      <td>...</td>\n",
       "      <td>4.900996</td>\n",
       "      <td>4.928956</td>\n",
       "      <td>4.811306</td>\n",
       "      <td>4.766196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.486640</td>\n",
       "      <td>5.887302</td>\n",
       "      <td>1.577844</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>1.576071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.228817e+17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.682271e+08</td>\n",
       "      <td>19.181167</td>\n",
       "      <td>55.193207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>2.035604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178306</td>\n",
       "      <td>0.159462</td>\n",
       "      <td>0.231818</td>\n",
       "      <td>0.250294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.849248</td>\n",
       "      <td>16.256782</td>\n",
       "      <td>4.092993</td>\n",
       "      <td>0.150323</td>\n",
       "      <td>1.575912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.740200e+04</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.775000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.295937</td>\n",
       "      <td>-0.497800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.764876e+07</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.742254e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.480947</td>\n",
       "      <td>-0.192658</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>4.910000</td>\n",
       "      <td>4.730000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.510563e+17</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.384447e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.510990</td>\n",
       "      <td>-0.131123</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>4.860000</td>\n",
       "      <td>4.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.439488e+17</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.347141e+08</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.537373</td>\n",
       "      <td>-0.072413</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.970000</td>\n",
       "      <td>4.910000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.042133e+18</td>\n",
       "      <td>2.023121e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.377356e+08</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.681642</td>\n",
       "      <td>0.288570</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     scrape_id  description       host_id  \\\n",
       "count  1.512000e+04  1.512000e+04          0.0  1.512000e+04   \n",
       "mean   4.158278e+17  2.023121e+13          NaN  1.482204e+08   \n",
       "std    4.228817e+17  0.000000e+00          NaN  1.682271e+08   \n",
       "min    1.740200e+04  2.023121e+13          NaN  4.775000e+03   \n",
       "25%    2.764876e+07  2.023121e+13          NaN  1.742254e+07   \n",
       "50%    5.510563e+17  2.023121e+13          NaN  6.384447e+07   \n",
       "75%    8.439488e+17  2.023121e+13          NaN  2.347141e+08   \n",
       "max    1.042133e+18  2.023121e+13          NaN  5.377356e+08   \n",
       "\n",
       "       host_listings_count  host_total_listings_count  \\\n",
       "count         15120.000000               15120.000000   \n",
       "mean              8.629233                  17.027315   \n",
       "std              19.181167                  55.193207   \n",
       "min               1.000000                   1.000000   \n",
       "25%               1.000000                   2.000000   \n",
       "50%               3.000000                   4.000000   \n",
       "75%               6.000000                   9.000000   \n",
       "max             149.000000                 510.000000   \n",
       "\n",
       "       neighbourhood_group_cleansed      latitude     longitude  accommodates  \\\n",
       "count                           0.0  15120.000000  15120.000000  15120.000000   \n",
       "mean                            NaN     51.507531     -0.132041      3.177183   \n",
       "std                             NaN      0.049415      0.099777      2.035604   \n",
       "min                             NaN     51.295937     -0.497800      1.000000   \n",
       "25%                             NaN     51.480947     -0.192658      2.000000   \n",
       "50%                             NaN     51.510990     -0.131123      2.000000   \n",
       "75%                             NaN     51.537373     -0.072413      4.000000   \n",
       "max                             NaN     51.681642      0.288570     16.000000   \n",
       "\n",
       "       ...  review_scores_checkin  review_scores_communication  \\\n",
       "count  ...           13909.000000                 13911.000000   \n",
       "mean   ...               4.900996                     4.928956   \n",
       "std    ...               0.178306                     0.159462   \n",
       "min    ...               1.000000                     1.000000   \n",
       "25%    ...               4.880000                     4.910000   \n",
       "50%    ...               4.950000                     4.980000   \n",
       "75%    ...               5.000000                     5.000000   \n",
       "max    ...               5.000000                     5.000000   \n",
       "\n",
       "       review_scores_location  review_scores_value  license  \\\n",
       "count            13910.000000         13910.000000      0.0   \n",
       "mean                 4.811306             4.766196      NaN   \n",
       "std                  0.231818             0.250294      NaN   \n",
       "min                  1.000000             1.000000      NaN   \n",
       "25%                  4.730000             4.690000      NaN   \n",
       "50%                  4.860000             4.810000      NaN   \n",
       "75%                  4.970000             4.910000      NaN   \n",
       "max                  5.330000             5.000000      NaN   \n",
       "\n",
       "       calculated_host_listings_count  \\\n",
       "count                    15120.000000   \n",
       "mean                         7.486640   \n",
       "std                         16.849248   \n",
       "min                          1.000000   \n",
       "25%                          1.000000   \n",
       "50%                          2.000000   \n",
       "75%                          6.000000   \n",
       "max                        131.000000   \n",
       "\n",
       "       calculated_host_listings_count_entire_homes  \\\n",
       "count                                 15120.000000   \n",
       "mean                                      5.887302   \n",
       "std                                      16.256782   \n",
       "min                                       0.000000   \n",
       "25%                                       0.000000   \n",
       "50%                                       1.000000   \n",
       "75%                                       3.000000   \n",
       "max                                     127.000000   \n",
       "\n",
       "       calculated_host_listings_count_private_rooms  \\\n",
       "count                                  15120.000000   \n",
       "mean                                       1.577844   \n",
       "std                                        4.092993   \n",
       "min                                        0.000000   \n",
       "25%                                        0.000000   \n",
       "50%                                        1.000000   \n",
       "75%                                        2.000000   \n",
       "max                                       47.000000   \n",
       "\n",
       "       calculated_host_listings_count_shared_rooms  reviews_per_month  \n",
       "count                                 15120.000000       13901.000000  \n",
       "mean                                      0.020503           1.576071  \n",
       "std                                       0.150323           1.575912  \n",
       "min                                       0.000000           0.010000  \n",
       "25%                                       0.000000           0.540000  \n",
       "50%                                       0.000000           1.070000  \n",
       "75%                                       0.000000           2.030000  \n",
       "max                                       3.000000          19.700000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superhost_listings = listings[listings['host_is_superhost'] == 't']\n",
    "superhost_listings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 11:47:15.786435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-19 11:47:15.786465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-19 11:47:15.787433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-19 11:47:15.791668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-19 11:47:16.320565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Neural Net modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to use GPU 0\n",
    "libDevicePath = '--xla_gpu_cuda_data_dir=/usr/lib/cuda/nvvm'\n",
    "os.environ['XLA_FLAGS'] = libDevicePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12096, 8) (3024, 8) (12096,) (3024,)\n"
     ]
    }
   ],
   "source": [
    "superhost_listings_filter = superhost_listings.loc[:, columns]\n",
    "superhost_listings_filter.head()\n",
    "\n",
    "y30 = superhost_listings_filter['availability_30']\n",
    "y60 = superhost_listings_filter['availability_60']\n",
    "y90 = superhost_listings_filter['availability_90']\n",
    "y365 = superhost_listings_filter['availability_365']\n",
    "X = superhost_listings_filter.drop(['availability_30', 'availability_60', 'availability_90', 'availability_365', 'has_availability'], axis=1)\n",
    "\n",
    "X = np.array(X)\n",
    "y30 = np.array(y30)\n",
    "y60 = np.array(y60)\n",
    "y90 = np.array(y90)\n",
    "y365 = np.array(y365)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y365,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# use minMax scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1000)              9000      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 250)               125250    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 635001 (2.42 MB)\n",
      "Trainable params: 635001 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 11:47:16.902460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.930322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.930546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.933986: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.934201: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.934392: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.992014: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.992233: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.992428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-19 11:47:16.992581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11728 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:2c:00.0, compute capability: 8.6\n",
      "2024-04-19 11:47:17.706054: W external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:504] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  /usr/lib/cuda/nvvm\n",
      "  /usr/local/cuda-12.2\n",
      "  /usr/local/cuda\n",
      "  /home/user2/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/user2/.local/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-04-19 11:47:18.389547: I external/local_xla/xla/service/service.cc:168] XLA service 0x7401354600b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-19 11:47:18.389593: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A5500, Compute Capability 8.6\n",
      "2024-04-19 11:47:18.398402: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-19 11:47:18.417504: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713541638.464186 3856869 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 3s 4ms/step - loss: 96.9139 - mae: 8.3184 - val_loss: 90.9311 - val_mae: 8.1367\n",
      "Epoch 2/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 94.5391 - mae: 8.2521 - val_loss: 91.7395 - val_mae: 8.0193\n",
      "Epoch 3/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.7725 - mae: 8.2138 - val_loss: 91.2496 - val_mae: 8.0177\n",
      "Epoch 4/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.8948 - mae: 8.2197 - val_loss: 92.4158 - val_mae: 8.3361\n",
      "Epoch 5/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.5138 - mae: 8.2075 - val_loss: 90.4827 - val_mae: 8.1561\n",
      "Epoch 6/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 93.2451 - mae: 8.2016 - val_loss: 91.4366 - val_mae: 7.9931\n",
      "Epoch 7/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.5705 - mae: 8.2061 - val_loss: 90.4852 - val_mae: 8.1844\n",
      "Epoch 8/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.1269 - mae: 8.2012 - val_loss: 90.3776 - val_mae: 8.1613\n",
      "Epoch 9/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 93.0425 - mae: 8.1990 - val_loss: 90.2559 - val_mae: 8.0558\n",
      "Epoch 10/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.9700 - mae: 8.2016 - val_loss: 91.5009 - val_mae: 7.9983\n",
      "Epoch 11/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.8839 - mae: 8.1840 - val_loss: 90.3140 - val_mae: 8.0876\n",
      "Epoch 12/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.9267 - mae: 8.1891 - val_loss: 90.1499 - val_mae: 8.1257\n",
      "Epoch 13/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.5343 - mae: 8.1747 - val_loss: 90.5514 - val_mae: 8.1367\n",
      "Epoch 14/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.7902 - mae: 8.1921 - val_loss: 90.4257 - val_mae: 8.1802\n",
      "Epoch 15/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 93.0628 - mae: 8.1990 - val_loss: 90.0378 - val_mae: 8.0792\n",
      "Epoch 16/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.5186 - mae: 8.1830 - val_loss: 90.4409 - val_mae: 8.1147\n",
      "Epoch 17/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 93.0077 - mae: 8.1927 - val_loss: 90.2521 - val_mae: 8.0458\n",
      "Epoch 18/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.1836 - mae: 8.1669 - val_loss: 90.3367 - val_mae: 8.0172\n",
      "Epoch 19/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.1626 - mae: 8.1679 - val_loss: 92.6075 - val_mae: 7.9724\n",
      "Epoch 20/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.3213 - mae: 8.1683 - val_loss: 90.1431 - val_mae: 8.1024\n",
      "Epoch 21/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.1423 - mae: 8.1667 - val_loss: 90.9929 - val_mae: 8.0084\n",
      "Epoch 22/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.0010 - mae: 8.1674 - val_loss: 90.0760 - val_mae: 8.0576\n",
      "Epoch 23/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.2311 - mae: 8.1666 - val_loss: 90.0011 - val_mae: 8.0641\n",
      "Epoch 24/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.9198 - mae: 8.1600 - val_loss: 90.0169 - val_mae: 8.0884\n",
      "Epoch 25/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.7567 - mae: 8.1512 - val_loss: 90.3478 - val_mae: 8.0759\n",
      "Epoch 26/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 92.0692 - mae: 8.1672 - val_loss: 90.2326 - val_mae: 8.0763\n",
      "Epoch 27/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4478 - mae: 8.1420 - val_loss: 90.1958 - val_mae: 8.1351\n",
      "Epoch 28/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3335 - mae: 8.1492 - val_loss: 90.1115 - val_mae: 8.0872\n",
      "Epoch 29/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5636 - mae: 8.1486 - val_loss: 90.1710 - val_mae: 8.0929\n",
      "Epoch 30/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5128 - mae: 8.1493 - val_loss: 90.3171 - val_mae: 8.0432\n",
      "Epoch 31/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.9204 - mae: 8.1624 - val_loss: 90.0991 - val_mae: 8.0929\n",
      "Epoch 32/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.8410 - mae: 8.1740 - val_loss: 90.2332 - val_mae: 8.0649\n",
      "Epoch 33/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.7344 - mae: 8.1591 - val_loss: 90.1283 - val_mae: 8.0865\n",
      "Epoch 34/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3833 - mae: 8.1512 - val_loss: 90.8145 - val_mae: 7.9907\n",
      "Epoch 35/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.7206 - mae: 8.1591 - val_loss: 90.6582 - val_mae: 8.0109\n",
      "Epoch 36/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.7582 - mae: 8.1518 - val_loss: 90.2210 - val_mae: 8.1114\n",
      "Epoch 37/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.6433 - mae: 8.1609 - val_loss: 90.1562 - val_mae: 8.0857\n",
      "Epoch 38/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5874 - mae: 8.1553 - val_loss: 90.1379 - val_mae: 8.0539\n",
      "Epoch 39/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.6589 - mae: 8.1642 - val_loss: 90.1663 - val_mae: 8.0566\n",
      "Epoch 40/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.8242 - mae: 8.1654 - val_loss: 90.2104 - val_mae: 8.0769\n",
      "Epoch 41/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.7245 - mae: 8.1686 - val_loss: 89.9599 - val_mae: 8.1098\n",
      "Epoch 42/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3310 - mae: 8.1439 - val_loss: 90.3084 - val_mae: 8.1122\n",
      "Epoch 43/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5505 - mae: 8.1628 - val_loss: 89.9977 - val_mae: 8.0742\n",
      "Epoch 44/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3637 - mae: 8.1513 - val_loss: 89.9943 - val_mae: 8.1175\n",
      "Epoch 45/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5340 - mae: 8.1588 - val_loss: 90.0185 - val_mae: 8.1129\n",
      "Epoch 46/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3999 - mae: 8.1519 - val_loss: 90.1736 - val_mae: 8.1055\n",
      "Epoch 47/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2719 - mae: 8.1486 - val_loss: 89.9677 - val_mae: 8.1117\n",
      "Epoch 48/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1248 - mae: 8.1443 - val_loss: 89.9212 - val_mae: 8.0557\n",
      "Epoch 49/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.6225 - mae: 8.1601 - val_loss: 89.9548 - val_mae: 8.0472\n",
      "Epoch 50/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2453 - mae: 8.1426 - val_loss: 89.9642 - val_mae: 8.1238\n",
      "Epoch 51/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2050 - mae: 8.1488 - val_loss: 89.9361 - val_mae: 8.0326\n",
      "Epoch 52/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4040 - mae: 8.1567 - val_loss: 89.9959 - val_mae: 8.0633\n",
      "Epoch 53/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2220 - mae: 8.1421 - val_loss: 89.9921 - val_mae: 8.1423\n",
      "Epoch 54/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2065 - mae: 8.1524 - val_loss: 89.7194 - val_mae: 8.0879\n",
      "Epoch 55/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4019 - mae: 8.1429 - val_loss: 89.9470 - val_mae: 8.1313\n",
      "Epoch 56/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.6191 - mae: 8.1611 - val_loss: 90.0993 - val_mae: 8.1302\n",
      "Epoch 57/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2841 - mae: 8.1512 - val_loss: 90.0624 - val_mae: 8.0575\n",
      "Epoch 58/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5100 - mae: 8.1510 - val_loss: 89.8820 - val_mae: 8.0492\n",
      "Epoch 59/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5828 - mae: 8.1534 - val_loss: 90.1746 - val_mae: 8.0955\n",
      "Epoch 60/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3327 - mae: 8.1601 - val_loss: 89.9896 - val_mae: 8.0355\n",
      "Epoch 61/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.4687 - mae: 8.1418 - val_loss: 90.1247 - val_mae: 8.1552\n",
      "Epoch 62/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9288 - mae: 8.1302 - val_loss: 89.9234 - val_mae: 8.0827\n",
      "Epoch 63/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4124 - mae: 8.1481 - val_loss: 89.7624 - val_mae: 8.0782\n",
      "Epoch 64/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2750 - mae: 8.1339 - val_loss: 89.9360 - val_mae: 8.0945\n",
      "Epoch 65/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.4045 - mae: 8.1499 - val_loss: 89.9353 - val_mae: 8.1462\n",
      "Epoch 66/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5564 - mae: 8.1648 - val_loss: 89.9788 - val_mae: 8.1166\n",
      "Epoch 67/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4199 - mae: 8.1567 - val_loss: 89.9792 - val_mae: 8.0666\n",
      "Epoch 68/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.7486 - mae: 8.1742 - val_loss: 89.9095 - val_mae: 8.1108\n",
      "Epoch 69/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3756 - mae: 8.1453 - val_loss: 90.1265 - val_mae: 8.0568\n",
      "Epoch 70/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2365 - mae: 8.1484 - val_loss: 89.9768 - val_mae: 8.0908\n",
      "Epoch 71/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1465 - mae: 8.1438 - val_loss: 90.3203 - val_mae: 8.1713\n",
      "Epoch 72/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5464 - mae: 8.1525 - val_loss: 90.0531 - val_mae: 8.1193\n",
      "Epoch 73/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5902 - mae: 8.1595 - val_loss: 90.1160 - val_mae: 8.0399\n",
      "Epoch 74/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5391 - mae: 8.1475 - val_loss: 89.9904 - val_mae: 8.1190\n",
      "Epoch 75/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5115 - mae: 8.1629 - val_loss: 89.7594 - val_mae: 8.0597\n",
      "Epoch 76/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2860 - mae: 8.1436 - val_loss: 89.8858 - val_mae: 8.1060\n",
      "Epoch 77/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3882 - mae: 8.1524 - val_loss: 89.8374 - val_mae: 8.0676\n",
      "Epoch 78/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5240 - mae: 8.1530 - val_loss: 89.8273 - val_mae: 8.0732\n",
      "Epoch 79/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5783 - mae: 8.1565 - val_loss: 89.9070 - val_mae: 8.0499\n",
      "Epoch 80/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3011 - mae: 8.1426 - val_loss: 89.8059 - val_mae: 8.1108\n",
      "Epoch 81/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1929 - mae: 8.1439 - val_loss: 90.0530 - val_mae: 8.1402\n",
      "Epoch 82/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5616 - mae: 8.1644 - val_loss: 89.9814 - val_mae: 8.0407\n",
      "Epoch 83/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1749 - mae: 8.1380 - val_loss: 89.7565 - val_mae: 8.0849\n",
      "Epoch 84/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2692 - mae: 8.1375 - val_loss: 89.9069 - val_mae: 8.0822\n",
      "Epoch 85/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0960 - mae: 8.1432 - val_loss: 89.8691 - val_mae: 8.0355\n",
      "Epoch 86/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1459 - mae: 8.1376 - val_loss: 89.8587 - val_mae: 8.1180\n",
      "Epoch 87/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2397 - mae: 8.1445 - val_loss: 89.6910 - val_mae: 8.0782\n",
      "Epoch 88/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1167 - mae: 8.1427 - val_loss: 90.2026 - val_mae: 8.0254\n",
      "Epoch 89/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4309 - mae: 8.1477 - val_loss: 89.8620 - val_mae: 8.1046\n",
      "Epoch 90/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9409 - mae: 8.1491 - val_loss: 89.7126 - val_mae: 8.0346\n",
      "Epoch 91/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0106 - mae: 8.1327 - val_loss: 89.8825 - val_mae: 8.0914\n",
      "Epoch 92/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2438 - mae: 8.1427 - val_loss: 89.8268 - val_mae: 8.0574\n",
      "Epoch 93/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1939 - mae: 8.1375 - val_loss: 89.8917 - val_mae: 8.1171\n",
      "Epoch 94/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.4686 - mae: 8.1551 - val_loss: 89.6860 - val_mae: 8.0281\n",
      "Epoch 95/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1811 - mae: 8.1310 - val_loss: 89.9607 - val_mae: 8.1114\n",
      "Epoch 96/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1875 - mae: 8.1483 - val_loss: 90.1995 - val_mae: 8.0158\n",
      "Epoch 97/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1010 - mae: 8.1403 - val_loss: 89.7120 - val_mae: 8.0870\n",
      "Epoch 98/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2670 - mae: 8.1506 - val_loss: 89.8126 - val_mae: 8.0798\n",
      "Epoch 99/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2029 - mae: 8.1382 - val_loss: 89.9618 - val_mae: 8.0555\n",
      "Epoch 100/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.5953 - mae: 8.1659 - val_loss: 89.7518 - val_mae: 8.0560\n",
      "Epoch 101/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8524 - mae: 8.1257 - val_loss: 89.7172 - val_mae: 8.0936\n",
      "Epoch 102/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4257 - mae: 8.1545 - val_loss: 90.2284 - val_mae: 7.9918\n",
      "Epoch 103/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3381 - mae: 8.1430 - val_loss: 89.9333 - val_mae: 8.0778\n",
      "Epoch 104/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1579 - mae: 8.1454 - val_loss: 89.7127 - val_mae: 8.0978\n",
      "Epoch 105/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2704 - mae: 8.1494 - val_loss: 89.7741 - val_mae: 8.0795\n",
      "Epoch 106/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3965 - mae: 8.1550 - val_loss: 89.8829 - val_mae: 8.0617\n",
      "Epoch 107/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1949 - mae: 8.1450 - val_loss: 89.8889 - val_mae: 8.0889\n",
      "Epoch 108/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0855 - mae: 8.1397 - val_loss: 89.5881 - val_mae: 8.0458\n",
      "Epoch 109/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1737 - mae: 8.1323 - val_loss: 89.8391 - val_mae: 8.0955\n",
      "Epoch 110/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2005 - mae: 8.1428 - val_loss: 89.9563 - val_mae: 8.0642\n",
      "Epoch 111/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9493 - mae: 8.1272 - val_loss: 89.7915 - val_mae: 8.1030\n",
      "Epoch 112/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8164 - mae: 8.1210 - val_loss: 89.8981 - val_mae: 8.1292\n",
      "Epoch 113/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4138 - mae: 8.1465 - val_loss: 90.0339 - val_mae: 8.1443\n",
      "Epoch 114/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2665 - mae: 8.1471 - val_loss: 89.5638 - val_mae: 8.0642\n",
      "Epoch 115/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0871 - mae: 8.1291 - val_loss: 89.5233 - val_mae: 8.0998\n",
      "Epoch 116/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3937 - mae: 8.1498 - val_loss: 89.6424 - val_mae: 8.0736\n",
      "Epoch 117/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0754 - mae: 8.1439 - val_loss: 89.6877 - val_mae: 8.0104\n",
      "Epoch 118/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1490 - mae: 8.1399 - val_loss: 89.6087 - val_mae: 8.0898\n",
      "Epoch 119/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2583 - mae: 8.1496 - val_loss: 89.6166 - val_mae: 8.0398\n",
      "Epoch 120/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1888 - mae: 8.1430 - val_loss: 89.5374 - val_mae: 8.0802\n",
      "Epoch 121/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9459 - mae: 8.1378 - val_loss: 89.9745 - val_mae: 8.0101\n",
      "Epoch 122/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3074 - mae: 8.1394 - val_loss: 89.6775 - val_mae: 8.0803\n",
      "Epoch 123/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1539 - mae: 8.1453 - val_loss: 89.6327 - val_mae: 8.0466\n",
      "Epoch 124/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0808 - mae: 8.1374 - val_loss: 89.7827 - val_mae: 8.1060\n",
      "Epoch 125/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0338 - mae: 8.1445 - val_loss: 89.7625 - val_mae: 8.0554\n",
      "Epoch 126/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0321 - mae: 8.1398 - val_loss: 89.7992 - val_mae: 8.0074\n",
      "Epoch 127/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1638 - mae: 8.1394 - val_loss: 89.6394 - val_mae: 8.0655\n",
      "Epoch 128/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9171 - mae: 8.1378 - val_loss: 89.7476 - val_mae: 8.0439\n",
      "Epoch 129/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2102 - mae: 8.1406 - val_loss: 89.9271 - val_mae: 8.0180\n",
      "Epoch 130/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0575 - mae: 8.1321 - val_loss: 89.5818 - val_mae: 8.0883\n",
      "Epoch 131/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4578 - mae: 8.1520 - val_loss: 89.7327 - val_mae: 8.0984\n",
      "Epoch 132/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1373 - mae: 8.1466 - val_loss: 89.7385 - val_mae: 8.0531\n",
      "Epoch 133/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9192 - mae: 8.1265 - val_loss: 89.6601 - val_mae: 8.0714\n",
      "Epoch 134/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9497 - mae: 8.1346 - val_loss: 89.5650 - val_mae: 8.0472\n",
      "Epoch 135/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1662 - mae: 8.1467 - val_loss: 89.7549 - val_mae: 8.0807\n",
      "Epoch 136/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9664 - mae: 8.1369 - val_loss: 89.5931 - val_mae: 8.0399\n",
      "Epoch 137/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4270 - mae: 8.1485 - val_loss: 89.5906 - val_mae: 8.0810\n",
      "Epoch 138/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5647 - mae: 8.1578 - val_loss: 89.8320 - val_mae: 8.0510\n",
      "Epoch 139/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2022 - mae: 8.1423 - val_loss: 89.6540 - val_mae: 8.0603\n",
      "Epoch 140/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1665 - mae: 8.1367 - val_loss: 89.8213 - val_mae: 8.0610\n",
      "Epoch 141/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1087 - mae: 8.1370 - val_loss: 89.7748 - val_mae: 8.0515\n",
      "Epoch 142/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1980 - mae: 8.1537 - val_loss: 89.8596 - val_mae: 8.0345\n",
      "Epoch 143/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1406 - mae: 8.1350 - val_loss: 89.8357 - val_mae: 8.0514\n",
      "Epoch 144/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1226 - mae: 8.1417 - val_loss: 89.8386 - val_mae: 8.0382\n",
      "Epoch 145/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3853 - mae: 8.1540 - val_loss: 89.7634 - val_mae: 8.0291\n",
      "Epoch 146/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9861 - mae: 8.1323 - val_loss: 89.6882 - val_mae: 8.0652\n",
      "Epoch 147/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3173 - mae: 8.1553 - val_loss: 89.5726 - val_mae: 8.0057\n",
      "Epoch 148/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1801 - mae: 8.1359 - val_loss: 89.5157 - val_mae: 8.0817\n",
      "Epoch 149/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2618 - mae: 8.1355 - val_loss: 89.6374 - val_mae: 8.0878\n",
      "Epoch 150/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3664 - mae: 8.1402 - val_loss: 89.7135 - val_mae: 8.0937\n",
      "Epoch 151/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9398 - mae: 8.1339 - val_loss: 89.5686 - val_mae: 8.0848\n",
      "Epoch 152/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3575 - mae: 8.1457 - val_loss: 89.6080 - val_mae: 8.0451\n",
      "Epoch 153/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0321 - mae: 8.1234 - val_loss: 89.7865 - val_mae: 8.1078\n",
      "Epoch 154/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1332 - mae: 8.1494 - val_loss: 89.5476 - val_mae: 8.0766\n",
      "Epoch 155/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9692 - mae: 8.1366 - val_loss: 89.7300 - val_mae: 8.0495\n",
      "Epoch 156/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1854 - mae: 8.1408 - val_loss: 89.8598 - val_mae: 8.0266\n",
      "Epoch 157/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1615 - mae: 8.1349 - val_loss: 89.5143 - val_mae: 8.0864\n",
      "Epoch 158/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1277 - mae: 8.1404 - val_loss: 89.7049 - val_mae: 8.0772\n",
      "Epoch 159/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2619 - mae: 8.1393 - val_loss: 89.5836 - val_mae: 8.0313\n",
      "Epoch 160/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0325 - mae: 8.1276 - val_loss: 90.3299 - val_mae: 8.0036\n",
      "Epoch 161/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4032 - mae: 8.1433 - val_loss: 89.6123 - val_mae: 8.0911\n",
      "Epoch 162/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7345 - mae: 8.1176 - val_loss: 89.5423 - val_mae: 8.0472\n",
      "Epoch 163/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2188 - mae: 8.1371 - val_loss: 89.8790 - val_mae: 8.0272\n",
      "Epoch 164/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8771 - mae: 8.1254 - val_loss: 89.5298 - val_mae: 8.0665\n",
      "Epoch 165/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.5143 - mae: 8.1390 - val_loss: 89.7707 - val_mae: 8.0458\n",
      "Epoch 166/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9277 - mae: 8.1253 - val_loss: 89.4996 - val_mae: 8.0941\n",
      "Epoch 167/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9885 - mae: 8.1297 - val_loss: 89.7281 - val_mae: 8.0454\n",
      "Epoch 168/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1931 - mae: 8.1411 - val_loss: 89.6229 - val_mae: 8.0298\n",
      "Epoch 169/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1362 - mae: 8.1406 - val_loss: 89.5169 - val_mae: 8.0483\n",
      "Epoch 170/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0000 - mae: 8.1343 - val_loss: 89.5898 - val_mae: 8.0706\n",
      "Epoch 171/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0955 - mae: 8.1385 - val_loss: 89.4999 - val_mae: 8.0535\n",
      "Epoch 172/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0431 - mae: 8.1327 - val_loss: 89.7051 - val_mae: 8.0934\n",
      "Epoch 173/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2538 - mae: 8.1390 - val_loss: 89.8057 - val_mae: 8.0529\n",
      "Epoch 174/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3274 - mae: 8.1469 - val_loss: 89.8114 - val_mae: 8.0468\n",
      "Epoch 175/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1152 - mae: 8.1456 - val_loss: 89.8335 - val_mae: 8.0128\n",
      "Epoch 176/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9302 - mae: 8.1312 - val_loss: 89.4765 - val_mae: 8.0399\n",
      "Epoch 177/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0713 - mae: 8.1391 - val_loss: 89.5527 - val_mae: 8.0678\n",
      "Epoch 178/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3137 - mae: 8.1336 - val_loss: 89.7587 - val_mae: 8.0538\n",
      "Epoch 179/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2084 - mae: 8.1351 - val_loss: 89.7684 - val_mae: 8.0730\n",
      "Epoch 180/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9269 - mae: 8.1298 - val_loss: 89.6735 - val_mae: 8.0095\n",
      "Epoch 181/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1104 - mae: 8.1399 - val_loss: 89.6377 - val_mae: 8.0571\n",
      "Epoch 182/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9434 - mae: 8.1340 - val_loss: 89.8352 - val_mae: 8.0571\n",
      "Epoch 183/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0161 - mae: 8.1377 - val_loss: 89.5070 - val_mae: 8.0568\n",
      "Epoch 184/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3684 - mae: 8.1445 - val_loss: 89.7267 - val_mae: 8.0214\n",
      "Epoch 185/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1970 - mae: 8.1352 - val_loss: 89.4251 - val_mae: 8.0468\n",
      "Epoch 186/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9009 - mae: 8.1379 - val_loss: 89.7646 - val_mae: 8.0184\n",
      "Epoch 187/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0433 - mae: 8.1429 - val_loss: 89.6740 - val_mae: 8.0199\n",
      "Epoch 188/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2867 - mae: 8.1381 - val_loss: 89.9207 - val_mae: 8.0412\n",
      "Epoch 189/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1733 - mae: 8.1371 - val_loss: 89.8557 - val_mae: 8.0092\n",
      "Epoch 190/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8780 - mae: 8.1267 - val_loss: 89.7365 - val_mae: 8.0152\n",
      "Epoch 191/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0180 - mae: 8.1196 - val_loss: 89.6511 - val_mae: 8.0694\n",
      "Epoch 192/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8739 - mae: 8.1345 - val_loss: 89.7290 - val_mae: 8.1088\n",
      "Epoch 193/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8284 - mae: 8.1357 - val_loss: 89.5171 - val_mae: 8.0316\n",
      "Epoch 194/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3897 - mae: 8.1247 - val_loss: 89.8459 - val_mae: 8.1394\n",
      "Epoch 195/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1677 - mae: 8.1508 - val_loss: 89.5102 - val_mae: 8.0710\n",
      "Epoch 196/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0137 - mae: 8.1388 - val_loss: 90.1204 - val_mae: 8.0206\n",
      "Epoch 197/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2819 - mae: 8.1414 - val_loss: 89.5312 - val_mae: 8.0381\n",
      "Epoch 198/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9143 - mae: 8.1308 - val_loss: 89.6174 - val_mae: 8.0405\n",
      "Epoch 199/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0247 - mae: 8.1091 - val_loss: 89.7110 - val_mae: 8.0956\n",
      "Epoch 200/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8980 - mae: 8.1406 - val_loss: 89.7195 - val_mae: 8.0245\n",
      "Epoch 201/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1288 - mae: 8.1275 - val_loss: 89.6370 - val_mae: 8.0519\n",
      "Epoch 202/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0560 - mae: 8.1362 - val_loss: 89.6598 - val_mae: 8.0366\n",
      "Epoch 203/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1409 - mae: 8.1385 - val_loss: 89.6154 - val_mae: 8.0842\n",
      "Epoch 204/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0030 - mae: 8.1312 - val_loss: 89.7767 - val_mae: 8.0674\n",
      "Epoch 205/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2614 - mae: 8.1425 - val_loss: 89.7430 - val_mae: 8.1005\n",
      "Epoch 206/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1033 - mae: 8.1433 - val_loss: 89.6305 - val_mae: 8.0816\n",
      "Epoch 207/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9798 - mae: 8.1389 - val_loss: 89.7519 - val_mae: 8.0499\n",
      "Epoch 208/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0538 - mae: 8.1366 - val_loss: 89.6105 - val_mae: 8.0512\n",
      "Epoch 209/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8368 - mae: 8.1283 - val_loss: 89.5985 - val_mae: 8.0342\n",
      "Epoch 210/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9234 - mae: 8.1277 - val_loss: 89.4836 - val_mae: 8.0502\n",
      "Epoch 211/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8545 - mae: 8.1269 - val_loss: 89.6424 - val_mae: 8.1014\n",
      "Epoch 212/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1815 - mae: 8.1454 - val_loss: 89.7435 - val_mae: 8.0330\n",
      "Epoch 213/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1059 - mae: 8.1379 - val_loss: 89.7378 - val_mae: 8.0657\n",
      "Epoch 214/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0380 - mae: 8.1372 - val_loss: 89.5357 - val_mae: 8.0777\n",
      "Epoch 215/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0944 - mae: 8.1417 - val_loss: 89.4278 - val_mae: 8.0683\n",
      "Epoch 216/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8321 - mae: 8.1281 - val_loss: 89.8154 - val_mae: 7.9967\n",
      "Epoch 217/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.5736 - mae: 8.1129 - val_loss: 89.4460 - val_mae: 8.0632\n",
      "Epoch 218/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1178 - mae: 8.1482 - val_loss: 89.7386 - val_mae: 8.0449\n",
      "Epoch 219/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.9126 - mae: 8.1260 - val_loss: 89.9375 - val_mae: 8.0374\n",
      "Epoch 220/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7809 - mae: 8.1354 - val_loss: 89.5591 - val_mae: 8.0262\n",
      "Epoch 221/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8502 - mae: 8.1286 - val_loss: 89.9320 - val_mae: 8.0304\n",
      "Epoch 222/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9322 - mae: 8.1351 - val_loss: 89.7527 - val_mae: 8.0121\n",
      "Epoch 223/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8783 - mae: 8.1327 - val_loss: 89.5795 - val_mae: 8.0220\n",
      "Epoch 224/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0025 - mae: 8.1343 - val_loss: 89.5366 - val_mae: 8.0479\n",
      "Epoch 225/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4185 - mae: 8.1412 - val_loss: 89.5500 - val_mae: 8.0448\n",
      "Epoch 226/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7876 - mae: 8.1302 - val_loss: 89.4438 - val_mae: 8.0637\n",
      "Epoch 227/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0844 - mae: 8.1457 - val_loss: 89.4991 - val_mae: 8.0493\n",
      "Epoch 228/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.6977 - mae: 8.1283 - val_loss: 89.3591 - val_mae: 8.0352\n",
      "Epoch 229/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 92.1960 - mae: 8.1519 - val_loss: 90.0485 - val_mae: 7.9917\n",
      "Epoch 230/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2471 - mae: 8.1348 - val_loss: 89.5252 - val_mae: 8.0161\n",
      "Epoch 231/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.4027 - mae: 8.1309 - val_loss: 89.4393 - val_mae: 8.0205\n",
      "Epoch 232/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9377 - mae: 8.1286 - val_loss: 89.4172 - val_mae: 8.0229\n",
      "Epoch 233/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7203 - mae: 8.1159 - val_loss: 89.9902 - val_mae: 8.0300\n",
      "Epoch 234/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1250 - mae: 8.1402 - val_loss: 89.4987 - val_mae: 8.0738\n",
      "Epoch 235/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7770 - mae: 8.1388 - val_loss: 89.6736 - val_mae: 8.0140\n",
      "Epoch 236/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9312 - mae: 8.1252 - val_loss: 89.4284 - val_mae: 8.0578\n",
      "Epoch 237/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1746 - mae: 8.1435 - val_loss: 90.0041 - val_mae: 8.0159\n",
      "Epoch 238/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8565 - mae: 8.1221 - val_loss: 89.5674 - val_mae: 8.0658\n",
      "Epoch 239/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2457 - mae: 8.1421 - val_loss: 89.5566 - val_mae: 8.0324\n",
      "Epoch 240/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0738 - mae: 8.1425 - val_loss: 89.6506 - val_mae: 8.0029\n",
      "Epoch 241/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2317 - mae: 8.1406 - val_loss: 89.7097 - val_mae: 8.1139\n",
      "Epoch 242/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9319 - mae: 8.1386 - val_loss: 89.6841 - val_mae: 8.0332\n",
      "Epoch 243/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9247 - mae: 8.1272 - val_loss: 89.5294 - val_mae: 8.0086\n",
      "Epoch 244/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8742 - mae: 8.1253 - val_loss: 89.9172 - val_mae: 7.9990\n",
      "Epoch 245/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8828 - mae: 8.1271 - val_loss: 89.7335 - val_mae: 7.9974\n",
      "Epoch 246/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0218 - mae: 8.1274 - val_loss: 89.4803 - val_mae: 8.0507\n",
      "Epoch 247/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9141 - mae: 8.1342 - val_loss: 89.7280 - val_mae: 8.0047\n",
      "Epoch 248/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8290 - mae: 8.1207 - val_loss: 89.8526 - val_mae: 8.0018\n",
      "Epoch 249/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0500 - mae: 8.1512 - val_loss: 89.8057 - val_mae: 8.0208\n",
      "Epoch 250/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.4299 - mae: 8.1305 - val_loss: 89.6605 - val_mae: 8.0578\n",
      "Epoch 251/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7317 - mae: 8.1248 - val_loss: 89.4026 - val_mae: 8.0475\n",
      "Epoch 252/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0510 - mae: 8.1314 - val_loss: 89.5433 - val_mae: 8.0616\n",
      "Epoch 253/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9809 - mae: 8.1292 - val_loss: 89.5039 - val_mae: 8.0336\n",
      "Epoch 254/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8213 - mae: 8.1366 - val_loss: 89.3977 - val_mae: 8.0427\n",
      "Epoch 255/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8382 - mae: 8.1207 - val_loss: 89.7260 - val_mae: 8.0602\n",
      "Epoch 256/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9689 - mae: 8.1286 - val_loss: 89.5619 - val_mae: 8.0739\n",
      "Epoch 257/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8532 - mae: 8.1282 - val_loss: 89.6782 - val_mae: 8.0137\n",
      "Epoch 258/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7422 - mae: 8.1287 - val_loss: 89.8874 - val_mae: 7.9938\n",
      "Epoch 259/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7383 - mae: 8.1126 - val_loss: 89.4141 - val_mae: 8.0346\n",
      "Epoch 260/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8286 - mae: 8.1301 - val_loss: 89.7582 - val_mae: 8.0167\n",
      "Epoch 261/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0335 - mae: 8.1294 - val_loss: 89.4250 - val_mae: 8.0402\n",
      "Epoch 262/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7870 - mae: 8.1216 - val_loss: 89.7523 - val_mae: 8.0084\n",
      "Epoch 263/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9747 - mae: 8.1293 - val_loss: 89.4822 - val_mae: 8.0271\n",
      "Epoch 264/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1664 - mae: 8.1363 - val_loss: 89.5966 - val_mae: 8.0256\n",
      "Epoch 265/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9404 - mae: 8.1310 - val_loss: 89.3534 - val_mae: 8.0592\n",
      "Epoch 266/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.3025 - mae: 8.1038 - val_loss: 89.9539 - val_mae: 8.0145\n",
      "Epoch 267/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9875 - mae: 8.1270 - val_loss: 89.4219 - val_mae: 8.0874\n",
      "Epoch 268/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0262 - mae: 8.1381 - val_loss: 89.7973 - val_mae: 8.0012\n",
      "Epoch 269/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8824 - mae: 8.1292 - val_loss: 89.4949 - val_mae: 8.0697\n",
      "Epoch 270/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.3714 - mae: 8.1333 - val_loss: 89.5293 - val_mae: 8.0305\n",
      "Epoch 271/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8190 - mae: 8.1272 - val_loss: 89.4358 - val_mae: 8.0501\n",
      "Epoch 272/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7515 - mae: 8.1267 - val_loss: 89.5184 - val_mae: 8.0558\n",
      "Epoch 273/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8488 - mae: 8.1202 - val_loss: 89.4234 - val_mae: 8.0690\n",
      "Epoch 274/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1725 - mae: 8.1416 - val_loss: 89.7910 - val_mae: 8.0130\n",
      "Epoch 275/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.6595 - mae: 8.1218 - val_loss: 89.5285 - val_mae: 8.0165\n",
      "Epoch 276/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8894 - mae: 8.1338 - val_loss: 89.9057 - val_mae: 8.0190\n",
      "Epoch 277/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7745 - mae: 8.1259 - val_loss: 89.7670 - val_mae: 8.0004\n",
      "Epoch 278/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1511 - mae: 8.1270 - val_loss: 89.5813 - val_mae: 8.0287\n",
      "Epoch 279/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9250 - mae: 8.1353 - val_loss: 89.8346 - val_mae: 7.9991\n",
      "Epoch 280/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.6351 - mae: 8.1180 - val_loss: 89.9179 - val_mae: 7.9952\n",
      "Epoch 281/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9664 - mae: 8.1232 - val_loss: 89.6179 - val_mae: 8.0567\n",
      "Epoch 282/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9268 - mae: 8.1283 - val_loss: 89.4753 - val_mae: 8.0577\n",
      "Epoch 283/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.5583 - mae: 8.1229 - val_loss: 89.4451 - val_mae: 8.0482\n",
      "Epoch 284/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7723 - mae: 8.1250 - val_loss: 89.7424 - val_mae: 8.0109\n",
      "Epoch 285/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7037 - mae: 8.1196 - val_loss: 90.2617 - val_mae: 8.0120\n",
      "Epoch 286/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8917 - mae: 8.1207 - val_loss: 89.3589 - val_mae: 8.0568\n",
      "Epoch 287/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9867 - mae: 8.1382 - val_loss: 89.3982 - val_mae: 8.0590\n",
      "Epoch 288/5000\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 90.7466 - mae: 8.1279 - val_loss: 89.9361 - val_mae: 7.9939\n",
      "Epoch 289/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7438 - mae: 8.1173 - val_loss: 89.5293 - val_mae: 8.0438\n",
      "Epoch 290/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1745 - mae: 8.1382 - val_loss: 89.7207 - val_mae: 8.0475\n",
      "Epoch 291/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2456 - mae: 8.1363 - val_loss: 89.5607 - val_mae: 8.0451\n",
      "Epoch 292/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.5607 - mae: 8.1235 - val_loss: 89.3113 - val_mae: 8.0199\n",
      "Epoch 293/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8816 - mae: 8.1248 - val_loss: 89.5102 - val_mae: 8.0275\n",
      "Epoch 294/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.1228 - mae: 8.1337 - val_loss: 89.4728 - val_mae: 8.0615\n",
      "Epoch 295/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9065 - mae: 8.1358 - val_loss: 89.9759 - val_mae: 8.0069\n",
      "Epoch 296/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9202 - mae: 8.1255 - val_loss: 89.5717 - val_mae: 8.0240\n",
      "Epoch 297/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9823 - mae: 8.1323 - val_loss: 89.9692 - val_mae: 7.9983\n",
      "Epoch 298/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7465 - mae: 8.1150 - val_loss: 89.6326 - val_mae: 8.0345\n",
      "Epoch 299/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.6600 - mae: 8.1298 - val_loss: 89.6229 - val_mae: 8.0302\n",
      "Epoch 300/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7773 - mae: 8.1248 - val_loss: 89.9165 - val_mae: 8.0223\n",
      "Epoch 301/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.4862 - mae: 8.1180 - val_loss: 89.6621 - val_mae: 7.9958\n",
      "Epoch 302/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7327 - mae: 8.1091 - val_loss: 89.3695 - val_mae: 8.0401\n",
      "Epoch 303/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7827 - mae: 8.1298 - val_loss: 89.5011 - val_mae: 8.0182\n",
      "Epoch 304/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.6519 - mae: 8.1196 - val_loss: 89.5196 - val_mae: 8.0290\n",
      "Epoch 305/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8288 - mae: 8.1251 - val_loss: 89.5100 - val_mae: 8.0402\n",
      "Epoch 306/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.6311 - mae: 8.1180 - val_loss: 90.0570 - val_mae: 8.0179\n",
      "Epoch 307/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7252 - mae: 8.1206 - val_loss: 89.6768 - val_mae: 8.0447\n",
      "Epoch 308/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.2616 - mae: 8.1386 - val_loss: 89.8307 - val_mae: 8.0140\n",
      "Epoch 309/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7722 - mae: 8.1210 - val_loss: 89.6317 - val_mae: 8.0240\n",
      "Epoch 310/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7781 - mae: 8.1384 - val_loss: 89.9854 - val_mae: 7.9863\n",
      "Epoch 311/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7714 - mae: 8.1151 - val_loss: 89.3679 - val_mae: 8.0557\n",
      "Epoch 312/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0179 - mae: 8.1360 - val_loss: 89.6026 - val_mae: 7.9957\n",
      "Epoch 313/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.0555 - mae: 8.1359 - val_loss: 89.3603 - val_mae: 8.0647\n",
      "Epoch 314/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8349 - mae: 8.1400 - val_loss: 89.6039 - val_mae: 8.0018\n",
      "Epoch 315/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9346 - mae: 8.1218 - val_loss: 89.4103 - val_mae: 8.0404\n",
      "Epoch 316/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 91.2670 - mae: 8.1316 - val_loss: 89.5979 - val_mae: 8.0346\n",
      "Epoch 317/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.1115 - mae: 8.1289 - val_loss: 89.3877 - val_mae: 8.0284\n",
      "Epoch 318/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.0485 - mae: 8.1271 - val_loss: 89.3382 - val_mae: 8.0504\n",
      "Epoch 319/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7865 - mae: 8.1130 - val_loss: 89.4872 - val_mae: 8.0634\n",
      "Epoch 320/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 91.3062 - mae: 8.1337 - val_loss: 89.3592 - val_mae: 8.0367\n",
      "Epoch 321/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7701 - mae: 8.1245 - val_loss: 89.7486 - val_mae: 8.0007\n",
      "Epoch 322/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.8661 - mae: 8.1220 - val_loss: 89.6035 - val_mae: 8.0391\n",
      "Epoch 323/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9957 - mae: 8.1203 - val_loss: 89.5115 - val_mae: 8.0594\n",
      "Epoch 324/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9875 - mae: 8.1367 - val_loss: 89.7047 - val_mae: 8.0296\n",
      "Epoch 325/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9980 - mae: 8.1251 - val_loss: 89.4456 - val_mae: 8.0738\n",
      "Epoch 326/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9641 - mae: 8.1338 - val_loss: 90.3015 - val_mae: 7.9911\n",
      "Epoch 327/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9960 - mae: 8.1189 - val_loss: 89.4653 - val_mae: 8.1020\n",
      "Epoch 328/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7771 - mae: 8.1369 - val_loss: 89.7221 - val_mae: 8.0219\n",
      "Epoch 329/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7790 - mae: 8.1233 - val_loss: 89.5801 - val_mae: 8.0391\n",
      "Epoch 330/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7289 - mae: 8.1343 - val_loss: 89.7126 - val_mae: 8.0167\n",
      "Epoch 331/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.6529 - mae: 8.1148 - val_loss: 89.7654 - val_mae: 8.0901\n",
      "Epoch 332/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9079 - mae: 8.1333 - val_loss: 90.2694 - val_mae: 8.0151\n",
      "Epoch 333/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.6546 - mae: 8.1220 - val_loss: 89.4417 - val_mae: 8.0434\n",
      "Epoch 334/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.9825 - mae: 8.1264 - val_loss: 89.4899 - val_mae: 8.0736\n",
      "Epoch 335/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.7763 - mae: 8.1320 - val_loss: 90.1542 - val_mae: 8.0147\n",
      "Epoch 336/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 90.7207 - mae: 8.1286 - val_loss: 89.8300 - val_mae: 8.0046\n",
      "Epoch 337/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9739 - mae: 8.1264 - val_loss: 89.3694 - val_mae: 8.0508\n",
      "Epoch 338/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.9363 - mae: 8.1343 - val_loss: 89.7144 - val_mae: 8.0193\n",
      "Epoch 339/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.5582 - mae: 8.1169 - val_loss: 89.7662 - val_mae: 8.0097\n",
      "Epoch 340/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8454 - mae: 8.1176 - val_loss: 89.5552 - val_mae: 8.0253\n",
      "Epoch 341/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8220 - mae: 8.1231 - val_loss: 89.8892 - val_mae: 8.0093\n",
      "Epoch 342/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 90.8535 - mae: 8.1234 - val_loss: 89.5288 - val_mae: 8.0533\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(1, activation='linear')) # output node\n",
    "model.summary() # see what your model looks like\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "\n",
    "# early stopping callback\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   patience=50,\n",
    "                   restore_best_weights = True)\n",
    "\n",
    "# fit the model!\n",
    "# attach it to a new variable called 'history' in case\n",
    "# to look at the learning curves\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    callbacks=[es],\n",
    "                    epochs=5000,\n",
    "                    batch_size=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEcCAYAAADdtCNzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABj1klEQVR4nO2dd3hUVdrAf/dOSyadFDoSOqIIAlIlgroKgqvsuqLiurp2saO4VkARUZeiIqwKrgrqqp+CQRBRmhRRutKU3gLJpNdp935/TOZmJpmWkEwScn7Pw8PklnPfe+fOec9525FUVVURCAQCgaAScn0LIBAIBIKGiVAQAoFAIPCJUBACgUAg8IlQEAKBQCDwiVAQAoFAIPCJUBACgUAg8IlQEII65c477+Srr76q9WPrk+HDh7Nx48Zab7dr164cPXoUgOeff545c+aEdGx1+frrr7njjjtqdG4gNm/ezNChQ2u9XUH9oa9vAQQNj969e2ufS0tLMRqN6HQ6ACZPnsy1114bclvvvfdenRx7rjNlypRaaefEiRNcfvnl7N69G73e9XO/9tprq/UdCpouQkEIqrB9+3bt8/Dhw3nppZcYNGhQleMcDofW6QgEgnMPYWIShIzbhPDOO+8wePBg/vWvf5Gfn88999zDgAED6NevH/fccw+nT5/Wzrn11lv5/PPPAfjyyy+56aabmD59Ov369WP48OGsXbu2RsceP36cW265hd69e/OPf/yDyZMnM2HCBJ9yhyLjrFmzGDt2LL179+aOO+4gJydH27948WKGDRtG//79mTt3rt/ns2PHDgYPHozT6dS2rVy5ktGjRwOwa9cubrzxRvr27cuQIUOYMmUKNpvNZ1tPPfUUM2fO1P5+7733GDJkCEOGDOGLL77wOnbNmjVcd911XHzxxaSlpfHmm29q+8aNGwdAv3796N27N9u3b9eerZtt27bxl7/8hT59+vCXv/yFbdu2hfxsAnHw4EFuvfVW+vbtyzXXXMMPP/yg7Vu7di0jR46kd+/eXHrppcyfPx+AnJwc7rnnHvr27csll1zCzTffjKIoIV1PUPsIBSGoFhaLhfz8fFavXs2LL76IoiiMGTOG1atXs3r1akwmU0DzyK5du0hNTeWnn37izjvv5JlnnsFftZdAx06YMIGePXuyefNmxo8fz5IlS/xeMxQZly5dyrRp09i0aRN2u50FCxYAcODAASZPnsyrr77Kjz/+SF5enpdy8aRXr15ERkby008/advS09M1BSHLMv/617/46aef+PTTT9m0aRMff/yxX7ndrFu3jgULFrBgwQK+++47Nm3a5LU/MjKS6dOns2XLFv7zn//wySef8P333wOwcOFCAH755Re2b9/uZT4EyMvL45577uHWW29l8+bN3H777dxzzz3k5uYGfTaBsNvt3HvvvQwePJiNGzfy7LPPMmHCBA4dOgTAM888w5QpU9i+fTtLly5lwIABALz//vs0b96cTZs2sWHDBh577DEkSQp6PUHdIBSEoFrIssxDDz2E0WgkIiKChIQErrrqKiIjI4mOjua+++7jl19+8Xt+q1at+Nvf/oZOp+P6668nKysLi8VSrWNPnTrFr7/+qsnRt29fhg8f7veaocg4ZswYUlNTiYiI4Oqrr2bv3r0AfPvtt1x22WX069cPo9HIww8/jCz7/9lcc801LF26FICioiLWrVvHNddcA8AFF1xAr1690Ov1tGnThhtvvDHgs3KzfPlyxowZQ5cuXTCbzYwfP95rf//+/enatSuyLNOtWzeuueYafv7556Dtgmv2cd5553Hdddeh1+sZNWoUHTp0YPXq1UGfTSB27txJSUkJd999N0ajkYEDBzJs2DC++eYbAPR6PQcOHKCoqIi4uDh69Oihbc/KyuLUqVMYDAb69u0rFEQ9IgzIgmqRkJCAyWTS/i4tLWXatGn8+OOP5OfnA1BcXIzT6dQc254kJSVpnyMjIwEoKSnxeS1/x+bm5hIXF6dtA2jZsiUZGRk+2wlFxuTkZK9ruWXKzMykRYsW2j6z2Ux8fLzP6wCMHj2asWPHMnnyZFauXMn5559P69atATh8+DCvvPIKv/32G6WlpTidTq1jDERmZiYXXHCB9re7PTc7d+7k9ddf548//sBut2Oz2bj66quDtutuu1WrVl7bWrVqxZkzZ7S//T2bYO22aNHCS5l6tvvGG28wd+5c/v3vf9O1a1cef/xxevfuzT//+U/eeustLcrqxhtv5O677w7pXgS1j5hBCKpF5dHcggULOHz4MJ999hnbtm1j0aJFAH7NRrVBcnIy+fn5lJaWatv8KYezlTElJcXLpFRaWkpeXp7f4zt16kSrVq1Yt24dS5cuZdSoUdq+SZMm0aFDB1asWMG2bdt49NFHQ5bB8/5OnTrltf/xxx/n8ssvZ+3atWzdupWxY8dq7QYbfaekpFRpLyMjg+bNmweVK1i7p0+f9vIfeLbbs2dP5s6dy8aNG7niiit45JFHAIiOjuapp57ihx9+YN68ebz//vtVTGqC8CEUhOCsKC4uxmQyERsbS15eHm+99VadX7N169ZccMEFvPnmm9hsNrZv3+5lEqlNGa+66irWrFnDli1bsNlsvPHGG0GdpqNGjeLDDz/kl19+8RrJFxcXExUVRVRUFAcPHuSTTz4JSYarr76ar776igMHDlBaWlpF/uLiYuLi4jCZTOzatUszcQE0a9YMWZY5fvy4z7bT0tI4cuQI6enpOBwOli1bxoEDB7jssstCks0fPXv2JDIykvfeew+73c7mzZtZtWoVI0eOxGaz8fXXX1NYWIjBYCAqKkqbya1evZqjR4+iqirR0dHodLqAJj1B3SKevOCsuO2227BarQwYMIAbb7yRSy+9NCzXff3119mxYwf9+/dn1qxZjBw5EqPRWOsydu7cmeeff54JEyZw6aWXEhsb62Vy8sWoUaP4+eefGTBgAM2aNdO2T5w4kaVLl3LxxRfz3HPPMXLkyJBkSEtL47bbbuO2227jyiuv1By6bl544QXeeOMNevfuzZw5cxgxYoS2LzIyknvvvZebbrqJvn37smPHDq9zExIStJF6//79ee+995g3b56X3DXBaDQyd+5c1q1bx4ABAzRHf8eOHQFYsmQJw4cP5+KLL+bTTz/l1VdfBeDo0aPcfvvt9O7dmxtvvJGbbrqJ/v37n5UsgpojiQWDBOcCjzzyCB06dOChhx6qb1EEgnMGMYMQNEp27drFsWPHUBSFdevW8cMPP3DFFVfUt1gCwTmFiGISNEosFgsPPvggeXl5tGjRgkmTJnH++efXt1gCwTmFMDEJBAKBwCfCxCQQCAQCnwgFIRAIBAKfCAUhEAgEAp+cE07q3NxiFKV6rpTExGiys4vqSKK6obHJ3NjkBSFzOGhs8kLjkzmYvLIskZAQFbSdc0JBKIpabQXhPq+x0dhkbmzygpA5HDQ2eaHxyVwb8goTk0AgEAh8ErYZxJo1a5g9ezYOh4O4uDimTZuGJEk88MAD2jGFhYUUFRWFXKpYIBAIBHVHWBREfn4+EydO5NNPPyU1NZUlS5YwadIk5s+f77XQy9SpU71W4xIIBOFBVVVyc7Ow2cqAujWlZGbKjW6VuMYms0teFaMxgoSE5BqvqREWBXH06FGSkpJITU0FXMXHnnzySXJycrSiYDabjfT0dG3pQYFAED6KivKRJInmzdsgSXVredbrZRyOxtPZQuOTWa+Xsdsd5OVZKCrKJyYmvmbt1K5YvklNTcVisbBr1y569uxJeno64KoP71YQq1atonnz5iEtoHK2bNp9msXrN5GVW0pirIkxaR0Z2CNwhU6B4FymtLSIZs2a17lyEIQPSZKJiUkgJ+dMw1YQMTExzJw5k2nTpmG1Whk6dCixsbHo9RWX/7//+z/+8pe/1Kj9xMTokI9ds/U4H367H6vdZcrKLrDy4bf7iY2J4LI+bWt0/XCSnBxT3yJUi8YmLzRNmTMzVUwmY9iW99TrG58iamwy6/UyOp0RUGv8ftRLLSaLxcKwYcPYvHkzZrOZM2fOcNVVV7F69WoSEhKq3V52dlHIIV1PvL2B7AJrle2JsSZeu39wta8dTpKTY8jKKqxvMUKmsckLTVfm06eP0qLFebUkUWAam7kGGp/MnvL6+m5lWQppYB22KKasrCySk5NRFIUZM2YwduxYzGYzAF999RVpaWk1Ug7VxZdyCLRdIBCEl7vuug273Y7DYef48WOkproWGerSpStPP/1CSG0sXvwFVquVG2+8JeBx69evZefOHTzwwMNnLbeb8ePv5qabbmXw4PAsnlWXhE1BzJo1i23btmG32xk8eDATJkzQ9n311Vc888wzYZEjMdbkdwYhEAhCZ9Pu03y59iDZBdZa9eW9++4HAGRknOLOO2/lv//9uMoxDofDy0Rdmeuu+2tI1xoyJI0hQ9JqJmgTIGwKYurUqX73rVixIlxiMCatIx8s34fNY7po1MuMSesYNhkEgsbOpt2nvX5H2QVWPli+D6DOAj7++tfRjBr1Z7Zu/YVWrVpz9933M2nSMxQXF2Oz2Rg0aDD33++aCcyf/x9KS0sZP/4Rli1LZ+XKb4mJieXQoYPExETz0kuvkpiYxLJl6Wzc+CMvvfQq27Zt4Y03ZnD++T3YvftXQGLy5Jdp394Vffmf/8xh1aqVxMbG0bt3H7Zu/YX58z8KKHNOTjavvTaNU6dOoKoqN910KyNGjCq3pLzKtm2/YDAYMZsjmTt3Abm5OUya9Cy5udkA9O17CQ899HidPM9QOCdKbVQH98u7eP1hEcUkENSQL9ce9BpkAdgcCl+uPVinvyWLxcKbb/4HAKvVyvTpMzGbzTgcDh57bDw//bSRAQMGVTlv7949fPDBJzRv3oLp01/iiy/+xz33PFDluMOHD/L008/z5JPP8MEH8/ngg/m88MJL/PjjWjZuXM9///sJJpOJZ5+dGJK8s2a9TocOHZk27XUsFgv//OctdO3aDYfDwZYtP/Pxx18gyzIFBQUAfPfdclq0aMHs2W8DaNvriyanIMClJK69rHOjc0YKBA2F+vLlXX31NdpnRVF4++3Z/PrrLkAlOzubP/743aeC6NnzIpo3dymuHj0u4JdfNvtsv1278+jSpVv5cReyYcOPAGzduoXhw68gMjISgBEjruG//w2es7Vly8+MH/8IAElJSQwcOIRt27Zw9dWjUBQnr7zyIhdf3JdBgy7Vrvm//33MnDmz6dXrYvr3Hxjag6kjGlfclkAgaBD489nVtS/PbI7UPv/vf4soLCzgnXf+ywcffMqll16GzeZbQRmNRu2zLOv8VmwwGk0ex8kex6k1DgGufJ4kSURHR/PRR58xfPiVHDx4gFtv/RvZ2RYuuKAn77+/iK5du7FixTIefPCeGl2zthAKQiAQVJsxaR0xVsoLCLcvr7CwkMTEJEwmE1lZmaxfv7bOrnXxxf1Yvfp7ysrKUBSFFSuWhXRe376X8PXXXwGQnW1h06YN9O7dl9zcXKxWKwMGDOLee8cTHR3NqVMnOXXqJFFR0VxxxVU8+OCj7N+/r15LfDRJE5NAIDg73H6GuohiCpUbbhjLc89N5PbbbyYlpTl9+vSrs2sNHZrGzp07+Mc/biIpKZkePS6ksDC4ifqRRybw2msvc9ttY1FVlXvvHU+HDh3Zv38f06e/hNPpxOl0MmDAIHr0uJDly5fy6acL0en0qKrCE0/8C1muv3F8vSTK1TbVSZRz01QTosJJY5MXmq7MIlEuMHq9TEFBIWZzFIqi8MorL5KUlMzdd99f36L5pNElygkEAkFj5sUXX+D06VNYrVa6du3OLbf8vb5FqnOEghAIBIIQmDbt9foWIewIJ7VAIBAIfCIUhEAgEAh8IhSEQCAQCHwiFIRAIBAIfCIUhEAgEAh8IhSEQCBoUDz22IMsXvx/XttUVeWGG65lx45tfs+bOnUS//d//wNc60H873+LfB63bFk6zz77ZFA51q1bw549v2l/79u3h8mTnw3lFkLmr38dzaFDB2q1zdpEKAiBQNCguOaaa1m2LN1r2/btW9HpdPTqdXFIbVx33V+DLhYUjB9/XMPevbu1v7t1O58XXnjprNpsbIg8CIFA4IXp1MdEnFpYJ22XtRqHs924gMcMHXoZM2a8wuHDh0hN7QDAN998zciRozl48AD//vcrlJWVYrPZuPba6/nb326u0obnehB2u52ZM19l+/atJCen0K5de+04f+1t3ryJ9evXsWXLz6SnL+Hmm8eRlJTCnDmztTUgli9fyieffIQkSbRq1YYnn3yahIRmAdefCMSJE8d57bWXycvLRafTcffdDzBgwCDKysp46aUXOHLkEDqdnnbtzuPFF1/h2LEjTJ06ubw+lJMRI0Zz8823VvMbCYxQEAKBoEFhMBi48sqrWb48nfvvf5iSkmJ+/HEt9947nqioKGbNehuj0UhJSQl3330bl1wyUFvUxxdLlvwfGRmn+Oijz3A4HDzwwF20bNkSgJYtW/psr3//gQwZMpRu3brzl7/ciF4v8/PPP2ttHjp0gHnz3mL+/IUkJSXx7rtzmTnzNaZMmQaEvv6EJ5MnP8uf/3w9o0Zdx+HDhxg//i4WLvyCXbt2UFhYyMKFnwMVa0R8+eUXDBw4mH/8406v7bVJ2BTEmjVrmD17Ng6Hg7i4OKZNm0bbtm2xWq28/PLLbNq0CZPJRK9evXjxxRfDJZZAIKiEtdXNWFtVHZXXFqF0Otdc82cmTHiQu+9+gB9+WEnPnheRnJxCTk42b731CgcO/I4kyVgsWRw48HtABbFt21ZGjBiFXq9Hr9dz1VUj2LVrBwBlZWXVbs/V5hYGDhxMUpJrVvDnP4/hH/+oeGahrj/hpqSkmAMHfmfkyGsBSE3tQKdOXdm9+1c6derMsWNH+Pe/p9O7dx8GDRoCQK9evZkzZzZ2u52LL+7LxRf3DfxQa0BYFER+fj4TJ07k008/JTU1lSVLljBp0iTmz5/Pa6+9hslkYsWKFUiShMViCYdIAoGgAdO5cxcSE5PYvHkTy5Z9rZmR/vOfOTRrlsiCBYvQ6/U8+ugD2Gy2gG0Fqkdak/Zcbfpa56Hic6jrTwSTUZIkWrduw6JFn7Nlyy/89NMG3nlnDh988CmXXXY5F1zQk59//omFC//LN998zfPP1+7gOixO6qNHj5KUlERqqksrp6WlsX79ek6fPs3ixYt5+OGHtYft1sgCgaBpc80117JgwTscP36MIUPSACgqKiQlpTl6vZ5Dhw6wc+eOoO307duPb79dhsPhwGotY+XKb7V9gdqLioqiqKjIZ5t9+vRj06YNZGe7BrTp6Yvp2/eSGt9rVFQ0nTp1YfnypQAcPXqEgwd/5/zzLyAz8wyyrGPo0Mt46KHHycvLpbCwgBMnjtOsWSIjR47m9tvvYs+e3UGuUn3CMoNITU3FYrGwa9cuevbsSXq6K0Lh2LFjxMfH89Zbb7F582aioqJ4+OGH6du39qdKAoGgcXHllSOYM+cN/vznMRgMBgBuu+2fvPji83z33XJat25Nr169g7Zz7bVjOHDAtWpbSkpzevXqQ0bGyaDtXXXVSKZOnczq1T9oTmo3HTp05J57HuDRRx8od1K35oknnj6r+33hhZd47bWX+eyzj9HpdDz77BQSEhLYtGkD8+a9BYCiOBk37h8kJSXz4YcL+O67bzEY9EiSxMMPP35W1/dF2NaD2LhxI2+++SZWq5WhQ4eyaNEi5syZw6233srrr7/O6NGj2blzJ/feey8rV64kOjp4rXKBQFA77N69h1atwrMehCC8nDp1lB49zq/RuWFzUg8aNIhBg1yLiVssFubPn0/r1q3R6/WMGjUKgIsuuoiEhAQOHz7MhRdeGHLbYsGghkljkxearsyKooRtEZ/GumBQY5LZU15FUaq8H6EuGBS2RLmsrCzAJeyMGTMYO3YsrVu3pn///mzYsAGAw4cPk52dzXnniZGMQCAQ1Ddhm0HMmjWLbdu2YbfbGTx4MBMmTABg8uTJPP3000yfPh29Xs+rr75KbGxsuMQSCATlqKpaJTJH0Lg5Ww9C2BTE1KlTfW5v27YtH330UbjEEAgEPnCFYjrQ6w31LYqgFnE6Hciyrsbni1pMAoGAyMhoCgvzUNXGY2cXBEZVFQoLc4mMrHnAjyi1IRAIiI6OIzc3izNnTgB1G9goyzKK0rgUUWOT2SWvitEYQXR0XI3bEQpCIBAgSRLNmqUEP7AWaKqRYuGktuQVJiaBQCAQ+EQoCIFAIBD4RCgIgUAgEPhEKAiBQCAQ+EQoCIFAIBD4RCgIgUAgEPhEKAiBQCAQ+EQoCIFAIBD4RCgIgUAgEPhEKAiBQCAQ+EQoCIFAIBD4RCgIgUAgEPhEKAiBQCAQ+EQoCIFAIBD4JGzlvtesWcPs2bNxOBzExcUxbdo02rZty/DhwzEajZhMJgAmTJjApZdeGi6xBAKBQOCHsCiI/Px8Jk6cyKeffkpqaipLlixh0qRJzJ8/H4A33niDLl26hEMUgUAgEIRIWExMR48eJSkpidTUVADS0tJYv349OTk54bi8QCAQCGpAWBREamoqFouFXbt2AZCeng5ARkYG4DIrjR49mkmTJlFQUBAOkQQCgUAQBElV1bpdgLacjRs38uabb2K1Whk6dCiLFi1i4cKFxMbG0rJlS2w2G1OnTqW4uJjXX389HCIJBAKBIABhUxCeWCwWhg0bxubNmzGbzdr2/fv3c99997Fq1apqtZedXYSiVO82Gtsas9D4ZG5s8oKQORw0Nnmh8ckcTF5ZlkhMjA7aTtjCXLOysgBQFIUZM2YwduxYAAoLXTehqirLli2je/fu4RJJIBAIBAEIW5jrrFmz2LZtG3a7ncGDBzNhwgQyMzN58MEHcTqdKIpCx44deeGFF8IlkkAgEAgCEDYFMXXq1Crb2rZty+LFi8MlgkAgEAiqgcikFggEAoFPhIIQCAQCgU+EghAIBAKBT8Lmg2horNl6nP8u3U12gZXEWBNj0joysEeL+hZLIBAIGgxNUkFs2n2aD7/dj9XuBCC7wMoHy/cBCCUhEAgE5TRJE9OXaw9qysGNzaHw5dqD9SSRQCAQNDyapILILrBWa7tAIBA0RZqkgkiMNVVru0AgEDRFmqSCGJPWEZNB57XNqJcZk9axniQSCASChkeTdFIP7NGC2JgIEcUkEAgEAWiSCgLgsj5t6dEuvr7FEAgEggZLkzQxCQQCgSA4QkEIBAKBwCdCQQgEAoHAJ0JBCAQCgcAnQkEIBAKBwCdCQQgEAoHAJ2FTEGvWrOH6669n9OjRjBs3juPHj3vtf+utt+jatSu///57uEQSCAQCQQDCoiDy8/OZOHEiM2bMID09nRtuuIFJkyZp+3fv3s2OHTto1apVOMQRCAQCQQiERUEcPXqUpKQkUlNTAUhLS2P9+vXk5ORgs9mYMmUKL7zwApIkhUMcgUAgEIRAWDKpU1NTsVgs7Nq1i549e5Keng5ARkYGy5Yt49prr6Vt27bhEKUKm3af5su1B0XJDYFAIKhEWBRETEwMM2fOZNq0aVitVoYOHUpsbCylpaX8+uuvTJgw4azaT0yMrtF5u4/lVVk46MNv9xMbE8FlfepHYQUjOTmmvkWoFo1NXhAyh4PGJi80PplrQ15JVVW1FmSpFhaLhWHDhnH//fezaNEijEYjAKdPnyYxMZFp06YxZMiQkNvLzi5CUap3G8nJMfxj8rc+14BIjDXx2v2Dq9VeOEhOjiErq7C+xQiZxiYvCJnDQWOTFxqfzMHklWUppIF12Ir1ZWVlkZycjKIozJgxg7Fjx3Lfffdx3333accMHz6cefPm0aVLl7DIJBYOEggEAv+ETUHMmjWLbdu2YbfbGTx48Fmblc6WNVuP+90nFg4SCASCMCqIqVOnBj1m1apVYZDExYfL9/rdJxYOEggEgiacSW3JLfW7T0QxCQQCQRNWEEkJkT63C/OSQCAQuGiyCuLvI7pj1HvfvliXWiAQCCposgrisj5tuW1EN23GEBWhw2iQeTd9D0+8vYFNu0/Xs4QCgUBQvzTZNamhwtfw8cr9FJc5te3ZBVY+WL7P6xiBQCBoajTZGQS4ymx8sHyfl3JwY3MofLn2YD1IJRAIBA2DJq0gvlx7EJtD8btfJMwJBIKmTJNWEMEUgIhoEggETZmQFcRPP/2kLfKTmZnJxIkT+de//kVWVladCVfXBFIAIqJJIBA0dUJWEJMnT0an0wEwffp0HA4HkiTx3HPP1Zlwdc2YtI5VQl0BoiP13Daim3BQCwSCJk3IUUxnzpyhVatWOBwO1q9fz6pVqzAYDFx66aV1KV+d4lYAYj0IgUAgqErICiI6OhqLxcIff/xBx44diYqKwmaz4XA46lK+OqeyknBHLgklIRAImjohK4hx48bx17/+FbvdztNPPw3Atm3b6NChQ50JFw7coa7uaCaRAyEQCAQuQlYQd999N1deeSU6nY527doB0Lx5c1566aU6Ey4c+Ap1dedACAUhEAiaMtUKc01NTdWUw08//YTFYqFr1651Ili4CLRokCi3IRAImjIhK4hx48axdetWAN555x0ee+wxHnvsMebNm1dnwoWDQKGuHyzfJ5SEQCBosoRsYvrjjz/o1asXAJ9//jkfffQRZrOZm266iXvvvbeu5KtzxqR19PJBeGJzKHy8cj8gIp0EAkHTI2QFoSgKkiRx7NgxVFWlY0dXEll+fn5I569Zs4bZs2fjcDiIi4tj2rRptG3blvvvv58TJ04gyzJms5nnnnuO7t271+xuaoC7o383fY/P/cVlTt5ftheHUwVcpqd30/fwyfe/c9MVXYSiEAgE5ywhK4g+ffowZcoUsrKyuPLKKwE4duwYCQkJQc/Nz89n4sSJfPrpp6SmprJkyRImTZrE/PnzmT59OjExMQB8//33PP3003z11Vc1vJ2aMbBHC22G4Au3cvCkqNQhop0EAsE5Tcg+iGnTphEbG0vXrl0ZP348AIcOHeLvf/970HOPHj1KUlISqampAKSlpbF+/XpycnI05QBQVFSEJEnVvYdaoSZlNUTFV4FAcC4T8gwiISGBxx57zGvbZZddFtK5qampWCwWdu3aRc+ePUlPTwcgIyODZs2a8cwzz7BhwwZUVeW9994LXfpaZGCPFnzy/e8UlVYv8U9UfBUIBOcqkqqqVe0nPrDb7cydO5clS5aQmZlJSkoKf/7zn7n33nsxGo1Bz9+4cSNvvvkmVquVoUOHsmjRIhYuXOgVJrt48WK++eYb3n333Zrf0VmwZutx/v3xtmqdk5wQyYJn/1RHEgkEAkH9EbKCePnll9m1axfjx4+nVatWnDp1irfffpsLLrhAy6wOFYvFwrBhw9i8eTNms9lrX8+ePVm7dm1Ivg032dlFKEpIt6GRnBxDVlZhle0PzV4X8izCqJfDWtTPn8wNlcYmLwiZw0Fjkxcan8zB5JVlicTE6KDthOyD+Pbbb5k7dy5DhgyhQ4cODBkyhLfeeovly5eHdL67LLiiKMyYMYOxY8eiqioZGRnaMatWrSIuLo74+PhQxap1brqiS0jHJcaaRMVXgUBwThOyD8LfRCPECQizZs1i27Zt2O12Bg8ezIQJEygsLOThhx+mtLQUWZaJi4tj3rx59eaoBpcvwl/Iqxv3WhFCOQgEgnOZkBXE1VdfzX333ccDDzxAq1atOHnyJHPnzmXEiBEhnT916tQq20wmE5999lno0oaJxFhTQOezqNUkEAiaAiEriCeeeIK5c+cyZcoUMjMzad68OSNHjsRms9WlfPVCoOxqNyJ6SSAQnOuErCCMRiMPP/wwDz/8sLbNarXSq1cvnnzyyToRrr6ovEaEL8R61QKB4FwnZAXhC0mSQvZBNDYG9mjBwB4tqqwXAWK9aoFA0DQ4KwUB1KtDORyIZUkFAkFTJaiC2LRpk999dru9VoVpiGzafVpTDrKEtixpz7xnSUlMoLjba/UtokAgENQJQRXEM888E3B/y5Yta02YhkZl85I7Fy+7wIotawcljgToVo8CCgQCQR0SVEGsWrUqHHI0SHwtR+pGwkl2fjHNwiyTQCAQhItqLTna1AgUyipLCorz3DexCQSCpotQEAEIFMqqk5wYz9rFLxAIBA2XJq8g5NLjJK+MxZj5TZV9Y9I6YtT7fkQ6SUFSHdzxyiqeeHuDWLtaIBCcczR5BaEv2A5ARMbHVfYN7NGC20Z002YScnlEb1SEDp3kBNUJuExRHyzfJ5SEQCA4pxBGEpzl/+t87nUnzHnyxNsb0OFEJ1U4sEV9JoFAcK7R5BWEVD4LUKXQJ1OunAgnMkqV7U+8vUEk0gnOGTzzgESSaNOjySsIt5kIyfcMojKbdp9GwuWD0EnOKvvd5iZA/JAEjZrKeUDi3W56NHkfRHUUxKbdp1mwdA8qlJuYqioIqDA3CQSNGV95QOLdbloIBaGW/wBCUBBfrj2IszybWpYUZEmUAxecu/h7h8W73XRo8gpC80H4cVJ74vnD0ElOZHzPIMAV8SSimgSNGX95QKLUfdMhbApizZo1XH/99YwePZpx48Zx/PhxcnNzueuuu7jqqqsYPXo048ePJycnJ1wilRO6icnzh6GTnAFnEIqKCH0VNGp85QGJUvdNi7AoiPz8fCZOnMiMGTNIT0/nhhtuYNKkSUiSxJ133smKFStIT0+nbdu2vP766+EQqQLV4fo/hCimMWkd0ZXnQsgoXmGuvhD2WkFjpnIeUGKsidtGdBMO6iZEWKKYjh49SlJSEqmpqQCkpaXx5JNPoigK/fv3147r1asXn3zySThE0pCU8iVTQ5hBuH8YH6/cH9TE5EbYawWNGV95QIKmQ1gURGpqKhaLhV27dtGzZ0/S09MByMjIoFkzVz1URVH45JNPGD58eDhEqkBxdeBqiGGuA3u0YOD5Kcjfq0FnEODKun7i7Q0ijlzQ4BA5DoJghEVBxMTEMHPmTKZNm4bVamXo0KHExsai11dc/sUXX8RsNjNu3Lhqt5+YGF0juZKTYyDDFZZkjjRhTo4J7USna9YRTEHoZAmrXaG4zKWEsgusfPjtfmJjIrisT9uay9yIaGzyQtOQec3W43z47X6s9opyMWf7blaHpvCM65vakDdsiXKDBg1i0KBBAFgsFubPn0/btq4Xcfr06Rw9epR58+Yhy9V3i2RnF6Eo1VsbOzk5hqysQqKKCjADpSUlFGUVhnays5RkQC/7VxCyBJEmHUWlDq/tVruTeV/upEe7eKB6ozi3zI2FxiYvNB2Z/7t0t6Yc3FjtTv67dLf2btYVTeUZ1yfB5JVlKaSBddgURFZWFsnJySiKwowZMxg7dixms5mZM2fy22+/8c4772A0GsMlTgXlJiYtYS4EpHLHtozD7zGKShXl4Ka4zKlFN4lMVUF9IHIcBKEQNgUxa9Ystm3bht1uZ/DgwUyYMIE//viDefPm0b59e8aOHQtAmzZtmDNnTrjEQnKW/yCUaiz+o1Z0/EmxBiwFVc+NitARYdT7/cG9m74HWapYxtRNXRT9E7ZmQWUSY00+302R4yDwJGwKYurUqVW2de7cmf3794dLBJ9IqrX8/+ooiIrZxpih5zH/mwNahrUbq13hku6JrN5+ym8z/qxitTmKE/V0BL4Yk9bR670AkeMgqEqTzqQ25KxDV3LI9Yfq31xUGcnj2AHdk4mMqKpnHU6VXQezkaSayVZbCXaino7AFyLHQRAKTbqaa/zWURV/VENBeB4rqQ6/voazmQm4zUye5qHkhEiuG5JaZXsgs5GwNQv8IXIcBMFougpC9bbvSDVUEKjOoPbcmnTG2QXWKuahrNxSPli+jwMn8tjw62kvs9G76Xt4N31PFWXhTzZZgjteWeV1fG35Ktzt5BRYaSZ8HgJBo6XpKghnmfff1XBSS4q3gvBlz9VJrrBBf7OLYERF6Pyah9buOBXQf+H2MQCU2Xxf332+W7ms33WKgycLqiidT77/nZuu6BJyB9+QfB7COS8QnB1NV0E4ir3+rNYMwnMlOdWhdTruzigqQofVrtRYOYArFLa4zHfobbCUD5tD4eOV+7E71CoKxh97j+b53F5U6qhWBx/I5xHOzjkcikooIMG5TtNVEE5vBVFzH4QTFW977hNvb9CypysjSVWsW9XGV3hsZfwpl5oQqIOv3Ek2FJ9HXSuqhjRTEgjqiqarIByBFYRks4CkRzXEVz3Xywfhfd6m3acDdoZmk+6sOm+jXmbwhS28fBDhwO0T8VQGPTsmVvGF+MO9PkZddJ6+RvJ1ragaykypMSF8U40PoSDKkSr5IGJ/vR3F2JzCC9+rcmplH4Qb96gyEMVlTob1bhUwP8Ifsixhcyj8vPcMknSW05BqEhWhqzJirs49KCosWOryaRSVOvw6x6MidEiSVOUYf/gayS9Yusfv8YmxprMyDXme64u6mCmF25Tl63rAWckgZlyNE6EgypFsWUQefYvSdg+AJCFbM/GbJlIpzNWNr1GlL3YdzGZY71bsOpjt5bNwVM6280Cvk7T9tWk+CgWjXkaSJGyO0K/rywzm9Cg/4u4gKkdked5bKJ2Ir2ce4DHSs2Oiz47qwIk87ftwy165I6zcyfmiupnIwTr/cHes/hSuJFe8fzWRoTZnXML3Ez6EgihHX3qI6N+fxpo8AsXc0VWjSfEzGvSs26RWvPShjh6zC6xs+PW0V2LSa59s8+soBgIqj7rE/QN8N93/qNzXOaE8i2ARWe5jAnUi1R2x7zqY7bOj8pwNeUZ4eXaEwQYAlTORfZnk3ErIn4mucscbblOWX4Vb6f2rrgy1NeMSM5HwIhREJSRnqet/1YbkR0FIfnwQoXaM4P0D27T7dEDlUFdERQT2h9w1+vwqEVrBcHeSoR4fShHe7AKr3zU1qvPMq3OsG8/vKdC5wWYblU1y/kx0NofC/KUVOS2BOtYn3t7AmLSOXHtZ7ZWhrs7zqc6xtVX7qbH6fhrrrEcoiEpISrmCcJb5VRDejmmPukzVHGm7fzD1VfYimKnK02dg1PuvGeI2ybj9B++m7yEqIrQFmEKJyIKKZ1V5xOgvB8XTJALVV1yVr71p92m/sibGmnjt/sFe2z75/vcaBxF4zmCCyfXB8n3ExkTQo118rXRC1VW44N35+TPP1Vbtp9r2/dSWP8rXAKFy2PvZmOjqC6EgKiG5E+hUW0gmJk+H9cAeLaptioGGW/bC02dgc/jvxd0dWqlVQVGr5yep5jIe5bJU1JJyjygrd0zufe5Oy50b4nBWv9N2O+j9yWq1O/loxT4vn1K4/EQ2h8KHy/dy3ZDUWjG9+OrIfaHXSYxJ61hlpuTPPOc5Ez2bKKbarEJ7NuaqQOeCdxl/X+9CY5j1QFNWEM4Sn5u1GYRirVivuvIxAcJcQx2BeY6eamL6aIgoZ5vgUQ3cmd4V1/Z+pp4zBXenVdNOO9h5RaUOL3NRXSiHQErHklsatChjqKPkyiZFf7Mmk0FmYI8WPDR7nV9lUrkTdCuK6iy+Eyy0GiqqFlQuHRMMX7O8UDvuYM87lNljY/jNN10FUT6DKG3zT/T52zAUbndtd5aCqrrMS0qZ73O9FhfyfhH8TaUHX9jCy0EZbPotqD42h1KtGVxjwmr3/26oBDa9BBrp+lIcniP+O15Z5bPd4jKn332Vr19TfI3SN/x62uu3VLlqQaC6ZJXbDlRkM1jOTm2YuhrD2htNWkGospmi7jOJ3vOIpiAkpRTK14YIxQfhlRNB1RFYKCMaX6U6SqzOs864duMKU1Wx2usnEkpw9tQ0ik2i6mjWlyL1ZV4J5HcJlbPpBP2N0ncdzNZ8PoGqFgQyGQXz+wUzNZ1tgc7GsvZG01YQOrPrs1zxGDyd0yFFMVF12l+TMsqVz3lo9rpq1XIKVpX1k+9/x2qv2p7JIBEdaSS7wIqEazQqOHeozvdZObIukN8lFDw7Qc93MsZsQFXVoMmQoYzSg3XE/kxGNT3PzdkU6BRRTD5Ys2YNs2fPxuFwEBcXx7Rp02jbti3Tp09nxYoVnDx5kvT0dLp06RIegRzFqLooAFTJoG2WlFJw+x4Uq6twUuVVfwL4IGqLUJWDXidx+8juXi+bLwXlz/RitavMfdw1GgslEUxwbuMZUny2DL7Q9Q4+OGutl/+ksKSiaoG/mUuwEb47xDcU/52/kX5NznPja9Zfag2tQGdjUQ4QJgWRn5/PxIkT+fTTT0lNTWXJkiVMmjSJ+fPnc/nll/P3v/+dW265JRyiVOD0mEFIHo/BWVoxg0BxKQAPBQJUKdZXF4TyAkdH6kMuxR1K9Ie7nY9X7q9VR6tRL5/TSiecUUvhoLacpz/vPRNSzTCbQ+G9cr9BKFUF3DJ+sHxfyHXJ3AoFQs/p8TzPV66J50DsodnrQg7ScPtIKuP5ez6b8jO1SVgUxNGjR0lKSiI1NRWAtLQ0nnzySXJycujbt284RKiKoxhVFwmAKleeQXi8PIoVZG8FIXlkT1NHCsLXFNZk0PH3q7v6fTkCxWWHGofufuk37T7N/KV7zsrEABXT6WA/yrO1ddcGJoNUbT+NUS+dU8qhNqnOc3E/9eqc4/ZH3DaiW9D3y+28liWpWtF2lXNN/HE2pf0923h/2d6g5WfeTd/DgRN53HpVt7O+ZjDCsiZ1amoqFouFXbt2AZCeng5ARkZGOC7vGw8TE1JFUpfLB1ER3urTD+Exg4jbcSMRJz+qdfF8rRk8/oaLAiqHD5bvq5JQ5l7burprEA/s0YJ/jjofoz70V6TysSaDTlNSr90/mLtG+2+vvpUDUCMnfqD8EEHdk11g1RImQ3GI+1IOibEm7hp9vt/kTneuSThwOFVWbz8VdEa0evupWlu3PhBhmUHExMQwc+ZMpk2bhtVqZejQocTGxqLX187lExOjq3+SoxhjZCuSk2PgTMX5ZpMTc3yFXEkJBjBXml7messdoxwkJrn2yh24ufayGK69rHNIxy5ev8lnxMfi9Ye1NqrTnvv42JgIPly+F0tuKZIsofjpyZMTIvn7iO7asdFm16zrvfQ9LF5/mL+P6M61l3XW2svKLQ1ZjtoiOSESS26pcMR7EFP+PXn6BXwhB/ju6xOTQebhN34MKn8gcgqsnLQUB5y9WHJLXX2FH2LMhrOSoSa86/HbuqxP2yr7A8kbKmFzUg8aNIhBgwYBYLFYmD9/Pm3bVr2pmpCdXVTtlzc5uiPF+vaUZBUSWeLErSJKi/Ipy84hwd12VjaKOdbr3MjCIjxVUmlRPkUhJv6cDYESjPx1uFm5pSEnJfmiR7t4pt8zEPAfEw9w3ZBU7Vhfa2m/+dkOCgrLGNijBdPvGRiwrbpIHEyMNWn3UVtO2HMBm90RcObkDoJoqPklVrsSMEckFMwROpZtOhrwmKSESLKyCv2accde3pkFS/cErCRcF1T+bbkJlowoy1JIA+uwmJgAsrKyAFAUhRkzZjB27FjMZnO4Ll+VSz+npNPzrs+VopiCm5i8RxruAn/1ib/pdW0m4/hrKypC5/VyBssyDSbXmLSOLHhqeK3K7ulrGZPWsVqmM3CZz4b1btUokpuqQzCzmsOp8uXag0RH1u5YsnJgYH0ihSBMVm4pd7yyinfT93iZcRcs3cN9/17Nu+nhVw5uKv+2apOwKYhZs2YxYsQI/vSnP2EwGJgwYQIAL730EkOHDuX06dPcfvvtXHPNNeESqQLZ2wfh6aQO5oMA/GdcV8ZZiiH7h5pIGBRfnV5tJ+P4u8bNV3b12hZK/Hogudwvu7/rVbezGta7VZUw4Mr+mGG9W/lVGm5/za1XdatSlM8XJoMupHbrArc9vTbJLrBSWuZAr6u9Xj2MVVmCcjYOZqdaM99VbVNXM+KwmZimTp3qc/uzzz7Ls88+Gy4xfFI5D0KqHMVUCany8qR+6jpVJurQdMxHZpDbdwWOhIE1E9YPNcngrqtrhBpS689s4T7X3/UAn0lKlSu4gks5+Ir28JUr0qlNPIvXHyYrt9TvvQVyDMoS/HPU+T7brVxP6Oe9ZzSbt8kg4VTOfs0P94CgukUjQ8G9JkRNIr3qm9pYB76hU1cz26abSe2JRxQTztLgM4hK5TXcBf6CXsbpsgka8n+udQUBNcvgrotrhBpSG6oiCbSaXGXFcTYKcmCPFlx7WeeAtttAU3lfysHfPVRWWpXj3gOVWomK0BFh1Acsr11XBSCDKQd33bF1O07VicmlJjk157pygMAz8rNBKAioVL47+AyicnmNUH0QiiEZAH1Rw3T41RbuTirYaPxs1gjwpzjqWkGGkl1bEyrfj8vRv7dKGK3bpBfMIemvAGT38+I5eLKgThIXZQktdLpTm/haT7h0v0e13W59Uhv5P5VNqLWJUBB4m4xCy4OomYKQnK4KsvqC7TWQsnERymg8HGax2qY21yMIhGfCYk2eT6Bn69lmZdwzgFCykyuf55lX46nwAl2vOu17Vputbq2yUHAv/FRbUW7RkXr6dUvx+yzPdrZVnUoKNUUoCPAyGVXOpPalICr7IAjZxFQEgL54X3mG9rkVEVMTwmEWq01qa2W0UDmb5xNslhVoFbhObeJDzqT3nDkEksNXra9QOklfivGmK7rUet0wdx2qnh0TfS4HGyqVO25PH5S/Z11dn1G4BlNCQYB3VJIzBBNTZR9EqDMIR8VoWrbloES0rJaYgvqnMc56KuNrFTjPETpU3GewTrjyzCEQgUyPlU1SwUbH7u21UQ7GE/eaEyaDDqu9emYszzXcPQmm5GsSVBCuZUuFggAktSIDUlLKKqq5lv8defQtylrehGpMdG1UHaiyyaMseGhRTO4ZBIBkzwahIBoljW3WU5lAeSqVw4Hdx3tGYPlb+CoU/Jkea1oiH4IrMV9EReh485E0n+Ykm0MhKkKHqobuEK+cC1RdahJUEI5lS4WCAG2BIHDNBjxnEPrCXUSemI8xaxn5fZe5jlEdqHJEhYJwr2MdBLcPAkC2Z/tYSUIgqHuqsxpaQ1eG/pRYMB+Ke6bi71kUlzm5a/T5ITnEfeUCVRd/QQXumVSwkPC6QigIwLXuFuWdfpnXjEC2ugoKGnPXVxyuOkE2VpytlPpeN6LyVRyFOCPaois7jmzLrkX5BYLQCZejPVz4y2kJ5Bj3TJL09ywG9mjBl2sPBlQQtWViDGa69Hcvdf2dCQUBlJ43HtmWhT2uL7G778OQux5VMiCpduSyk9px+vxfMGWmg2pHlYzejShlUF4+3B+Sswhn5Hnoyo4j2S11cSsCQVDC7WivD4I5xt33GuxZBBqhL3hqeJ3I7Iv6+s6EggBUfSxF3WciOQpQ9zyIIX8Lij4eHPnoyk5oxyX8fDkA9tje3osM4cqmVoMpCEcRSmwfADGDENQb54KjPVSC3Wuw/Q1ltlVf35lQEB6o+ljsCUMw5qxxOaFlE7K9akeuL9yNM6KN1zZJKUNV7OhKj+CM8l1SW3IWoejjUPTxPtsVCMJFQ/cteFLTXBA3oUQRNbSRuy/q4zsLXxWxRoIt+WrXB9mEKkcA3rWaACTVBnLVGYT58L9ptrEPuuLfte0Gy0rk0mOgqkiOIlR9NIoxEcnHDEJXfKCW76Yqkj2fyGNvg1r7mbRNgajfn8WYuay+xWgyBFsIq65xF3ZMTnBZB4IttHWuIRREJaxJLgWhykZUvWsdCGdke01ZuLHHV6rqqZSiK3bFJRstK13bnMXE7bgR8+EZoJQioaDqYlANicj2HK/T9QU7aLbxYvR5m+vgriowZS4hev9T6At31el1zlUiTryPMeub+hajyRBK6fi6ZmCPFix49k8seGo4r90/uFaVQ8xv9xKz+z7vjaoTufR4rV3jbBAKohKKuQOOqK4gR+CI6enaZkxBMaZ4HVfW5g6vvyVnqZYZbbR8B4Ah7xck1YGueD+G3A0ArhmEIdGVB+GBrsQ1e9AV/1H7N+WBOypLLg28QEqt4SwDj/DeRo2qIjmLvcKVBXVLdUJyg2HMXEbyylivwJP6Rle8H13RPq9t5oNTSVzfo0HIKRSED4q6vU5xx2exx7kcyshGFKOr0J4jqhvFHZ7CEXuR1zmSsxTZ6pr2GnLWYsxciiFvEwDGvA3Eb/8LAKouCsWY5HJ+K3ZQrETtexKjZQUAOmvdvhRuGXU1URCqiunkQlen7wNDzloke57XtoTNQ0hedY4kBCplSCheCY+CuqU2F8KKPPEuAPqi3WclU20iOUuqVIM2ZS137WsAfkqhIHxgb5aGLWUkjtiLAZDLjmsKwppyLSUdn65yjqSUIltPYUscjiO2FzG778foY3EgVR+DLWUUsj2HiJMfEL1vAubj84jI+F/5tVw1YEyn/4/II2+GJrDqJGb3fegLdgQ91D2D0JVVX0EYs1cSu+d+og5MqbJPslmI3zqamN/u9tqu9/DHNHbcMwfJIRREuKjVhbDKKySoDagGmqSUVl1Pxp2Aq6pI9jyf/spwIRREAByxvQFwRl+gmZjciqIy7hmEw9yZ4s4vIjvyMOT/jL1cyWioCrakq7HHDyBm32NEnvzAa7dcHlYbeWQ25sOvhVTMXi49QsSpRcRtuw7TqY+rVJt1XVcl4sT7mglLV7Sv+nbO8ppTutLDVXbpSg6V/+/H0X4OdKrumUN9mpjkkkMNxj4dDnyt/ldTJ7FWIaEBBWhIzpIqCkKrJq2UkriuG0lrU+tBMhdhC3Nds2YNs2fPxuFwEBcXx7Rp02jbti2HDx/mqaeeIi8vj/j4eKZPn0779u3DJVZAVEM8uf3X4TR3IPLILNc2DwWR1/tLJNVO3I4bkewWZEc+iqkV9oQhOKK6ItnzKDz/DZr9NAQAh7kzjvj+IEnk9/ofkUffQolojTFrBSaLa1qps54CZzH6ol+RVCeSLRPV1NynfJFHZiFbz2iRV7I9h9jd95JnaoE90TuJR1+4i5i9D2t/G3N/JHF9D7IuzwbZO0rLH1K54vG1QJKu5GD5M2vm81xd2Umc0WdXjqC+cf+Q69PEFLv7PhRDAgW9Pq03GcJNrYV3arXTQlwiOBw4S4FKg0CthE9xyHXe6oqwzCDy8/OZOHEiM2bMID09nRtuuIFJkyYB8MILL3DzzTezYsUKbr75Zp5//vlwiBQyjtheqPpYVGMS4D2DsCddgT3uEgAM5dFHiqlFuQL4jPy+y3FGX0BJ+8fJGbSF3MFbXfsB1ZBASafnKGtzB05zxQhBLjuJoWCH1hnri/b6FkxVif7jeczH5hC973GvXb4WJNIV7/fZjD5/S5VtculRzIdfr7ruhSPP9cGHD8KtIBR9nM/ryGVBRr2KrUGN7HzhNi1JjvqbQUi2LGRbZngv6izFdGpRo1+araJ22tl3uobsNUQeeePsGtGCHkq8nq2kumYQXlWildpd+yJUwqIgjh49SlJSEqmpro4wLS2N9evXk52dzZ49exg1ahQAo0aNYs+ePeTk5ARqrl5wRqaiIuGMbOu1XTUmYkscTsTpzwFQIlq5/jen4ozqBJJMcecXcEZ1Cdg2gIqE7MhzmYnK0Rd7KwjJmgmKVTNFuY7xtvPrfCgVz9wMa8poVzY4YMxZVeVY86HpRB2Ygun0l97XLndA+xqBuRWEl/lFqSiC6JmR7ovkH5KI3fWPgMfUN5oPoj5NTI4CJEdBWK9ptHxL7O770Plx7urzNjd45Q4eJqZamEFEZHyM+dD0s2tEtbmCHlAqrUHjVhAexT3DPShwXzccF0lNTcVisbBrlyv2Pj09HYCMjAyaN2+OTudaE1qn05GSkkJGRkY4xKoWtqSryR30C0pk+yr7Sto/Brg6ekdMr2q37TR3cP0ffQEAkac+oqz5GBR9PLqi/S6TU95mOLiApHWdSFzXFWPOapdclfMx8FYqkcfeJnb7jUQem6tts8cPIq//Wuxx/TBmr65yvrv8eeRRbye5XD6DqBypBBU+CNmeC6qKXHIYbLkV5/qaQagKKHZN2ZkyF1c9BjDkbsDgQ85wU6EgiuptNC05CpHs4VUQsj2//H+XszRq/0Si9rsCNXRF+0j45cqK3J+GjNbx+p9ByGWniDzyJnLpUfT5W/0eJ9nzkZ2F4CwB1Unc1mur/Y56yuE9sKo605GtNV/A6GwIiw8iJiaGmTNnMm3aNKxWK0OHDiU2NpaSktqxryUmRtfovOTkmGqe0cdPQ9dAqwPootqTJOuqL4i5D2yX0F/4JOijQJKJaD0aVo8g0rKESMvXYK0o7ifbc4ixfA46M8aud8DmDa5CgRf/G7K3YDj2GcnyPtj/Jhz5CHQRLrNQfE8wJhDd4zaiI2OgeX84/CHJSdHelWgdrg7bULiD5HgVDK6EQQ6VL5lqP1P1nDKXgtAreSSXfAObxsFly7XdUWQSVfl5734ZDrwLF05y/a0z+/5Odr7iUjbddlbdZ82GskyI6x7asw4Bv+9FkWuULKGS3Ezn+q7qAlueq+3KviHF7krIdEpVZKz+u1yOowT2vAod/wl5O6H1qKrHWFwDhnizDZJjYMsmkHSYk2PA6VrXIc6Y49rniVuJ+qhyXG15c3fBqitgxA4wtwrtHMUOkg4k9zjY1fHGRCrESHvBmln1fvd+A388Q3T+d1B8BP58xLfMksvcmBxdArIDctZgLP4N/pIV+j2VVCj6pHgdRJW3X76AWUxkhYk3wZRX9fkGocbvhAdhc1IPGjSIQYMGAWCxWJg/fz6tW7fmzJkzOJ1OdDodTqeTzMxMWrasXtx8dnYRSjWXlfK10PvZkQJlNVV4zdAN3IQzqqvrhQawFKPr8DIJPw1FMSZR0u11Yk5/RGGL24jZ9xhq5o84YntTKJ1PMwBnKVnx44gochDjWIBjwx3oi36jrPn1WFvcQNzOm7Ea2lJw0SdQBBQVEiG1I8ZeQO7BNTijuiA5S4jZfR/G7A0ohgRkey7WtbfhjGxHcecXiS3MxATgKMaS4SofItuykO3ZxNvzUZHBmkPZkeVEqgqcqRhR2XN/Jy+rEF3xH+hKD2NLvJyEAwvRFx/BvncuBsBhbEGuj+8kofgMsvU02T72NfuxF7qyE2RdWTuj6kDvRUSeBfdPznL6tCtgIUiJ9xrJsDIBe1x/8i7xHpVLtmySAJwlZJ3J0RTI2bzL5oMvE3XoFZx/vIfOehLL0D+qBEWY8y1EAYXZGZRFFNKsLA+AnKxCjNmniQOKc05SUkmG5JWxlLb+B0Xne9vq/cqrWNGVHMQZfX6VXaaMX4i1ZpF3fAf2Zr47Pl3x75gPTqOwx9sgR5C45jzszYZScNFCABIdVmSguCAP/Ykp6It2kzMkzfte8zKIApScHUjOIiyZeSRsGoi+231kNbtdOy6hNAc9kJtxCGQjCYBTMpNTje9BV3wGd0hHTlYmzpJ4UBy4vZzFuadxD0EKMw9SFlHetmJHX7DNFfDih2DvhCxLIQ2swxbmmpXl0qyKojBjxgzGjh1L69at6d69O0uXLgVg6dKldO/enWbNfEfCnMs4o8+vUA7ubVFdyBm4kdwB6yhrezeM3IG1+fUASCg4ozprvg1bwqVARWiuSzn8hcKeH2BLvoaizi9S3PlF7/bNnQBI+Hk4zdZfSOTRN7XcDVvSVYDL7GM++gZx269HX1gxgo/e+zhJa1NptukS4reMKL/2RUiKVTN/keVaQ8MRfT664j+QSw7TbGMf4rb/lYiTH2mmMEO+y8EvO/J9PhvZno3syPNastWN5tsIcdnXiBMLMOSsD9npJ5ccIuL4e4C3c9pQuIOkH5J9+nvOBrf5zpC/GV2hh83fWYqu9FDFcbXkh3CHLMu28iTPwqqzNM205vZBOfIrPpebn6rE6pebcyJP/tf1t7MEY9bygKa5mN330WzTAK1NLxnK71fy844AmA9OI+LM/2HKTEef/7PLn5f5tWunqniEKZeW5xdULbkv211mUdmRj6Q60ZUccq0hn7WhkjzlZjdbFrL1jOsSOrNf2XziEQ2oRch53J+n30FnrTC7m858RcIvVyKXHqne9WpA2BTErFmzGDFiBH/6058wGAxMmDABgEmTJrFw4UKuuuoqFi5cyOTJk8MlUqNAMXfwCh1VDc1QdC7N7zR3BtlAzqAtFFzkcmw7os/X6ka5lQWSRGn7h11Ocw88/5btuZg9fA42jzBZp6k1htxN6EqPaFFcEWf+r4qsbv+L1mmX/6jscf2Q7RYiTn2kHRtxYr63LJGpSPbcqs5OVUEqr1vlVXpAVb2WhpVtZ6rIA2A8s4SYX/+JMWs55kOvErP3EeK3jiTphyQM2Wt8nuNJwuahxOx7DMlR4BXeqs/7CUm1oS/8LeD5usJfid77KLE7b/Gx7zcSV7dxFXN0b/PIMYlwd66A+fBrWrl5OHsFoc/dhFx2Crm8k9Si5gp8KAgteiuvvOhkgcsfpTo1pV65OrFs8zC1qCrmY28Tt+NGIo++5VemiNNfuK5jrxqk4h4c+PJ/uXG/m/qi3USc/NB1adlcfn4Bkjuc1FmC7MhDdhZVGVhUbl+f/7PrQ9HBSse57jtm70OYD73sulY1FYRnlJJbQbgVFKApHtfnCh+EuwqCrjRw4EdtEDYT09SpU31u79ixI59//nm4xGj8SBJKZDvkoj04ysuKe0VIyQYcsb0w5P2EI7ZXwKaUiIqIrKJOkzAUbAV0mDIX44zqpu3L7/0Z0X+8gDH7e2wJQzFlfYOklKHoY1H1cejKHdCO2IugvA9XDIlap2GP60/kyQ8wH/43jugertpUhTtRZbMW513W8iaiDr2M5MhHNSRU3K4jT+u85LLjGHLWItuziTw2j6LuMypu23rGZwCBKXMJEae/0KLMAEraP4r5yEzMh18nP/EyHw/GStQfz1PS/jHk8o5Ytp7xchrqyyvvymXHkUsOopirZvYas5YTt+PGig3OEpAjkUsPo5haYT42B9lRgNGykrK2/wTVqdXlcUa0wWhZiXvOUjlSTXYUUDluSC49hmw7gyOuX9V7qkTcjr9hbXljlZpcvoo4SuV+BtmeB87iinwYe56mqORKo3FPBSFbT2MoX5Ex6uAU6PVQQNlkew4K3slh7u8h8sR8Ik4tci3/6zbvOYuJ/e2eitlX7gaPnIcSUKwVIdq4ovAqZgAWFI/IRNlR0UEDGNxh4EUVszcUh8tBXX6f7ntV5cDrwVTGy1muzSA8FITHoMd9DcmeqykL96yvLhGZ1I0QZ0Q71//mTj732+P6oSJrxQb94mHSKk19jIKLFlHQ8wNy+//opVyc0edrGeGqPlr7XNzpec2+q+qicHp0kmUtbtA+uzssCRVr8+tRTK3K5W9Pfu8vKOw2A2ek654ku/cP1HNhJWPOWmL2P0HUoVeQHXlEHpmt7TPkbSbi5EfIJYeRrGeI+2UEcukxLfxWu8+2d1HceTJFnV/CmLsOfcH2Ko/FkLsJ87G5mD1Gu7L1tNcMwp0xbj4yk2abBrpMLJWyxSNOLcJpbE5xh6e1NmJ33Ejihl5EHXgByeYeKSvgLCVu+w3E7r6nXM670ZceQlf8B5L1TJUoFl8ziNhf/0nCz5djyvgf0XsfxWD5vkLe4j8wH5iCIWcdOIuRHfnoSg5p0Wdu9L5MTB4zCNnjurI9t8L0U2UGUWEe0ef/jCF3I46oLq5Q0+yfvC+gWL1mjlEHJhO9+wHMB17CeOar8mu7OnRDwVaMeRu8Onx94W5MmV9jzF3n+rtgG7qSg9pMWy476VJu7vtxlmgzALnSyo5VZxC/uD7YcrV305+ZS1KDFBB0FhO991HNtOUdxVRSfi8VM1L3M1QlA5I9B4Ple5LWnEfEifdd+62+Z821iVAQjRBn5HmunAwfo1Zwhd3mX/wVqiE+aFu5fVeQe4lHeJ4kaYUIcy/5gfyeH4Kk07bpSo/hiB8AuExYjqiuLlki2qHqXM5DR2QHrC3+WiGvh5ylbe9GiWhdfh+p2JL+RFnbOzUzmlYG3T0C9DA3mE67zFrFqU+g6qK87OXRfzxLzJ4HiNnzAMactRjzNmC0fOvVAeb3/oKirq8BUNbmHyj6WC8lYz4wBQ59qHWSEacWVlz7zFeYyjsrqFAQsiMfSSkjbvtfiN9ytWZjlxyFGC3fYW1+Hfa4vq5zyk5hzHblnURk/E8rnBh1cCqJ6y/AmF3RoVubXwdA3JZrSFrXuWIkW45syyLm13/S7MfzIWc7qKqWpxC973EiT8wn0sOMZz44lajDrxO76+/IVlfHo8//2RWDX44qGdCVHqmipN0+CNme59U5So5cj5G463syZq3AfGCKl9ks8uQHSEoZJakTXYEMmRXru0vWMyT/kOxV38uYswZT1jIij/+HiPKcIPcsRrt/D3Nj5RwBSXUgO4uwJ7gqGOjKTiB5zGgkR6E2A4j/+Uqvd0CudO8GjxmVrtzm71dB+PCReWLK+pbIE/OJ/v1pdMV/EOFRZidu1zii9j9N1IFJ2OMuwRHVXfuenJHnIdssxOx1zbzc31k4ciOEgmiElLa7j8IL3vO7BrZqTMSeOCykthwJA3G4q9ZW3hfXD1t5R+X2LzhiLqSs1S2Utr7DtU1nxhmZitOciiOmB2WtbiH/4i9xxF8Cl6+hsPsskPUUd3yW/Is+RTXE4/RQEG6UcrOSbM8h4sQHJK1uhz5/m9cMQmc9iaKPp6TjM9iaXeZ6Fq3GVdy3bMKYu56IjE8AV9l1T8e3Pa6fFvKo6mMpa307pjOLkaxn0BXtJerw6/DTbZqC8LSrR554D9meU2HTrpRsZSjYhqFwF7pyx7s+/2ckpQxb8kgte96Q7/JbWFOuQ7bnYCjcrt2zl80eUCLbY0sYis6PGcGU8TERpz93+Xt2/ouIk++7OsWY3too35C30TUydxZjyvpWu5a+8FfX5/LjnEZX1JI9wZVTU9nM5O6cJUee18wlZvd4rV3Jno1kzyF25y1EHX6dmH2PlX8nZgy5P7rajx+AI+YCyPpRa8Pd6ZqPVJgLXXJmu2Y55Yqmcu6HziOvxnMkbY8fWPG5PHBDLjuBrlyhOI3NvUwzkmrDdKYiIdRzZuJGLe8mdSXlDv0aKgj3LElfuJvIo29gsnzrtdt87C1key5FXaai6qO1jGolsh26suNVkk3dA4y6RCiIRohi7oC15Q3BD6zNa0a2JWfQFoo7PY8zqjNF58/SwiwLLvqIoi4vg2yisMdclPLEP5qnaetmlHR4ElvKSFdb5cu1epYYcc8gdIW/EbP3QSSllOj9E7UIp7Ly6C3F1BwkWRuV21Ku1doouGA+qhypRWKZytflUIzJOKK6e/k2AMpa/g0JBZPlO69EQkNOxQjXHtffa7GoYLVxTGeWABUlUhwxF7hkxlUOHaDkvAdwRpzn83xFH0vBhf8FoLTtP/1fx/Idzoi2lLa5EzJWELP3EVfbnZ5BxWWbl+256Ir2YrR8j6SUUNxhIgDG3LVebbmrFtsSr3TJXslRrZmY7PlenaO+eK9mopGdRZgP/xtUh9ZJq7IJR0wPJMWKKptQIlq7zI05W7SZlmcnp1aK4oNyh2y5Y9wT7xmEh4KIuwRV5woOdSs8XdlJ5LITqMgo5tQqHau+8DdXnpCqeEVQuQcDjpgLXdcp99e4jylp/wiF3StCeIMpCNnuGgToSg74DW5Q9HE4Yvto9wD4fVf8BWbUJkJBCELGGdUFZGPV7TEXViiFUNpxKwivGYRLQbjNOtakqzDkb8Z8ZCYARd1nUdZyLMWdXnDtb3ULxalPYEusiOyxJV2ONeWaKtfL7/UpeX3Sq8oRfQFOU2uMWcswZaZrI2md7bRWEtoR27PKbMHdAfvClPk1Ecffw5SZjmJIRDUmoxoSUSU9xnIF4YjuQW7/1ZS2uVMreeKmuPMUrC3GuO4n5Tpy+33n91plrW6mJHUCdLoHp8mVO2RrNoyyNv+kpP2jAMTtvImIjE9Q5UjKWruUtSFnXcW9SAbXqB5wRHfHGdEGfeEOr+toJiZHns8QVDeRR9/ClnQFpW1cik1SrDjLAweckeeBJLv8ZvYCzXSo8/Ct2JsNq1LLS1JKiN15MwZ3NFE5Ok8FYa0wtSimFjiie5TfTzcUYwq6koPoyk6gmFqi6GO9QkbBVTlAX7izPNKpwuRW2vYuVCTszS4DY4I2a3GbmMpa/I2yNv+gtM2dOKIvQHYWEv/TUCJOvE/8z1cScfIjIg97BlKUO5qV0vKAkKrYEwaDrPeKiHL75wAcURUJoZ73XVcIBSEIO/b4ga7Kth6OcNUQj4qEvuQAjsgOFF7wHsWpEyr26+MpvOAdbCmuzFfF1IKSTs+BbMAWP8gVQaKLwpryZ8BVgsQR1R17XD8cMRehmrxXBARAkrAlX40p6xtke7bXOh8l5dd2xHgvDGVNvgZb4hU+78uaPAp90W5i9j2GIW8TDnckmCRr5eKdEe1AH41qTKKo+wwUo3eVUsXkkSQqSZq/xxP3jKas1S2u2l+XzCNn8E6yB+8E2UBR9xkUd55M/kUfI5cdx5S1DHtsb5SIliiGJPQehRuViNbaNRVTiqv8iuU7Io/MRlc+ytXyB+x5Pp3j7ugdCRVry5uwx1+i7XMPAir/b7SsQC457DWatyZdVWWWB7ii5lS71zbZWmFu8RxJK8YU7AmXup69zow97hIMeZuQy064fF+y71BUQ/4WzbzkNLVElSMp7jwZyxW5FHd5EczttJIw7lmUWq7MirrPoKzVza52CncQdeBFDPmbifr9GcxHZ1f4pULwGdi0JY8rzMee0YbOqIqKyOEwMYUtzFUgcOOMuZDcwZVGUJIOa8u/EZHxP+wJQ1ANcZR0ep7IY/9xORQDZCzn91kK5SM/W8ooirq8TFnLsVoF3kCUtntAc+ZaU0YTk5xKtrMdkmIn8vi72ModnQCWYSdR9THo87dgyl6JNXkkpqxlOE2t0VlPUpL6KKaspdrxqjGx4rPb7FEpBLVy6XC3v8ITVY70KrGuGBJxmjt6h/XqIlA8THbuZ2GPG4Axb4MWSeaI6aHNZMA1m7MlX01p0W6cUd0o7jwZY84aov94DptlJYUXzEN2FKDKJiTF6rPooi3pKpwRbdCVHceaPBI8THJuM6JWb8zskjl2973lz6M/iiGJsla3YG15IxEZH2t+iUC4FtiScUa0xpT1DaqkQ1KdKKYWFDe/nuIOT7raTxiEKWspsvUM1uSrfc6AFUMzdEV7MZevKV/c8Tmc0d09SnQAUe3Q5R8i4uRHxOwZD4BqqJjtuNevh4rIqIraZRZUYzKyLQt7TE8vx7cnlrTDmoJ0vy+qbNbyO1QkirpOA6UEJbI9EScWuPwaUt2N84WCEDQYCru/gTOyPWWt/q5ty7l0T8DsWQBkj9dY0lF63viQr+mM6kRh11fRF+93KZTk0SjlJQqy01yRSgU95mK0fIeqL4/SiuuL5bJjqLIRU+bS8sSsj3DE9qWk7b0g6TEfewt7bIXzX1/iWqiptJ33AvVlrf+OMfdHLW/EawZRTl6fr4k8/i5IErqSwxR1flGrGhwMW/IIjHkbKnw2zYZ7KQglog1KRBuKurtMeUpke3L7ryP69+cwZS4m8cfztfNMluWYTi1ElfRIqkcmumKluOvL3jL3/RbFkKCFjVbMILzt6Yb8zdjj+rtG6XivJ+JWSpVRJQOSateCEQCsKX/G3iwNe8Kg8vBt1zthj3eV95GUEpSINl6rAeb3/Agl8jyi9z1GxOkvNP+SYu5QNXDD3A79yXRi9jyAoo9HiWijRe0BKHr/dY/0RXsxH7kLY/YqbImXk3/RIqIOTq1Slt97QGHW2nU/E9WYhBLRmoLeXyDZsrAlDK1T5QBCQQgaErpISjo+47VJNcR5jdTqgrJ29wbcb211C9ZW3pnQ7hBia8u/YXUWU9r2LpAkiru9CrgczIqHc7Gw+5vo8ze7ors82255I1ktbyRu2xgM2at8rljoiO9PYYC6O4Eoa3ULurJjmq/GljIKDryg7Xf7gzxRIs9zzYY8quvakq5AV3rIpUiRyb50D/qCncTtvKlKLgG4Ru6uC5RS1uJv2JJd5VjwcL7aYy7CULhT8z9BRTSboo9FMbXAmnItEae/cGXx66KRnUUUXLQQyVFA7G93aedJqpOyNhW1ktw4YnqiGFOQbZkoppbIasUMyJZ4OeijcUamamHEqmzG4Su/KKrCD5CddqDKTMRTWVTGdGaxFt6sGJOxpYzGljKa5JUVsw57pSrQ7nBva8ux2vPxNEeqxmRsza+lrhEKQiA4W3RRKB4dH1Als7qszW3Q5ja/TTiiz3eVRPcRyXM2qMZEirq9rv3tLM++tyZeiSO+v1bbq4o8Mb0oaXcf5vLoLlUXTUmHp4j99XZXuZeINtjLTUmeOS9V0EVSeOF7PneVdHyWuB03oCufXYGrA1XlCOxx/V0Dhk7PIzlLMB97GyWiNVLJIVekkjGRMstKVNlE5KmPcPgo8AeArCdnwEaiDk3DmnJNRQkOSa8pK/fsRjEkkp12yLc501yenGpq7dNMpVaaQaiSEXCiypFe+Q6Kh48l7+Kv0ZUdcyWVVvrey9rcgSobKe70rFbd1enD/FjXCAUhEDQAijs+S0nq48EPrAWyhmeCpPc2zVVGkijuOh3JWUrkyf8i2yyUtn+QnOgLoNxhrBqTyLrcAlJoS9Zq9F9AUfYxbInDscf10wICAErbjcdWrrzc0WLuWZWt2TDKLvpYM8W4FU9Jh4k+TXNuVFOKZkJzm8Yc0T00ReD2kziiz/fv64pw+bMccRf73l8JVy5DDBEnP8SYt1HbrvMobWJPvAy7r5NxzcC0WZhqRJUjfPqn6hqhIASChoAuAlUXEfy4WrpWqJS0fwRj9mqszUcDVF1X3MdoOigdb6c01uXnybvkB69dSmRbr9pI4PKjcGAytpRR2gzI+5x2Vbb5w5YwFKNlJQUXLdK2OSPdC3YFWFMkJY2S9o9Qct6DPnc7YvtQ0u5+bIlXEHX4NaytbkLVx+I0d8L4yxU4I9uj6sza4mLVQpIo6vJyRfHNMCIUhEAg8Iti7kDOpb/WqwzO6PNrbb0Pe9IV5CZ5hyk7o7qgymbsPkKKNWQDxZ2nBNivp7jrKwDkebTviL+EvIu/Qok4r0o15epQ1vbOGp97NggFIRAImjSqMZHsofu0vIbaxu6RzNnYEApCIBA0eUIpbNkUEZnUAoFAIPCJUBACgUAg8EnYTEyrV69m9uzZqKqKoig8+OCD/OlPf2LNmjXMnj0bh8NBXFwc06ZNo23btsEbFAgEAkGdEhYFoaoqTz75JIsWLaJLly7s27ePm266iX79+jFx4kQ+/fRTUlNTWbJkCZMmTWL+/PnBGxUIBAJBnRI2E5MsyxQWumKfCwsLSUlJ4fjx4yQlJZGa6kpUSUtLY/369eTkVF20XCAQCAThJSwzCEmSmDVrFvfffz9ms5ni4mL+85//kJqaisViYdeuXfTs2ZP0dFfN/oyMDJo1axakVYFAIBDUJZKqlhcrr0McDgd33nknDz74IH369GHr1q08/vjjfPPNN+zcuZM333wTq9XK0KFDWbRoEQsXLqRr167BGxYIBAJBnRGWGcTevXvJzMykTx9XCd0+ffoQGRnJwYMHGTRoEIMGuWqOWCwW5s+fL5zUAoFA0AAIiw+iRYsWnD59mkOHDgFw8OBBLBYL7dq1IyvLtQyfoijMmDGDsWPHYjb7XvVJIBAIBOEjLCYmgK+//pp3330Xqbxa4kMPPcQVV1zBM888w7Zt27Db7QwePJinn34ak8kUDpEEAoFAEICwKQiBQCAQNC5EJrVAIBAIfCIUhEAgEAh8IhSEQCAQCHwiFIRAIBAIfCIUhEAgEAh8IhSEQCAQCHzS5FaUO3z4ME899RR5eXnEx8czffp02rdvX99ieTF8+HCMRqOWDzJhwgQuvfTSBiX79OnTWbFiBSdPniQ9PZ0uXboAgZ9vfcvvT2Z/z7u+Zc7NzeXJJ5/k2LFjGI1GzjvvPKZMmUKzZs0a5HMOJG9DfcYA999/PydOnECWZcxmM8899xzdu3dvkM84mMy1/pzVJsatt96qLl68WFVVVV28eLF666231rNEVRk2bJi6f//+Ktsbkuy//PKLeurUqSqyBpKxvuX3J7O/562q9Stzbm6u+tNPP2l/v/LKK+q//vWvoHLVl8yB5G2oz1hVVbWgoED7vHLlSvW6664LKldDlbm2n3OTUhAWi0Xt06eP6nA4VFVVVYfDofbp00fNzs6uZ8m88fUlN1TZPWUNJGNDkj9UBdGQZFZVVf3222/V2267rdE8Z7e8qtp4nvFXX32lXn/99Y3mGXvKrKq1/5yblIkpIyOD5s2bo9PpANDpdKSkpDTI8uITJkxAVVX69OnDY4891ihkDySjqqoNWv7Kzzs2NrZBPXNFUfjkk08YPnx4o3jOnvK6acjP+JlnnmHDhg2oqsp7773XKJ5xZZnd1OZzFk7qBsiiRYv4+uuv+b//+z9UVWXKlCn1LdI5TWN43i+++CJms5lx48bVtyghUVnehv6Mp06dypo1a3j00Ud59dVX61uckPAlc20/5yalIFq2bMmZM2dwOp0AOJ1OMjMzadmyZT1L5o1bHqPRyM0338y2bdsaheyBZGzI8vt63u7tDUHm6dOnc/ToUWbNmoUsyw3+OVeWFxr+M3Zz3XXXsXnzZlq0aNGgn7EvmXNzc2v9OTcpBZGYmEj37t1ZunQpAEuXLqV79+4NwsThpqSkRFuaVVVVli1bRvfu3RuF7IFkbKjy+3ve0DDel5kzZ/Lbb78xZ84cjEZjULnqW2Zf8jbkZ1xcXExGRob296pVq4iLi2vQz9ifzCaTqdafc5Or5nrw4EGeeuopCgoKiI2NZfr06XTo0KG+xdI4fvw4Dz74IE6nE0VR6NixI88++ywpKSkNSvaXXnqJ7777DovFQkJCAvHx8XzzzTcBZaxv+X3JPG/ePL/Pu75l/uOPPxg1ahTt27cnIiICgDZt2jBnzpwG+Zz9yfvUU0812GdssVi4//77KS0tRZZl4uLimDhxIj169GiQzziQzLGxsbX+nJucghAIBAJBaDQpE5NAIBAIQkcoCIFAIBD4RCgIgUAgEPhEKAiBQCAQ+EQoCIFAIBD4RCgIgaCe6Nq1K0ePHq1vMQQCvzSpWkwCQSCGDx+OxWLR6tUAXH/99Tz//PP1KJVAUH8IBSEQeDBv3jwGDRpU32IIBA0CYWISCILw5ZdfMnbsWF588UX69OnD1VdfzaZNm7T9Z86c4d577+WSSy7hyiuv5LPPPtP2OZ1O5s2bxxVXXEHv3r0ZM2aMV5mEjRs38qc//Yl+/foxefJk3HmrR48eZdy4cfTp04f+/fvzyCOPhO1+BQI3YgYhEITArl27uPrqq/npp59YuXIl48eP54cffiA+Pp7HH3+cTp068eOPP3Lo0CFuv/122rZty8CBA3n//ff55ptveOedd0hNTWX//v1aGQqANWvW8MUXX1BUVMSYMWMYNmwYQ4cOZfbs2QwePJgPP/wQu93Or7/+Wo93L2iqiBmEQODBAw88QN++fbV/7tlAs2bNuO222zAYDIwcOZLU1FTWrFlDRkYGW7duZcKECZhMJrp3784NN9zAkiVLAPj88895+OGH6dChA5Ik0a1bNxISErTr3XXXXcTGxtKqVSv69+/Pvn37ANDr9Zw6dYrMzExMJhN9+/YN/8MQNHmEghAIPJgzZw5btmzR/v3tb38DoHnz5kiSpB3XqlUrMjMzyczMJC4ujujoaK99Z86cAeD06dO0a9fO7/WSk5O1z5GRkRQXFwPwxBNPoKoqf/3rX7nmmmv44osvavU+BYJQECYmgSAEzpw5g6qqmpLIyMhg+PDhpKSkkJ+fT1FRkaYk3Kt3AbRo0YJjx47RpUuXal0vOTmZl156CYAtW7Zw++23069fP84777xavCuBIDBiBiEQhEBOTo7mD1i+fDkHDx4kLS2Nli1b0rt3b2bMmIHVamXfvn188cUXjB49GoAbbriB2bNnc+TIEVRVZd++feTm5ga93vLlyzl9+jQAcXFxSJKkLb4jEIQLMYMQCDy49957vfIgBg0axOWXX07Pnj05evQoAwYMICkpiTfeeEPzJcyYMYMXXniBSy+9VKvJP3jwYABuv/12bDYbd9xxB7m5uXTo0IE5c+YElePXX3/l5ZdfpqioiMTERJ555hnatm1bNzctEPhBrAchEAThyy+/5PPPP+eTTz6pb1EEgrAi5qwCgUAg8IlQEAKBQCDwiTAxCQQCgcAnYgYhEAgEAp8IBSEQCAQCnwgFIRAIBAKfCAUhEAgEAp8IBSEQCAQCnwgFIRAIBAKf/D/WI6CtfOCWHAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378/378 [==============================] - 0s 753us/step\n",
      "95/95 [==============================] - 0s 820us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaQAAAEUCAYAAABkhkJAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABP9UlEQVR4nO3dd3wT9f8H8FeSku50UaBlShWoVQQBBQELRSxggYIDrDIFEWRTpChSplBFQKCIgmUIoqAMKYUCMvTLD3BQZhmK7JbVlY40bZP7/VET0pCkl3G5u/T9/IMHTZO7d5N87n2fLWEYhgEhhBDCMynfARBCCCEAJSRCCCECQQmJEEKIIFBCIoQQIgiUkAghhAgCJSRCCCGCQAnJhY0YMQLbt293+HMJcQXNmzfH9evXAQAzZ85EcnIyq+da6+eff8bw4cNtem1NI6F5SMLSunVr/f9VKhXkcjlkMhkAYPbs2ejTpw9foREiOO+88w5atmyJCRMmVHn8wIEDSExMxJEjR+Dm5mbytc2bN8e+ffvQuHHjas/D9rm3bt1Ct27dcP78ebPnJebROyYwGRkZ+v9HRUVh3rx5eOGFFx55XkVFBX3hSY3Xr18/LF68GOPHj4dEItE//vPPP6N3795URkSGmuxE4sSJE3jxxRfx9ddfo2PHjpg+fToKCgowatQotG/fHu3atcOoUaNw584d/WsGDRqErVu3AgC2bduGN998E0lJSWjXrh2ioqJw5MgRm5578+ZNvPXWW2jdujWGDh2K2bNnIz4+3knvBCEPvfTSSygoKMCff/6pf6ygoACHDh1CVFQUBgwYgLZt26JTp06YM2cOysrKTB4nISEBS5Ys0f+8Zs0adOrUCZ06dcKPP/5Y5bmHDx9GbGwsnn32WURGRmL58uX637399tsAgHbt2qF169bIyMjQlyedkydP4tVXX0WbNm3w6quv4uTJk/rfDRo0CEuXLsXAgQPRunVrDB8+HLm5ufa9SSJCCUlEHjx4oC9sc+fOhVarRf/+/XHo0CEcOnQI7u7umDNnjtnXnzlzBo899hiOHz+OESNG4KOPPoK5FltLz42Pj0fLli1x4sQJjB07Fjt37uTk7yWkOh4eHujZsyd27Nihf2zPnj1o2rQpvLy8MH36dBw/fhzff/89jh07hu+++67aY/76669ISUlBSkoK9u3bh2PHjlX5vaenJ5KSkvDnn3/iq6++wubNm3HgwAEAwMaNGwEAf/zxBzIyMqo0wQNAfn4+Ro0ahUGDBuHEiRMYNmwYRo0ahby8PP1zUlNTsWDBAhw7dgzl5eVISUmx9e0RHUpIIiKVSjF+/HjI5XJ4eHggICAA0dHR8PT0hI+PD0aPHo0//vjD7OtDQ0PxxhtvQCaToV+/frh//z4ePHhg1XOzsrJw9uxZfRxt27ZFVFQUV38yIdWKjY3F3r17UVpaCgDYsWMH+vXrh6eeegqtWrWCm5sbGjRogAEDBlgsHzp79uxB//790axZM3h5eWHs2LFVfv/888+jefPmkEqlaNGiBV555RX8/vvvrGI9fPgwGjdujNjYWLi5uSEmJgZNmzbFoUOH9M/p378/HnvsMXh4eKBHjx64cOGCFe+GuFEDq4gEBATA3d1d/7NKpcKCBQvw22+/oaCgAABQXFwMjUajHwhhqHbt2vr/e3p6AgBKSkpMnsvcc/Py8uDn56d/DABCQkKQnZ1tx19GiO3atm2LwMBA/PLLL2jZsiXOnTuHFStW4OrVq1i4cCHOnTsHlUoFjUaDiIiIao937949PPXUU/qf69evX+X3p0+fxqJFi/D333+jvLwcZWVl6NGjB6tY7927h9DQ0CqPhYaG4u7du/qfg4OD9f/39PQ0W0ZdEdWQRMSw0xYAUlJScPXqVWzZsgUnT57Epk2bAMBsM5wjBAcHo6CgACqVSv8YJSPCt759+2LHjh3YuXMnOnbsiNq1a2PWrFlo2rQp0tPTcfLkSUyaNIlV2ahTp06V73RWVlaV30+ZMgXdunXDkSNH8Ndff2HgwIH64xqXUVPHNj5ednY26taty/ZPdWmUkESsuLgY7u7uUCgUyM/Px4oVKzg/Z/369fHUU09h+fLlKCsrQ0ZGRpXmBkL4EBsbi2PHjmHLli2IjY0FUFk+vL294e3tjStXrmDz5s2sjtWjRw9s374d//zzD1Qq1SPlqri4GH5+fnB3d8eZM2eQmpqq/11gYCCkUilu3rxp8tiRkZG4du0adu3ahYqKCqSlpeGff/5Bly5dbPq7XQ0lJBEbMmQI1Go12rdvjwEDBqBz585OOe+iRYtw6tQpPP/881i6dCl69eoFuVzulHMTYkqDBg3QunVrqFQqdOvWDQAwbdo0pKam4tlnn8XHH3+MXr16sTpWZGQkhgwZgiFDhqB79+5o3759ld8nJiZi2bJlaN26NZKTk9GzZ0/97zw9PfHee+/hzTffRNu2bXHq1Kkqrw0ICMCqVauwdu1aPP/881izZg1WrVqFwMBA+94AF0ETY4ndJk6ciKZNm2L8+PF8h0IIETGqIRGrnTlzBjdu3IBWq8Wvv/6KX375BS+99BLfYRFCRI5G2RGrPXjwAOPGjUN+fj7q1auHWbNm4cknn+Q7LEKIyFGTHSGEEEGgJjtCCCGCQAmJEEKIIHDahzRmzBjcunULUqkUXl5e+PjjjxEeHo6oqCjI5XL9qgPx8fFOG7JMCCFEmDjtQyosLISvry+Ayv1JkpOTsX37dkRFRWHVqlVo1qyZzcfOyyuGVmtd6EFBPsjJKbL5nM4kpliBmhGvVCpBQIA3RxE5n6uXIYDi5RIXZYjTGpIuGQFAUVFRtctqWEOrZawuTLrXiYWYYgUoXrGpCWUIoHi55OhYOR/2/dFHH+Ho0aNgGAZr1qzRPx4fHw+GYdCmTRtMnjwZCoWC61AIIYQImNOGfe/YsQO7d+/G6tWrkZ2djZCQEJSVlWH+/PkoLi7GokWLnBEGIeQ/OTlFVt/hBgf74v79Qo4icjyKlzu2xCqVShAU5GP2906bGBsbG4uZM2ciLy8PISEhAAC5XI64uDiMHj3a6uO5emESU6xAzYi3usJECLEPZwmpuLgYSqVSn3wOHjyoXyFXN9iBYRikpaUhPDycqzBMOnb+DrYduYIcpRpBCnf0jwxDh4h6To2BEEJIVZwlJJVKhQkTJkClUkEqlcLPzw+rVq1CTk4Oxo0bB41GA61Wi7CwMCQmJnIVxiOOnb+D9XsuoqxCCwDIUaqxfs9FAKCkRAghPOIsIdWuXRtbtmwx+bsdO3ZwddpqbTtyRZ+MdMoqtNh25AolJEIIsZM0OwsoLYX2sabWv5aDeAQtR6m26nFCCCHsSO7fh390V/jM/tim19e4hBSkcLfqcUIIISxotVC8PxLSvFwUxyfYdIgal5D6R4ZB7lb1z5a7SdE/MoyniAghRPy8li2G/PBBFM1Lguapp206Ro3bD0nXT0Sj7AghxDFqHf8/eC2ch9J+r6J08DCbj1PjEhJQmZQoARFCiP0kDx7A991h0DR5DEWLvgDsWCKuRiYkQgghDqDVQjH2XUjzcpG/aQsYX/uWgKOERIjAmdvG5erVq0hISEB+fj78/f2RlJSEJk2a8B0uqUE8VyyF/OABFCYtRsXTz9h9PEpIhAhcUlJSlW1cPvzwQ2zfvh2JiYmIi4tD3759sXPnTsycORMbNmzgOVpSY/zvf/BeMBelffujdOg7DjlkjRtlR4jYmNrGJScnB5mZmYiJiQEAxMTEIDMzE7m5uXyFSWoQSU4OMHAgtA0boWjxMrv6jQxRDYkQETDexiU7Oxt169aFTCYDAMhkMtSpUwfZ2dkIDAxkdUxbF4oNDvat/kkCQvE6mFYLDB0I3L8P2bFjqN20vsMOTQmJEBGYP38+gMpltz799FNMmDDB7mO6+or5AMXLBc/lS+GTlgYkJ+N+wycAK+KtbsV8arIjRERiY2Nx4sQJ1KtXD3fv3oVGowEAaDQa3Lt3T7+6PiFccPv9BLw/mQ1171jAhm2DqkMJiRBDWm31z3Gi4uJiZGdn63/WbeMSFBSE8PBwpKamAgBSU1MRHh7OurmOEGtJcnOgGDUM2gYNUbhkucP6jQxRkx0h/5H/sg++495D3p6D0DZuwnc4AMxv4yKRSDBr1iwkJCRg5cqVUCgUSEpK4jtc4qq0WviOew/S+/eQv3s/GIUfJ6ehhEQIANm//8B31DvQNGoMbV3hrOJhaRuXsLAwbN261ckRkZrI88sVcN+fjsIFn6HimdacnYea7EiNJykqhGLwm4CbDMp1mwAPD75DIkQw3P44Ae/5s6B+pQ9Kh7/L7bk4PTohQqfVwvf9UZBd+QcFW3ZA26gx3xERIhiSvFwoRg2HNrQ+Cpeu4KTfyBAlJFKjeS35DO57UlE0dwHKO0fyHQ4hwsEw8B0/GtK7d5Cfug+Mnz/np+Q0IdEaXETI5HvT4J00H6WvD4Tq3TF8h0OIoHiuSoZ7+h4UzU9CRes2TjknpwmJ1uAignXxInzHjET5M61RaOeS+YS4Gre//oD33JlQ9+oN1Yj3nHZeTgc10BpcRIgkygKgb1/Aw71yEIOnJ98hESIYkrxcKN4dVtlv9EWyU2/WOO9D4mINLkJsptXCd8xI4N9/ofxpF7T1G/AdESHCwTDwnTAG0jvZTus3MsR5QuJiDS6gZiwMKaZYAZHEO3MmsG8vkJwM/z49+I6GEEHx/Hol3PemoWjeQqf1Gxly2ii72NhYzJw5s8oaXDKZzOY1uFx9YUgxxQqII1556s/wmzsXqrhB8Bw92up4q1sYkhAxczv5J7znzIS6ZwxUIx2/Th0bnPUh0RpcREhkFy/Ad9x7KG/TFkULP6dBDIQYkOTnVfYb1Qtxer+RIc5qSLQGFxEKSX4e/AYPBOPtDWXKRlqJgdRYx87fwbYjV5CjVCNI4Y7+kWHo8GRd+E54H9Ks28jflQ7GP4C3+DhLSLQGFxEEjQaK996B9PYt5G9PgzYklO+ICOHFsfN3sH7PRZRVVK5on6NUY/2ei3jspw0I3pOKojmfoKJNO15jpJUaiEvzXjAX8oMHULjoC1Q89zzf4RDCm21HruiTkU7jW5cQ/v1CqHv0gmrU+zxF9hAlJOKy3Hdug9eyxVANGobSwcP4DocQXuUo1VV+9i4twrTUz5DrHQDpFysF0a9Kq30TlyQ7dxa+E8ag/Ln2KFrwGd/hEMK7IIX7wx8YBuP3rUBQUQ6+emM6mABhDCqjhERcjiQ3B35D34JW4YeCb74F5HK+QyKEd/0jwyB3q7zkx5zajRf+OY6NLw7GM2/H8BzZQ9RkR1xLRQUU7w6H9E4W8nfuAVO3Lt8RESIIHSIqN548+d0eDD+yDqeaPYfAmQn6x4WAEhJxKd5zEyH/9RAKlybzPmKIEKF5oYEHXklfAoTUQ4Off0D9wCC+Q6qCEhJxGe4/bYHXl8uhGj4SpXGD+A6HEIcxOX/I2poNw8B30jhIb99E/s69YASWjABKSMRFuJ05Bd9JY1HWoSOK5i7kOxxCHMbc/CEAViUlj5TVcE/diaKZcwU7BYISEhE9yYMHUAx9C9rAICjXbABq1eI7JIfJy8vDBx98gBs3bkAul6Nx48aYM2cOAgMDERUVBblcDnf3ytFT8fHx6Ny5M88RE0czNX+orEKLbUeusE5IbmdOwSfxQ6i7R0M1ZhwXYToEJSTCOYc0N5hTXg7Fu0MhvX8Pv37xHdZuvYwc5Vmz59HFkqtUI9DRsXBAIpFgxIgReP75yjvapKQkLFq0CJ988gkAYNmyZWjWrBmfIRKOGc8fqu5xY5JCJRQjhkBbOxiFy1YBUuEOrqaERDh17PwdrE27gApN5crsOUo11qZdAGC5ucE4idUJ8MSlG/nQMoBUAkS2CsXjDfzhkTAVL5/4Fcm9J+HAFTdUaNT686zfcxH/3MrHmSs5yFGq4e0hg7pcWyUWW5o+nMnf31+fjACgVatW2Lx5M48REWcLUribTD5V5hWZwzDwmTwe0ps3kL9jD5gg4fUbGaKERDi1+cBlfQLQqdAw2HzgstkkYKrN3LBAahngUEYWJJs2YdKJndj5bG/sfSISMDpPWYUWhzKy9D8Xl2oeOZe1TR980mq12Lx5M6KiovSPxcfHg2EYtGnTBpMnT4ZCoWB9vJqwpxgg/niHxkRgxdbTUJc//P6615JhaExE9X/bl18CO7cBCxciIKY757HaixIS4VSRqsKqxwHTbebGHr/zD97fvxKnGz6NlBeH2hMi66YPvs2dOxdeXl54++23AQCbNm1CSEgIysrKMH/+fMyZMweLFi1ifTxX31MMcI14Ixr5Y3CP5o80e0c08rf4t7mdPQ3/iRNR1q07lEPfAxz8Ptjy3la3pxglJCI41SUI/+J8fPjzAuR5++PTmKnQSmV2nY9V0wfPkpKScP36daxatQrS//oAdJtayuVyxMXFYfRofjZVI9zrEFHPqlq8pFAJ3xFDoA2qjcLlXwm638iQOKIkouXtYTpZmHscsJwg3DTlmJb6KRSlhZjfZzqUnuybqEyRu0nRPzLMrmNwbcmSJTh37hySk5Mh/28ZpJKSEhQWVt6dMgyDtLQ0hIeH8xkmEQqGgc+U8ZDduA7lV2vB1K7Nd0SsUUIinIrr3hwyo0WEZZLKx80xXHPL2IjDKXjqdiaWvTwW1+o0feS4Pp6Vlf4ghTu6tg595Di650j+e86Qni0E3X/0999/Y9WqVbh37x4GDhyIvn374v3330dOTg4GDRqE3r17IyYmBlevXkViYiLf4RIB8NiwFh47tqE4YQYq2nfgOxyrUJMd4ZTuYm/NsG9Tr6kT4ImGu7fildN7sL1tLGRvDsSIBv7VHvdxM88RS9/CE088gUuXLpn83Y4dO5wbDBE82bmz8JkxDWVdu0E1bhLf4ViNEhJxOOMh2y3Dqh9qWt1rervdQ6fDq1EW2RWdNqegk1vlV7e62o1x2/ux83cwdeVR0cxDIoQtSVEhFCMGQxsQCGXyatH0GxmihEQcytSQbcOh16bm/lT3Gm1WNiI+i0dJ7bpQfb0WcLPta+uoJVgIERyGgU/8BMiuXUXB9t2i6jcyxFlCoiVPaiY2Q7aN5/5Yeo1bRTmm70qCl7oY8/rOwwQ7NhJzxBIshAiRx8b18Nj2I4qnf4zyDh35DsdmnCUkWvKkZmI7p8fweZZeM+rQaoRnX8LCmKk47RnKSWximYdEiCmy8+fg89EHKIvsipIJU/gOxy6cNTKaWvIkKyvLwiuIK2A7p8fweeZe0+NMOnqc3Yctz72Ko8062j1fyNzrxTAPiRBT9P1Gfv5Qrlwjyn4jQ07pQ3L0kidAzVj2REyxApXxmlrmxJjxsiemXhN++wLePbgafzZ5FpteiGO/VIoFdi3BQojQMAx84idCdvVfFPy0C0xwMN8R2U3CMIx1a4fYYPbs2bh79y5WrFgBqVSK7OzsKkueFBcXW7XkCeD6y56IKVagarymRszpFjitbhXuHKUajzNKzEuZiEKZByYO/BQedYIcNhrOntW+q1v2RGxcvQwBrh2vx8b18J08DsXTPkLJlGkcR/YoUS4dREue1DzWLnNS5TWlpfDv2wOyijKod+3B8haOXX1Adx6xXagIMSTLPA+fD6eirHMXlEyM5zsch+E0IemWPPn666+rLHmi0Wjg6+tLS56QqhgGPtMmo1bGSRSs3QSNg5MRIS6hqKiy30jhB+XK1YDMvrUchYSzhKRb8qRJkyYYOHAgAKBBgwZISEjAuHHjoNFooNVqERYWRkueEACAR8rX8Ny8EcVTpqHsld58h0OI8DAMfD+YBNm/V1Dw489g6tblOyKH4iwh0ZInxBq1/u9/8JmRAHV0T5RMnc53OIQIksfmjfD48QcUf/Ahyju96JRzcrrjsxFxjxEkLkF66yYUIwZD81hTFCZ/Lfqhq4RwQXYhEz7T4yv7jSZNdco5daub6Obq6VY3OXb+Difno5JP+KVSQTH0LUBdBuWG78Eo/PiOiBDhKS6GYuQQMD6+Tu03srS6CRdoLTvCH4aB75TxcDt7Gspvv4fm8Sf4jogQQfJNmALZ35dRsHWnvt/IGU1pzl7dhGpIhDeeXyXD48cfUPLBhyh7uSff4RAiSO7fb4LHD9+hZMo0lL/YBYDzmtKcvboJ1ZAIL2r9ehjesz+Guldvh7WHm7pjBKzbi4kQIZFdvADfaZNR1rFzlcmvzloouH9kWJUV8gFud1mmhEScTnrjOhTvDoXm8SdQuGKVQwYxmNpaIiU1ExKpBBUaRv+YbruJPl1oqSAicLp+I28fFK76pkq/kbOa0mzZYNMelJCIc5WUwG9IHFChgXL9d2B8HJMYTN0xahjdPw/p7iL7dKH+KiJsvtPjIbt8CQVbdkBbt2oCCFK4m0w+XDSl2bLyiq2oD4k4D8PAd9L7kGWeg/LrFGiaPu6wQ1tzZ0jbTRChc/9+Ezy+34SSSVNRHtn1kd/3jwyD3K3q5ZvLpjRnoRoSh5w5oUwMPJOXwWP7TyiaMQvlUd0demxzd4zmnkuIUMkuX4JvwhSUvdDJ7CRxZzelOQslJI7QdtlV1Tr0C7znJaK0Tz+oxk1y+PFNdb7KJAADCbQGC9q7ySSiuou0tPPy1atXkZCQgPz8fPj7+yMpKQlNmjThO2Rij5ISKEYMBuPl9Ui/kTFnNqU5CzXZccTZE8qETHr1XyhGDYOmeTgKv1gJSCQOP0eHiHoY0rOFvvYTpHDHi61CYby7ikbD+W4rDqXbeTk9PR27du1Cw4YN9Vu1JCYmIi4uDunp6YiLi8PMmTN5jpbYbdw4yC5dhDJ5NbT1QviOxukoIXGEtsv+T1ER/IbGAQAK1n8HeHtzdqoOEfXw2ZiOSEmIwmdjOuL3C3dhnH4YAN/tN73GohCZ23k5JycHmZmZiImJAQDExMQgMzMTubm5fIVK7OS+ZTOQkoKSiVNQ3rUb3+HwgprsOOLMUTCCxTBQjB8N2aWLKPh+G7RNHnPq6YtLTe9aa+5xoTPceTk7Oxt169aF7L8mHZlMhjp16iA7OxuBgYE8R0oMselLll2+BN8PJgEvvoiSqR86LbYZq48hK0el/zk0yBPzRnZw2vmN1YiEZM8uobZy9oQyIfL64nO4p+5E0az5KO8SVf0LDNCAkEfNnTsXXl5eePvtt5GZmWn38Wzd/VZs273zGe/hv25iw95LUJdX3gTlKNXYsPcSFL4e6NKmYeWTSkqA94YBXl7Ad98hOCTAKbGNSTpQJRkBQFaOCokpJ7By2kusjuHo99blExJfgwtcdRQMW/L9e+G1YC5K+78O1eixVr3WUZ+Zj6cbilQVJh8XG+Odl0NCQnD37l1oNBrIZDJoNBrcu3dPvxszG7SFOffWpZ7XJyMddbkG61LPI6KRPwDAZ/I4eJ47h/zvt8G/fn2Hxmvpxu7mvWKTr7l5r5hVDKLcwpxvzlpiwxRXHAXDhuzK3/AdPRIVT7VE4eLlVg9icNRn9uZLzbA27YJ+pQagcpTdmy81syoevpnaeTkoKAjh4eFITU1F3759kZqaivDwcGquE5jq+pLdf9oCz43rUTwxHuVR7GolbIlxpK/LJyQaXOBckkIlFEPigFpuUK7bVNkMYSVHfWauUEs1t/NycnIyZs2ahYSEBKxcuRIKhQJJSUk8R0uMWepLlv3zN3ynTEBZ+xdQ8oHj+434vBm3lcsnJBpc4ERaLXzHvgfZlX9QsHUntA0b2XQYR35mYq+lWtp5OSwsDFu3bnVyRMQa5vqSX2tfH4p33gTj6VE538jN8Zfi6m7sQoM8H+lD0j3OF86Gfefl5WHkyJGIjo5G7969MXbsWP2Q1KtXr2LAgAGIjo7GgAEDcO3aNa7CcNklNoTIa/GncN+TiuI5n9i1vTJ9ZsRVmJofN6RnC3TbtARuF85Dmfw1tKH1OTl3dVtHzBvZ4ZHkw/coOwljPHPQQfLz83Hp0iX9HIqkpCQUFBTgk08+weDBg/Hqq6+ib9++2LlzJ3766Sds2LDBquNb0yHLxyg7e/HdGWut4OOHgT59UPrGmyhcvsruya9cj7LjokNWbGhQAz/ct22F4r13UDJ+MopnzKryO0fGa9yHBFTe2A3p2cIhZUlUgxpMTejbvHmzfkLf2rVrAVRO6Js7dy5yc3M565D951Y+8grVYADkFarxz638Kh+ILRc/Gpb8kOzvy8Bbb6G8VWsUfra02mTE5r1j09Q2fOHBRx5LSbA8vPzb9Is4cioLWgaQSoDIVqEYFN3C4msIcRTZlb/hM2UCyp9rj+KEGZyeS4x9qBYTUn5+vsUX+/v7szoJFxP62N6pfvnjKRzKyHoYCwMcysiCp0ctjH6tFbt5AkZseY0tRDHfo6AAGBYHeHqi1s87EdywjsWnO+q96z1lp8nHhy88iF2f9zX5u+q+C1xxVDkiIqdSQTFiKOAuh/KrFE76jYyJrQ/V4jvSv39/SCQSMAyD7OxsKBQKAIBSqURISAgOHnz0DtUUR0/oA9g3N+w9ft3s469FhpmdJ7B8SwaWbD5p8k6azdwCezmjqcHuWp5WC8XggZD/+y8kBw/ivoc/YCJmw/NIJZWJwJCj3zvD982wRmRO2rHK70J1bG2yc1Q5IuLm8/F0uJ0/i4LvtkJbvwHf4QiSxYSkKygzZ85Et27dEBkZCQA4cuQIjh07xuoEXEzos4a5C5HucXMjUcoqmCrP1d1ZD4pu4RJDyR0xR8Hr00/gvm8vChcsgm/nzmaTkeF5zH0eXLx336ZfrFIj4osjyhERN/cdP8FzQwpKxk5E2UvRfIcjWKxG2Z07d05fiAAgMjISv//+e7Wv003oS05ONjmhDwDvE/qsGUp85FSWxdeIaSi5vauRy1N/hvfiT6F6azBKh4+06jymcPHe6T4vobC1HBFxk/37D3wmj0d5u+dRPP1jvsMRNFYJKSAgACtXrsStW7dw+/ZtfPnllwgIsLzekm5C37179zBw4ED07dsX77//PgBg1qxZ2LhxI6Kjo7Fx40bMnj3b/r/ERqaGGJuju7t3hWHJ9tTyZBcyoRg7CuVt2qJo4ecWBzGwOR5X752VA8g4Z0s5IiJXWgrfEUMrJ4p/vRaoVYvviASNVa/a559/jhUrVmDs2LGQSCRo27YtPv/8c4uvEcuEPlMjUXIL1TA1GF4qMf8aoY9eMWbr5FNJfh78hrwJrY8PlCkbAXfLzzd3Hl1fkq3vXUpCVLWj7Ez1V5ni7WF+EzRHsqUcEXHzmTkdtc6dQcHGH6jfiAVWCcnf3x8zZsxAcXExvDncz4YvxiNRzPU9RLYKNfsasWGzGvkjgx46NUGP+WMhvX0L+dvToA0JNXVoVudxxFyI6oZ4R7YKrbYPSSYB4ro3tysOtly9HJGq3Hdug+e6b1AyZjzKXu7JdziiwKqt6uTJk+jVqxdeeeUVAMDFixcxa9YsLuPi1aDoFujaOlRfI5JKgK6tXWu+irkZ5LokoRuMoKvd5CjVqPhoBuSHfkHRgkWoeO55s8e25jxcMvU5hjf2R5DCHZL/Yhke86TTbixqWjmqyaT/XoHPpHEob9MOxR8l8h2OaLCqIS1YsADffPMNRo8eDQBo0aIF/vzzT04D49ug6BYulYBMsVTLMx6M0OnS/9D/xE841KYXnho8zGHn4Zq5z5GPGfw1sRzVSKWlUIwcCrjJqN/ISqzXsjMeli2V0u7nrsyw36fJ/auYkL4cmaHhWNbJumREqqJy5Pp8Zn2EWmdPo3D5VzYvMFxTsSoNISEhOHnyJCQSCcrKyvDNN98gLEw8I8qI9XRNbL4qJT76eSGK3b2xoPcH8AtwnbXcnI3KkeuT79oBz5TVKBk9DmXR1G9kLVZNdrNmzcL8+fNx9+5dREZGomPHjkhMFEe7qLeHDMWlGpOPiwVX6+ZZOm7/yDB8u/s8pu7+HEFFOUh44xOU+AVhiIiGtguNmMsRqZ706r/wnTgW5W3aUr+RjVglpKtXrz4yPPWvv/5CmzZtOAnKkeK6N0dKaiYMNg116sgqe3G162N1x+0QUQ9PfrkQj984jaUvj0NO86cxRGRD24VGzOWIVEOtruw3kkqh/Hod8N9CAMQ6rJrs5s2bx+oxIeoQUQ/DY57kbWSVvexdUcHW47r/+AMe35IC1fCReGvjfHw2pqNo3jOhEnM5IpZ5z56BWmdOoXDZl9RvZAeLNaSMjAxkZGQgNzdXv10EABQVFUGjebQZTKh0d/xC3BulOlytm2fpuG5nTsF38jiUdeiIorkL7ToPcZ1yREyT79oJrzVfoWTU+yjr+Qrf4diNz611LCak8vJylJSUQKPRoLi4WP+4j48Pli1bxnlwhLst2M0dt4lMBcXQt6ANDIJy9XoasuoAVI5cl/TaVfhOfB/lrZ9F8cf8LYHmKFx1EbBlMSE999xzeO6559CvXz/Ur8/NNrukKuPdbVuGBeHo2TsWV1SwhakVFDwlWiTuXwrpg/vI35UOpo7lvY0IO65QjsS46zLn1Goo3h3qUv1GlprynfF5s+pDmjFjBpRKpf7ngoICvPPOO5wFVVMZro7AoPLu5OjZO+j4dD2Hr3RgagWFhVd3oPapEyhc9AUqnmlt759DjNhSjpKSkhAVFYXmzZvj8uXL+sejoqLQo0cP9O3bF3379sVvv/3GWdymvpfr91zEsfN3ODunGHjPnYlapzJQ+MVKaBs34Tsch+B7ax1Wo+zy8vL0m4oBgJ+fH3JycjgLqqYyd3dy5koOPhvT0eHnM1xBwf37TVBs24CSUWOgfuNNh5+L2FaOunXrhsGDB+Ott9565HfLli1Ds2bNHB6nMb7vmoVInpYKr6+/RMm7o1HWK4bvcByGqy4CtljVkKRSKbKyHi5Sefv2bUgsbDlAbMPX3Ylbxl/wnToRZZ0jUZxIo764Yks5atu2LWebV7LF912z0EivX4PvhDEob9UaxTPn8h2OQ/G9tQ6rGtLEiRMRFxeHdu3aAQD+/PNPzJkzh9PAHEks7d983J1I7t2rHMRQp25lO7gbq68EsYGjy1F8fDwYhkGbNm0wefLkKrUvR+L7rllQysoq+40YpnLQjwv0Gxnie2sdCcOY2vnnUbm5uTh9+jQYhkGrVq142+FVJyenCFoWm90YjxoBHLf9gSMYDrH08XSDqrSiyiReTmMtK4P/q73hduYU8lL3Q/N0S5sPJbYh9bbEK5VKEBRk39JJtpajqKgorFq1St9El52djZCQEJSVlWH+/PkoLi7GokWL7IrNnMN/3cSKraehLn84RN29lgxjX38GXdo05OScgjV5MrBkCfDTT0D//nxH43Is3g5fuXIFYWFhOH/+PACgzn+jrrKzs5GdnY2IiAjuI7STkNu/jZNlkaoCbjIJvGtJUVKq4bw25zNjGmqdOAblqm/sSkbEMi7Kka4ZTy6XIy4uTr+CuDXY3tRFNPLH4B7NH2lliGjkL/ibEEfeKMn37IbfkiUoGTEKxZ27Axz87ebi5XNukDlc3NRZTEhr167FvHnzsHDho5MjJRIJNmzYYFUwfBBy+7epZFmhYeDn7Ybv58dwWtg9Nq6v3Dzs/QlQ93+ds/MQx5cj3ZwmX19fMAyDtLQ0hIeHOypcYoL0xnX4jh+N8mdaO72fle+5Qabi4aoLxGJC0i1r8u2331p94KSkJKSnp+P27dvYtWuXvqkhKioKcrkc7v9tfR0fH4/OnTtbfXy2hNz+zdsghj9OwCdhCsq6RKF4xixOz+VMQryLBOwrR/PmzcO+ffvw4MEDDBs2DP7+/li1ahXGjRsHjUYDrVaLsLAwThdpFdoF0enKyqAYNQzQaqFcvQ5wd+61g20rz7fpF3HkVBa0TOVmlJGtQvF4A3+HlgmuvwsWE9K+ffssvvjll182+zshDFcF2G3VzRc+kqX07h0ohg+CNiQUyq9SAJl4Vj23hG1B4WOAiz3laMaMGZgxY8Yjj+/YscPesFgTcrO3M3jPm4Vaf/2Jgm82QNvkMatfb++NEpsb12/TL+JQxsMRnFoGOJSRhSOnsqH9b5iAI5IH198Fiwnp0KFDAICcnBxkZGSgffv2AIATJ07gueees1iQ2rZta3dwjmA4akRoo+ycnizVaiiGvQ1pYSHyftgOJoDfgSmOxKag8HWnb085EgKhNXs7syYsT98Dr1UroBo+EmW9Y61+vSO+c2xuXI+cynrk9wD0yUjH3uTB9XfBYkJasGABAGDUqFHYvXu3vjP23r17ohiuqsPX4qrVFRynDrFkGPhMj0etP39HwTcboHlS+ANSrMGmoPB1p89VOXIWITV7O/OmQnrzBnzHjUL508+gaNZ8m47hiO8cmxtXFmNT9OxJHlx/F1hNOrl9+7a+EAFA7dq1ce3aNZtOuGnTpirDVefMmWPTcFVbh98GB/va9Dpjh/+6iQ17LuBBngq1AzwxuGd4lSGwh/+6iQ17L+mHyuYo1diw9xIUvh5Vnteniy/6dHmC01gBAKtWARvXAx9+CL/hgxx3XAMOjdfacwd44n6eyuTjurhyzRTEXKXaKbE7shw5E9/N3oY3dlLJoxdfTm4qysuheHcYUKGp7Dfy8LDpMI6oUZi7cQWAqSuPWp1g7EkeLcOCqjQNGj7uCKwS0nPPPYd33nkHr7zyCiQSCXbv3o3nn3/ephM6YrgqwH7IqiFH1ZCM79Lu56mwfMspKAtL9V+edannq8zbAAB1uQZfbT+Ddannq60RObI253b8GPzHjUNZt+5Qjpvq1OGqzhLb6TGTF83YTo/p4wo0c3cXqHBnFbu985AcWY6cqUNEPfxzK79Kh3nHp+s5pdnbuKyZK/KObj70/mQOav31B5Jfm469W64jSHHHptYLR9UoDJf5AkzPrzRFKpFUabaz90bizBXTS12Ze9xarBLSzJkzsX//fvzxxx8AgAEDBqB79+5Wn4yv4aqmRp8Mim5h8/HYVMPNFZAiVQWKVBX653DdhyHNug2/dwZB06gxCld9Y/cgBlva7z/bfBIXrufrfw4N8oS6XFvlGP87k1Xtcyydh81dpKlt6yUSOO1O31HlyNmOnb+Do2fv6JOBlgGOnr2Dxxv4c56UTJU1UxzZfCjfvxdeyV9gb6ue2Nuo8obB1rLKVe2yuveFq1F2XPchsV6p4fbt27h+/TpeeOEFqFQqaDQa+PiYv1s0HK4aEBBgdrjqjBkzqjRjsMW2hmQ8+kTHvZYE6nLGpg9p+MKDZn+nuyMy1bRgju65hrE4pMZRWgr/vj0gu3wZ+XsPQtPcuiRsnHxMbYXhJpPA3WgiL/AwMcgkqLLyhK1sWbGC7V1k19bsblAcsVKDteWIS2zLkLlmoSCFOyeL/hqyVNZ0LH03rC1H0tu3EBDVETfdAzDpjYUod6u6NJAtf7M1N3Fs47X0vqQkRFkVnzXs/S7YNTFWZ8uWLfjhhx9QUFCAAwcO4O7du0hMTMT69evNvkYIw1UB4LCJZAQA6nLbh0Kaq4brjgdY18moe65hLH26sO/TMPWFB8PAf/JYvHjqJJYN+BhNKvzRgX1IJjuPTSX2Cg2DCs3DfrK1aRfAaBl9EnJEMgJs6ydge3d95FSWXTVmtmwpR0LA5yg7c2XN1E2c3XT9RmXlmB8b/0gyAmz7m42b2xyBr4EmXPcnskpImzZtwtatW/HGG28AAJo0aYLc3FyHBMA1NtdDR4x6cRRdLIYDHSzdYZlKHGvTLqDnX6noc2o/NrcfgP3120DOIulW13nMRoWjMpAJ1l4M2D7flr/TFmItR3yOsjN3AeRifUfvBXNR648TUH6VgrLsEEAgIwtN4WugCdfTaFglJLlcDrnBqrYVFRUOObmQ2DvqxZF3izlKNfpM2Wlyx1jjGp2pWkCLa2cx/HAKjoc9h80dBgCoPumy7Tzmk7UXA7afi9RJO6mItRz1jwzD2rQLVW423GQSp/S9OWtqhPxAOrxWLIVq8HCo+72G/mYWZRbChHqA31W5uZxGwyohtWvXDqtWrUJpaSmOHj2K7777DlFR3LVTOpJMCmhYVGTsHfUyIumgQy/iup05TTWTGSYX4wtusPI+pqV+hmz/ECzuMRGM5OHeJpYuzmybt/hiy8WAbU02slWoPaGxJuZyxBh9uY1/5hIXTV6GpLdvwXfsKFREPI2iuQv05wT424aBDa7fFz6wSkhTp07F1q1b0axZM/zwww+IjIzE66+LY0FOT3c3/ag2cxxx5+PsGoUuuRjWAuTlakzftRC1NOWY32c6VO5eVV5jqSbApiYhd5Oi49P1cOZKjlNn6dt6MTA1XLleoCfu5KocNuLSGmItR9uOXHmkL1DD4JEat1DXErSovByKUcMBdRmUa9YBnp76X5m64IvybxSRahOSVqtFnz59kJqaqm/7FpPqkpGjvlS29rnIJICnR/VJ05iuRqevBZRrMPbASoTd/Rfz+k7H7cD6j7zGUny2dB6zGW1oD6kEWDPN9hqEqeHKDwrUeCfmSfTp8oRT502JuRyxGdQg1gVYvZPmo9bvxyu3YAkzPUFdR6x/oyVCS7DVJiSpVIrmzZsjKysLoaHOadpwFkcOj2STjIxrGMZfALazrg1rdLrXFi1chK4XjuCnroORGdEBKNU88jpLzZK2dB77eJpOpD6ebvohoLbMJNextynN0nwxc6tjcEXM5cjczZZhjdtRyzKZHDEKy01nts4z/GftFnRYthh7n34Z398JRf/zdyzW+ErLKkz+jat3ZWL1rkyzc39ahgWZLfPWvA+OThS2JlhHz+s0xGoe0uDBg3H27Fm0bNkSngZV2lWrVjkkCFuwnUPhrPH61V142Xxw5na3tZTEAKDWr4fhN6Afynq8AuU3G3Dswj2bRiZZWwiOnb9jsrN7WK9ws6MATZG7SRFWX4FLN/Id+iW39Nnv+ryv03eMFVo5ckQZspanXApVGf99lUGFD/DFxsnI9Q5A/JufoqwWf6PnPOVStI+op7/ISyQAu9mh7I5t7fvtXkuG8gqNviyyudl21Fw+Vn1IY8eOZfM0QWJzd+cI1XWgs5ndbsuQSun1a1C8OxSaJ5qhcPmXgFRqc4estZ2kbOI1FYstd4y2ENKioIC4y5GjCCEZSbUaTE37HPKKMiTFfMBrMgIq3xPDwUuOSka6Y1vLcMkztt0QhzIcM5fPYkJSq9XYvHkzbty4gWbNmuG1116DmxurHCYYka1CTY5Uc/TIKuMLr62LQFo1pLKkBH5D3wI0WhSs+w6Mz8PJtFyNwDFVi/psTEeL8fI1GojvRUF1XKEcuZK3/m8zIm5fwKKek0z2tRL+WCwV06ZNg5ubG9q2bYtff/0V//zzj8nVF4Ts8Qb+JhPSHxfv4VBGlkPv0P+5lY+8QssrNdjSn2LcZlsv0BN3ckowefdidL50DjumLkXnpmEWX2OqCay655hagy4r5+GK2jlKNVbvygRgeWUJLtucLeFzUVBDrlCOXEXraxl44/cfse+pl3AkPJLvcIgRiwnpypUr2LVrFwDgtddeE8UQVWNr/rtgGrN3gVPjmkKdAM8qF29z3GtZt7ipqZ0gs3JU6PfHDkRe+g3rOg3GT5rGuJZ+UX+RN7d7JIBqn/N/57KhLmfgJpM8suqCYTIylLI70+wgATaxcMXSoqDWLM1kL1coR64gsDAHU/YswbWgRvi660i+wyEmWExIhs0KYm1iYLt0kG6kjC1ylGrWNR91ucbuTuLW1zIw5H/f4n/NXsBP7foBqLzIm6oJGmLzHN0af9YsAaTRAr2n7GT9fLaxVMeWDltTnzWXi1ECrlGOxK6y32hxZb9R7w+g5rnfiJhmsXRcvHgRzz77LACAYRio1Wo8++yzYBgGEokEJ0+edEqQ5KF6+dmYuvtz3AhqiKXR4yuH5NRQjuogH77wIKdJicoR/+KOfY+nbp/H5z0m4lZgA77DIWZYTEgXLlxwVhyEBY8yFT7aWbm0yfw+06GuZdsulsS5qBzxq9X1U3j9RGW/0eEnu/AdDrFAWv1TiCAwDCamL0PD3Fv4NCYed/3FOTOcWCcpKQlRUVFo3rw5Ll++rH/86tWrGDBgAKKjozFgwABRbIXOh8CiXExJW4KbQQ2p30gEKCGJxOu//4iOfx/D+k6DcKpxK77DIU7SrVs3bNq0CfXrVx2enJiYiLi4OKSnpyMuLg4zZ87kKULhkmo1iE/7HB7lpVgYM5X6jUSAEpIItP33T7x99Dscad4Z29vG8h0OcaK2bdsiJCSkymM5OTnIzMxETEwMACAmJgaZmZmi2FvJmd489gOevnUeK196D7eCGvIdDmGBhvwIXGjebUzZsxhXg5tg+ctja/QgBlIpOzsbdevWhUxWOYVAJpOhTp06yM7ORmBgIOvj2Lsdu5A9c/003jixFQcionDoya58h1MjBAfbP5WCs4SUlJSE9PR03L59G7t27UKzZs0AVLZ9JyQkID8/H/7+/khKSkKTJk24CkPUPNUl+GjnAmikbpjfdzo1ORCHYruWndgEFOViyp7FuBXUAKui3uU7nBqDzdqQ1a1lx1mTHbV920fCaDF571LUz8tC0itTcV9Rh++QiECEhITg7t270Ggq1xzTaDS4d+/eI017NZFUq8GUPUvgVaZC0itTaSSqyHCWkKjt2z4Djm9B+yu/45vIYTjb6Gm+wyECEhQUhPDwcKSmpgIAUlNTER4eblVznasacHwLnrl5Fl92G4UbtRvxHQ6xklMHNVhq+yYPPXfld7x17Hv88mRX7Godw3c4hEfz5s3Diy++iDt37mDYsGF45ZVXAACzZs3Cxo0bER0djY0bN2L27Nk8R8q/ljfOYODxLfjlya74JaIb3+EQG4h2UIOrdsg2yLmJKXuW4O+6YVjZ7T0axOAkjuiQ5cKMGTNMLsQaFhaGrVu38hCRMPkX5yE+bTFuBdbHl91G8R1OjWPtGp3mODUhGbZ9y2Qyu9q+XbFD1ru0CDN2LkCZTI5P+iTwvk9LTeKIDlnCj8r5RovhVVaCGa/Npn4jHhjuoWQPpzbZUdu3eRJGiyl7lqKu8i4W9v4AD3yD+Q6JEFF448SPeObmWayKehc3ajfmOxxiB84SErV9Wyfu/zaj3dU/sbrLOzjfIILvcAhxKK526a3sN/oBh8IjcaAG9Rv5e9fiOwROcNZkR23f7L1w+f8w8MRW7IvohrRnevIdDiE2Md4l2VMuRfLkLvqf7d12xZh/cT6m7FmCrIBQm/pbwxv749KNfNbbdHPJeCsVT7kU6nKtxU0tDfdkk0kl0PD4h8ikjunrFu2gBlfR6MF1TExfhov1muFLGsRARMjSBXPqyqP6TSztPYfh9Vaq1WDyniXwLi3GzP6JKJV7sj6WqV2iHZ0sdduZjPz0IDQsdkkx3krF8GfDjSUNY+4QUXX3Y+Mdnp2ZpDzdRTiogVTloyrEjJ2fQCX3xILe01Dh5prVcOK6zO0jdez8HazfcxFlFZUXVrYbWJqzZlrleXSJ47Xff0LrG6exrPv7uB7chNUxpJKHxzHm4+mm30XaXobJV+uYLbtQVqHFtiNXLO5qPfXNZ6v8XF2SDVK4o2VYEM5cydHfNFj6nFISoswe01HvHSUknuhGBtUuzMGHb8xDrm8Q3yERYhfDJiTjGo09vD1k+poWADx18xzijn2PQy0isf+pl1gfR8tUXqRN1egYxjHByiRA/8iwh8d1yFEr5SjVGL7woMkaninmEkyQwh2fjelo8jUjkg6a/Nx0LXLmErePp2NSCSUkngw6ugltrmdgxUujcTG0RfUvIJyhRlLb6S6QLcOCcPTsHX2NyJEtRSVqDYpLK4cV+5XkIz7tc2T7h2DlS7Y1cWsZ4FBGFu7kluBensru2pshBhL870wWvknN5KxvKkepxvo9FwHAYlLqHxmGNbsyH0mKuYVqfWJu3shf/x4EKdzRvJF/lWY/nchWoQCAMjPDu809bi1KSDzodOl/eO2PbdjTMhrpLaP5DqfGk8koJdkjR6nGoYwszo6vq7xUTo1YAp/SYsyyst/IFFMXXntpGYaT4xoz1YRnWEMNUrijToCnyRqa7v3UMlXfgxylGrmFppPzoYwsi59xWYVjsi8lJCdrcv8qJqQvR2ZoOL7uOoLvcAiACo0AhlmRar3++09off00Vrw0GtdY9hu5shylWt+U6ePphmJVhT4B5SjVNtX8HNRyaTNKSE7kq1Jixs4FKHb3xoLeH6BCRoMYCGEj4tZ5xP3fZhxp3hnpT7/MdziCoUs6jhpUwDfaMdZJpFoNPti9CIHFuZjfJwH53gF8h0SIKChKCjB19+e441cPyd3H0NQIF0YJyUmG/rYBrW6cQXK30fg7pBnf4RAiChJGi8l7lsK3tBALe0+Fys5+IyJslJCcIPLCEfT7ayd2teqFX56qOcubEGKvV//YhjbXM7C66whcC36M73A4I5M8HFotRo5aGor6kDgWdvcKxu1Lxrn6Efgmcjjf4RAiGk/eOo+3j36HX5t3wl4X7jeSSYDhMU+iQ0Q9BAf7os+UnQ6dv8Q1uZu0ytwre1BC4pCipAAf/rwQSi8FFvaeCo2M3m5C2FColJiathh3/epixUvi6DeSSirDZLNUkI6pSa5dWodWO4xe7iZFx6fr6VdZ8PaQoUStqTJKTiKxftScm0yCzi1DqqzeYByfbnh5rlKNQJaTdFmf3yFHIY+QaSowLfUz+JUUYNrAT1Dg5c93SITwTu4mga+XHDlKNeRuEpPzVySMFpP2LK1MSm8mQeXuZdU5JAAkEgm0ThjDbFi7ASov1mvTLlQ7lSC8sf8jS/3o6FaQOHIqS7+4qvEEVlNJwHgekq7WYjw3SbegLNvjGtOtoRcc7MtqHzFrUELiyPBf16HlrXP4vMdEXKn7ON/hEMIpT7kUb0e30F/8jOfFAJUX7yE9w6tc8L5Nv1jlwuteS4pe/9uGttdOYmW3USgNj0DXRgFVnmO87I/xMSJbheLxBv5V7uKN12wrLavQr/5gyHBZHUvHNXcB1/3f8DnutaTIylHpn2MpGekMim7xyGK11TFebNU4Jt36glqDibFXbisxpGcLh9Vw7EUJiQNR5w+iT0Yqdj7bG4ef7MJ3OIRwxnhxVUsrB5i6+za+8LqdOA7/zzahtE8/vL76U7z+X1OduYvzsfN3cPTsnSoXWd3K2J+N6Wj2Lt7cIqGGk0nNJQW2NQih2Xbkin5pJx02i7Y6EyUkB3vizt94/8CXON3waaS8OJTvcAjhjbUXZkluDhSjhkHbsBGKFi9j1W9k60XW0sKjrsrcyg2OXMvPXjTs24H8i/Px4c8Lkeftj09jpkIrdcweIVwRSjexp9z019Dbo/L98/F0e+Ta5CaToGvrUP0FxJ4+b0dtLsaHqKgo9OjRA3379kXfvn3x22+/8R2SbbRa+I57D9IH96Fcsx6Mwo/Vy2y9yPaPDIPcrer3zpGjxYTIXLIVUhKmGpKNpEadpl4SDRYcWgLf0iJMHbgAteoGI9So7dhWoUGeVY5jvLsk+5grmzTMdXgat3WHBnlCXa5ltVeKuXOZGunjJpNgWK9wq5p32PyeTWeyKXzutOkIy5YtQ7Nm4p5s7blyOdz3p6NwwWeoaNmK9etsremY6utx5GgxIeofGVZljypAeEmYEpIBXXu4pY2tdM8xvkAmnvwWDS6fhvKrFHzc7zV8m36R9QrIPp5uWDbhxSp7vhhSl2vNboTGdqdLuZvUZOelcYenoQcF6iqvMRefMeP9Vtj0JVTXvMPm90DVC4y5jmtT8RLzvD1kJt9HXQ3WXm6/n4D3/FlQx/RF6fB3rXqtPRdZofb1cEUMSZi3hBQVFQW5XA5398qLQXx8PDp37sxXOFYz/DJ7bFwP35+/Q8nYiVD3ew1A5ZBNtnSbg9nS/GDuDtHH0w3utWSsv3hs2uJNFX5jpi4Gzir4xucx3rXUFKHdIdoiPj4eDMOgTZs2mDx5MhQKhUOPH9e9OdakZj4yxyWue3O7j63vN2rQEIVLV1jd9iqGi6yQCD0J81pDcoWmBrc/f4dPwhSUdYlC8UeJ+setaQXS3X3a0vxg7g7xzZeaWfXFY5MMTRV+3XBaLibJ2Uts8dpi06ZNCAkJQVlZGebPn485c+Zg0aJFrF4bFOTD6nkK33xIJRJoDDKSVCKBwtcDwcG+NsUNoLItd3gccP8e8H//h9phDWw6TJ8uvujT5Qmzv7crRh4EB/vi8F83sWHPBTzIU6F2gCcG9wxHlzYN+Q7tEY5+b6nJzgS2iUF6JxuKYW9DGxIK5VcpgOxhE4Y1WzjrjmtL84Oj7hDNxWvc32/uDouLSXKOILZ4rRUSEgIAkMvliIuLw+jRo1m/NienCFoWX9J1qecf6WfTaBmsSz2PiEb+VsVryHPlcvikpqLwk09R2qgZwMHnIbbPOTjYFz8f/rvKdeB+ngrLt5yCsrBUUDdPtry3UqnE4o0QrwnJnqYGtnd3bPl61dJn+6ExEVix9TTUBtvyuteSYWhMxMM7ArUa6DsUKCoE9u9D7WaNqxyvR/vGSDt2/ZHzGHfyGx63TxdfKHw9rL4zqu4OkQ1z1yUtw/4uSAx3ol/+eAp7T9yAVstAKpWgx/ONMPq1VnyHZZOSkhJoNBr4+vqCYRikpaUhPDzc4efhYriw25+/w3teItSv9EHpO6NsPo4rEsN8Ia7wlpDsaWoA2N/dsSFBZT9O7yk79TWMwT2aP1LriGjkr78j8JkyHp7HjqHgmw0oq9fkkbu71yLDoCotZzXT2/C4EY38kTSqQ5W7D2fc4VmqFbI5v1DvRA0HVBgvVaPVMkg7dh2q0nJWs+Kru7tztpycHIwbNw4ajQZarRZhYWFITEys/oVWYlt7ZkuSlwvFu8OgDW1gU7+RqxPDfCGu8JaQ7GlqcCRvDxnU5Vr9jos5SjXW77mIIT1bVBkpZshjfQo8v12HkglTUNY71uyxbZ3pzQcxDAm1lvGgBlPrpgGVA1CsXaZFCBo2bIgdO3Zwfh5LtWerMQx8J4yB9O4d5KfuA+Pnb09oLqkmTtrV4WVibElJCQoLK++muWxqqE5KQhQ85G6PzF3RVY9NcTt+DD4fToW6W3cUJ8xwRphO0SGiHob0bKH/0gcp3AW1xpUtTDV9mCLyaUicc+SESs+vkuG+Nw3FiXNR0bqNvaG5pJo4aVeHlxqSs5oaWMViRfVYmnUbfu8MgqZhIxSu+qbKIAZXIPQhodZi28Qh4oUanMJRtWe3v/6A95yZUPeMgWokPy0iYlCTh7LzkpCc1dQAAF3N7C3StXUoAPbV4xMZ1/HksFfhma/EvNfnoP2tUnRgt7oJ4Qnb1SUiW4U6IRrxMrxA2jpcXpKf91+/UX0UfpFM/UbVcLWbQ7Zcfti3qb1FDJevZ3P3d+xcNnymTETTrMuY3ycBZz3q4dJ/qxrUxC+NWJj6bCX//cOY2cqAmGbXHjgMA9/xYyC9k438Xelg/AO4CZKInssnJODh4AJThYlN9bjgs6Xoc+4XfNd+AI4/3h5AzRmGKWaWPluhjgp0RZ6rv4T73t0omvMJKp5ty3c4RMBqREKqjqXqca2jvyFu71c4HvYcvu8woMrvasIwTLGrqU0fQuGW8Re8Z38MdY9eUI16n+9wiMDVyITEZrFPAJDeugnFyCG4E1Qfi3tMBCOpOvKlJgzDJMRWkoJ8KEYOhbZeCAq/WEn9RqRaNS4hGc9N0c07Aoz6g1QqKIa+BajLcGHFRmjOlwEuNEeHEE4xDHwnvA9p1u3KfqOAQL4jIiJQ4zbos7Qshx7DwHfKeLidPY3CL1ejZc8XXG6ODiFc8lyzCu5pu1D88RxUtGnHdzhEJGpcDYnNvCPPr5Lh8eMPKE6YgbKXewKgvghC2HLL+Aves2ZAHd0Tqveo34iwV+NqSNXNOq/16+HKwtSrN0omxjszNEJEr7LfaBi0deuhcNmX1G9ErFLjEpKlZTmk169BMXIINE80Q+GKVYC0xr09hNiOYeA7aRykWbeg/CqF+o2I1Wpck53ZuSlNfOEX8yqgZVCwfjMYH+FvpUCIkHikfA331J0oSpyHinbP8x0OEaEal5AAE/1BDAPfUcMgyzyHgs0/QtuURs8RYg230xnwSfwI6u7RUI0ey3c4RKSoTQqA54ov4LFjG4o/SkR5VHe+wyFEVCTKAihGDIE2uA4Kl1NTN7FdjawhGap18AC8589Cad/+UI2bxHc4hIiLrt/o1k3k79wLJjCI74iIiNXoWxnpv1egGDUcmubhKFxKKxATYi2PlNVw37UDxR8mouI56jci9qm5CamoCH5D4wCpBAXrvwO8vfmOiBBRcTtzCj6JH0LdrTtU74/nOxziAmpmkx3DQDF+NGSXL6Hg+23QNnmM74gIERVJobKy3yioNgpXfE39RsQhamRC8vri88rhqbPmo7xLFN/hECIuDAOfyeMhvXkD+dvTwARRvxFxjBp3WyPfvxdeC+aitP/rNDyViNrVq1cxYMAAREdHY8CAAbh27ZpTzuuxPgUeO7ehePpMVLTv4JRzkpqBt4TES2G6fBm+o0eiIuJpFC5eToMYiKglJiYiLi4O6enpiIuLw8yZM7k/aUYGfD5OQFnUS1CNncD9+UiNwltCcnZhkhQqgdhYoJYblOu/A7y8OD0fIVzKyclBZmYmYmJiAAAxMTHIzMxEbm4uZ+eUFCqBN96ANjAISuo3Ihzg5Rvl9MKk1cL3/VHA5ctQrl4PbcNG3JyHECfJzs5G3bp1IZPJAAAymQx16tRBdnY2NydkGPhMGQ9cvYrCr1LA1K7NzXlIjcbLoAZLhSkwkN2CjEFBPuxPOHs2sHc3sHQp/Pu9YkvIvAgOFtd6ehSvuFhVhr76CtixDZg/H/69o7kLigNi+5zFFK+jYxXtKLucnCJotUy1z5PvTYPfrFkoHRAHj/Hjcf9+oROis19wsK9oYgVqRrxSqcS6iziHQkJCcPfuXWg0GshkMmg0Gty7dw8hISGsj8G2DMnOnUXAhAko7xIFeUKCy3/OfBJTvFyUIV6a7AwLEwCbChMbssuX4DtmJMpbtUbhZ0tpEANxGUFBQQgPD0dqaioAIDU1FeHh4axbGNiSFBVCMWIwtAGBUCavpn4jwilevl3OKEySgnwoBg8EPDygXLsJ8PBw2LEJEYJZs2Zh48aNiI6OxsaNGzF79mzHnoBh4BM/AbJr//UbBQc79viEGOGtyW7WrFlISEjAypUroVAokJSU5LiDazTwHT0CshvXUbAtFdr6DRx3bEIEIiwsDFu3buXs+B4b18Nj248onv4xyjt05Ow8hOjwlpC4LExen86H+4F9KExajPL2L3ByDkJcmez8Ofh89AHKIruiZMIUvsMhNYTLNQjLd+2E95JFUL01GKVD3+E7HEJER99v5OcP5co11G9EnEa0o+xMkV3IhGLceyhv0w5FCz+nQQyEWIth4BM/EbKr/6Lgp13Ub0ScyqVufbw/mQ2tjw+UazcC7u58h0OI6MguZMJj21aUTJ2O8o6d+Q6H1DAuVUMqnjkXjJcXtPUcO3yckJpC07wF8nekUd8r4YVLJSTNE834DoEQcZPJUP5CJ76jIDWUSzXZEUIIES9KSIQQQgSBEhIhhBBBoIRECCFEECghEUIIEQRKSIQQQgRBtMO+pVLbVmGw9XV8EFOsgOvHK7a/rzo1oQwBFC+XHF2GJAzDVL9DFyGEEMIxarIjhBAiCJSQCCGECAIlJEIIIYJACYkQQoggUEIihBAiCJSQCCGECAIlJEIIIYJACYkQQoggUEIihBAiCDUiIV29ehUDBgxAdHQ0BgwYgGvXrvEdUhVJSUmIiopC8+bNcfnyZf3jQow7Ly8PI0eORHR0NHr37o2xY8ciNzcXgDDjBYAxY8agT58+iI2NRVxcHC5cuABAuPEKkdDfKypD3HJaGWJqgEGDBjE7duxgGIZhduzYwQwaNIjniKr6448/mKysLKZr167MpUuX9I8LMe68vDzm+PHj+p8XLlzITJ8+nWEYYcbLMAyjVCr1/9+/fz8TGxvLMIxw4xUiob9XVIa45awy5PIJ6cGDB0ybNm2YiooKhmEYpqKigmnTpg2Tk5PDc2SPMixMYol77969zJAhQ0QT7/bt25l+/fqJJl4hENN7RWWIe1yWIdGu9s1WdnY26tatC5lMBgCQyWSoU6cOsrOzERgYyHN05okhbq1Wi82bNyMqKkrw8X700Uc4evQoGIbBmjVrBB+vkIj1vRJD3FSGqqoRfUiEG3PnzoWXlxfefvttvkOp1vz583H48GFMmjQJn376Kd/hEAKAypAxl09IISEhuHv3LjQaDQBAo9Hg3r17CAkJ4Tkyy4Qed1JSEq5fv46lS5dCKpUKPl6d2NhYnDhxAvXq1RNFvEIgls/WmNDjpjL0KJdPSEFBQQgPD0dqaioAIDU1FeHh4YKoAlsi5LiXLFmCc+fOITk5GXK5HIBw4y0uLkZ2drb+54MHD8LPz0+w8QqRWN8rIcdNZci0GrFB35UrV5CQkAClUgmFQoGkpCQ0bdqU77D05s2bh3379uHBgwcICAiAv78/du/eLci4//77b8TExKBJkybw8PAAADRo0ADJycmCjPfBgwcYM2YMVCoVpFIp/Pz8MG3aNERERAgyXqES+ntFZYg7zixDNSIhEUIIET6Xb7IjhBAiDpSQCCGECAIlJEIIIYJACYkQQoggUEIihBAiCJSQRGz//v1o3rw5rly5YvF569atg0qlsvk827Ztw5w5c2x+PSFCRWVIWCghiVhqairatGmDtLQ0i8/bsGGDXYWJEFdFZUhYKCGJVHFxMU6ePIn58+dj9+7dACqX7khKSkLv3r3Ru3dvfPvtt9iwYQPu3buHIUOGYNCgQQCA1q1b64+zd+9eJCQkAKicgf36668jNjYWQ4cOxYMHD5z/hxHiJFSGhMflV/t2VQcOHEDnzp3x2GOPwd/fH+fPn8fp06dx69YtbN++HW5ubsjPz4e/vz/WrVuH9evXV7ukR5s2bbBlyxZIJBJs3boVa9as0Rc0QlwNlSHhoYQkUrt378aQIUMAAL169UJqaipu3bqFgQMHws2t8mP19/e36ph37tzBpEmTcP/+fZSVlaFBgwaODpsQwaAyJDyUkEQoLy8Px48fx99//w2JRAKNRgOJRIKIiAhIJBKrjqVWq/X/nzdvHoYOHYpu3brhxIkTWLFihaNDJ0QQqAwJE/UhiVB6ejpiY2Nx6NAhHDx4EEeOHEGDBg3w5JNP4vvvv0dFRQUAID8/HwDg7e2N4uJi/etr166NK1euQKvV4sCBA/rHCwsLUbduXQDAjh07nPb3EOJsVIaEiRKSCO3evRsvvfRSlcdefvll/V4kffr0QZ8+ffTLwr/xxhsYOXKkvkN2ypQpGDVqFIYMGYLg4GD9McaOHYsJEyYgLi7O6qYKQsSEypAw0WrfhBBCBIFqSIQQQgSBEhIhhBBBoIRECCFEECghEUIIEQRKSIQQQgSBEhIhhBBBoIRECCFEECghEUIIEYT/B2MfTiZOoRQJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatterplot of actual vs. pred\n",
    "# specify the dimensions \n",
    "fig, axes = plt.subplots(1,2) # 1 row, 2 columns\n",
    "\n",
    "# this makes the individual subplots\n",
    "# Training Results\n",
    "axes[0].scatter(x=y_train, y=model.predict(X_train)) #first row, first entry (left top)\n",
    "axes[0].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[0].set_title(\"Training\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[0].get_xlim())\n",
    "axes[0].plot(x, x, color='red')\n",
    "# Validation Results\n",
    "axes[1].scatter(x=y_test, y=model.predict(X_test)) # first row, second entry (right top)\n",
    "axes[1].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[1].set_title(\"Validation\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[1].get_xlim())\n",
    "axes[1].plot(x, x, color='red')\n",
    "\n",
    "# tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 783us/step\n",
      "378/378 [==============================] - 0s 806us/step\n",
      "8.092685207724571\n",
      "8.019872546984406\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred\n",
    "\n",
    "trainpreds = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_train, trainpreds)) # train\n",
    "print(mean_absolute_error(y_test, pred)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12096, 8) (3024, 8) (12096,) (3024,)\n"
     ]
    }
   ],
   "source": [
    "# superhost_listings_filter = superhost_listings.loc[:, columns]\n",
    "# superhost_listings_filter.head()\n",
    "\n",
    "# y30 = superhost_listings_filter['availability_30']\n",
    "# y60 = superhost_listings_filter['availability_60']\n",
    "# y90 = superhost_listings_filter['availability_90']\n",
    "# y365 = superhost_listings_filter['availability_365']\n",
    "# X = superhost_listings_filter.drop(['availability_30', 'availability_60', 'availability_90', 'availability_365', 'has_availability'], axis=1)\n",
    "\n",
    "# X = np.array(X)\n",
    "# y30 = np.array(y30)\n",
    "# y60 = np.array(y60)\n",
    "# y90 = np.array(y90)\n",
    "# y365 = np.array(y365)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y365,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# use minMax scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 1000)              9000      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 250)               125250    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 635001 (2.42 MB)\n",
      "Trainable params: 635001 (2.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 17772.8223 - mae: 114.7382 - val_loss: 16436.1914 - val_mae: 112.2775\n",
      "Epoch 2/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16699.9688 - mae: 112.8547 - val_loss: 16249.3838 - val_mae: 111.4865\n",
      "Epoch 3/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16481.2715 - mae: 111.6999 - val_loss: 16134.0449 - val_mae: 110.8925\n",
      "Epoch 4/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16393.2227 - mae: 111.0273 - val_loss: 16151.1758 - val_mae: 111.8164\n",
      "Epoch 5/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16424.8145 - mae: 111.0223 - val_loss: 16073.1426 - val_mae: 110.4233\n",
      "Epoch 6/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16335.4561 - mae: 110.8759 - val_loss: 16528.8633 - val_mae: 109.4974\n",
      "Epoch 7/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16275.5752 - mae: 110.4846 - val_loss: 16021.4521 - val_mae: 109.9031\n",
      "Epoch 8/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16342.1758 - mae: 110.5938 - val_loss: 16063.4336 - val_mae: 109.3572\n",
      "Epoch 9/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16288.7959 - mae: 110.4456 - val_loss: 16022.7227 - val_mae: 110.2809\n",
      "Epoch 10/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16292.5088 - mae: 110.2688 - val_loss: 16199.6777 - val_mae: 110.9031\n",
      "Epoch 11/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16372.6035 - mae: 110.6113 - val_loss: 16092.2754 - val_mae: 111.1771\n",
      "Epoch 12/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16310.1504 - mae: 110.4670 - val_loss: 15960.4004 - val_mae: 110.0211\n",
      "Epoch 13/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16294.7422 - mae: 110.3186 - val_loss: 16058.8682 - val_mae: 109.3408\n",
      "Epoch 14/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16367.8545 - mae: 110.7403 - val_loss: 16009.7002 - val_mae: 109.3058\n",
      "Epoch 15/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16345.7793 - mae: 110.4230 - val_loss: 16054.2842 - val_mae: 109.0166\n",
      "Epoch 16/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16415.5430 - mae: 110.8124 - val_loss: 16046.9062 - val_mae: 110.6452\n",
      "Epoch 17/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16368.4375 - mae: 110.6129 - val_loss: 16031.5342 - val_mae: 109.5127\n",
      "Epoch 18/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16299.4736 - mae: 110.1305 - val_loss: 15973.8926 - val_mae: 110.4085\n",
      "Epoch 19/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16255.6201 - mae: 110.1459 - val_loss: 15976.7646 - val_mae: 109.5461\n",
      "Epoch 20/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16326.6309 - mae: 110.3167 - val_loss: 15994.4678 - val_mae: 109.5285\n",
      "Epoch 21/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16252.9990 - mae: 110.2696 - val_loss: 15994.4316 - val_mae: 109.8111\n",
      "Epoch 22/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16274.5010 - mae: 110.1628 - val_loss: 15968.3291 - val_mae: 109.5024\n",
      "Epoch 23/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16222.7656 - mae: 110.1423 - val_loss: 16036.7051 - val_mae: 109.1636\n",
      "Epoch 24/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16353.2773 - mae: 110.5250 - val_loss: 15956.9951 - val_mae: 109.3559\n",
      "Epoch 25/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16382.7588 - mae: 110.3987 - val_loss: 15959.3955 - val_mae: 110.5407\n",
      "Epoch 26/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16306.9004 - mae: 110.3220 - val_loss: 15939.9229 - val_mae: 109.5918\n",
      "Epoch 27/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16307.0752 - mae: 110.2732 - val_loss: 16071.1777 - val_mae: 108.9181\n",
      "Epoch 28/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16293.4580 - mae: 110.1925 - val_loss: 16033.6416 - val_mae: 110.9501\n",
      "Epoch 29/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16306.0273 - mae: 110.3181 - val_loss: 16135.2236 - val_mae: 108.9025\n",
      "Epoch 30/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16407.2988 - mae: 110.6258 - val_loss: 16218.2646 - val_mae: 111.6152\n",
      "Epoch 31/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16180.9814 - mae: 109.9457 - val_loss: 16046.3740 - val_mae: 110.3518\n",
      "Epoch 32/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16306.7051 - mae: 110.2417 - val_loss: 16030.2021 - val_mae: 110.5452\n",
      "Epoch 33/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16335.6777 - mae: 110.3874 - val_loss: 16210.0625 - val_mae: 108.6128\n",
      "Epoch 34/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16331.1592 - mae: 110.3175 - val_loss: 16005.0527 - val_mae: 108.9488\n",
      "Epoch 35/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16321.6270 - mae: 110.4538 - val_loss: 16137.5234 - val_mae: 108.8518\n",
      "Epoch 36/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16318.4004 - mae: 110.2141 - val_loss: 15978.1133 - val_mae: 109.5516\n",
      "Epoch 37/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16327.2422 - mae: 110.3576 - val_loss: 15976.0742 - val_mae: 109.2681\n",
      "Epoch 38/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16340.4512 - mae: 110.5752 - val_loss: 16265.3584 - val_mae: 109.1063\n",
      "Epoch 39/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16234.8086 - mae: 110.0407 - val_loss: 15965.5742 - val_mae: 109.2114\n",
      "Epoch 40/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16351.2354 - mae: 110.2995 - val_loss: 16019.9814 - val_mae: 110.2050\n",
      "Epoch 41/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16195.4678 - mae: 109.9166 - val_loss: 15923.6494 - val_mae: 109.9809\n",
      "Epoch 42/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16309.1494 - mae: 110.2747 - val_loss: 16031.3018 - val_mae: 109.0212\n",
      "Epoch 43/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16237.6787 - mae: 110.0019 - val_loss: 16127.7246 - val_mae: 108.8772\n",
      "Epoch 44/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16249.5264 - mae: 110.1630 - val_loss: 15994.2607 - val_mae: 109.5025\n",
      "Epoch 45/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16323.9961 - mae: 110.3274 - val_loss: 15963.3701 - val_mae: 110.4309\n",
      "Epoch 46/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16291.5713 - mae: 110.3100 - val_loss: 15976.2051 - val_mae: 109.5961\n",
      "Epoch 47/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16306.9678 - mae: 110.1488 - val_loss: 15962.2178 - val_mae: 109.3391\n",
      "Epoch 48/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16283.3701 - mae: 110.4060 - val_loss: 15978.4951 - val_mae: 109.1049\n",
      "Epoch 49/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16255.3242 - mae: 110.1609 - val_loss: 15962.2568 - val_mae: 109.3500\n",
      "Epoch 50/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16259.8457 - mae: 110.0241 - val_loss: 15923.9775 - val_mae: 109.9870\n",
      "Epoch 51/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16271.2471 - mae: 110.1499 - val_loss: 15970.2227 - val_mae: 109.4517\n",
      "Epoch 52/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16326.1543 - mae: 110.2806 - val_loss: 16027.4619 - val_mae: 109.3505\n",
      "Epoch 53/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16120.3174 - mae: 109.7297 - val_loss: 16007.3027 - val_mae: 108.8110\n",
      "Epoch 54/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16213.4541 - mae: 109.9935 - val_loss: 16041.6934 - val_mae: 108.7085\n",
      "Epoch 55/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16226.8408 - mae: 109.9111 - val_loss: 15984.9766 - val_mae: 110.1787\n",
      "Epoch 56/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16244.8457 - mae: 110.0395 - val_loss: 15900.1426 - val_mae: 109.9205\n",
      "Epoch 57/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16255.3760 - mae: 110.2215 - val_loss: 16078.2285 - val_mae: 108.8574\n",
      "Epoch 58/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16249.0527 - mae: 110.1094 - val_loss: 15964.6074 - val_mae: 109.2806\n",
      "Epoch 59/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16251.8213 - mae: 110.1252 - val_loss: 15935.7588 - val_mae: 109.3940\n",
      "Epoch 60/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16249.1914 - mae: 110.0748 - val_loss: 15953.5977 - val_mae: 109.5245\n",
      "Epoch 61/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16251.2383 - mae: 110.1388 - val_loss: 16095.4209 - val_mae: 109.0083\n",
      "Epoch 62/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16271.0850 - mae: 110.2150 - val_loss: 15920.3799 - val_mae: 109.7806\n",
      "Epoch 63/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16196.6074 - mae: 110.0170 - val_loss: 16074.1914 - val_mae: 109.1904\n",
      "Epoch 64/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16333.6377 - mae: 110.5000 - val_loss: 15985.1230 - val_mae: 109.1173\n",
      "Epoch 65/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16160.8516 - mae: 109.9260 - val_loss: 15988.9590 - val_mae: 109.1777\n",
      "Epoch 66/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16222.4141 - mae: 110.0120 - val_loss: 16033.5039 - val_mae: 108.9411\n",
      "Epoch 67/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16255.9365 - mae: 109.9626 - val_loss: 16199.3477 - val_mae: 108.8139\n",
      "Epoch 68/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16130.8623 - mae: 109.8058 - val_loss: 16448.0879 - val_mae: 108.7485\n",
      "Epoch 69/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16199.4736 - mae: 109.9551 - val_loss: 15973.1270 - val_mae: 109.4194\n",
      "Epoch 70/5000\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 16213.9912 - mae: 110.0649 - val_loss: 15964.5996 - val_mae: 109.0359\n",
      "Epoch 71/5000\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 16246.2236 - mae: 110.0117 - val_loss: 16018.5518 - val_mae: 109.2001\n",
      "Epoch 72/5000\n",
      " 85/242 [=========>....................] - ETA: 0s - loss: 15914.2305 - mae: 109.0421"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3856592/1335192276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# attach it to a new variable called 'history' in case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# to look at the learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu')) # (features,)\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
    "model.add(Dense(1, activation='linear')) # output node\n",
    "model.summary() # see what your model looks like\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "\n",
    "# early stopping callback\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   patience=50,\n",
    "                   restore_best_weights = True)\n",
    "\n",
    "# fit the model!\n",
    "# attach it to a new variable called 'history' in case\n",
    "# to look at the learning curves\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    callbacks=[es],\n",
    "                    epochs=5000,\n",
    "                    batch_size=50,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of actual vs. pred\n",
    "# specify the dimensions \n",
    "fig, axes = plt.subplots(1,2) # 1 row, 2 columns\n",
    "\n",
    "# this makes the individual subplots\n",
    "# Training Results\n",
    "axes[0].scatter(x=y_train, y=model.predict(X_train)) #first row, first entry (left top)\n",
    "axes[0].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[0].set_title(\"Training\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[0].get_xlim())\n",
    "axes[0].plot(x, x, color='red')\n",
    "# Validation Results\n",
    "axes[1].scatter(x=y_test, y=model.predict(X_test)) # first row, second entry (right top)\n",
    "axes[1].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[1].set_title(\"Validation\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[1].get_xlim())\n",
    "axes[1].plot(x, x, color='red')\n",
    "\n",
    "# tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred\n",
    "\n",
    "trainpreds = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_train, trainpreds)) # train\n",
    "print(mean_absolute_error(y_test, pred)) # test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpe351",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
